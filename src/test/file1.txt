the quick brown fox, jumped over the lazy dog queue
the quick-on brown Fox jumping over the lazy dogs
Start Page
	•	We need the links to the social media pages 


Sign Up
	•	Please validate to ensure only phone numbers can be inputted
	•	Please validate to ensure that only syntactically correct emails can be inputted 
	•	When searching for rank, it selects Corporal as part of the search items as well. This type to search doesn’t work well.  
	•	Terms and Conditions are still showing dummy text
	•	We sent wrong phone and stuff, but the error message that was returned didn’t show any message

Dashboard
	•	The top part is bleeding into the unsafe areas so needs to be adjusted. This should be corrected across all screens 
	•	Balances are showing # instead of NGN or N 
	•	Wrong content for Request Submission failed 
	•	Withdrawal request is also failed. Is this because no endpoint has been mapped to this?
	•	All numeric fields should trigger number keyboards only. 

Loan
	•	Loan Summary Shows “dig” 
	•	When you click Loan Summary, it shows “Select Rank” 
	•	Is guarantor dummy? Why is it showing USD? 
	•	Loans data not clear and should behave as a drop down 
	•	What determines the interest rate on the loans?  
	•	Add guarantor now working
W. Bruce Cro  Donald Metzler Trevor Strohman
Search Engines
Information Retrieval in Practice
©W.B. Cro , D. Metzler, T. Strohman, 2015
 is book was previously published by: Pearson Education, Inc.
 Preface
 is book provides an overview of the important issues in information retrieval, and how those issues affect the design and implementation of search engines. Not every topic is covered at the same level of detail. We focus instead on what we consider to be the most important alternatives to implementing search engine components and the information retrieval models underlying them. Web search engines are obviously a major topic, and we base our coverage primarily on the technology we all use on the Web,1 but search engines are also used in many other applications.  at is the reason for the strong emphasis on the information re- trieval theories and concepts that underlie all search engines.
 e target audience for the book is primarily undergraduates in computer sci- ence or computer engineering, but graduate students should also  nd this useful. We also consider the book to be suitable for most students in information sci- ence programs. Finally, practicing search engineers should bene t from the book, whatever their background.  ere is mathematics in the book, but nothing too esoteric.  ere are also code and programming exercises in the book, but nothing beyond the capabilities of someone who has taken some basic computer science and programming classes.
 e exercises at the end of each chapter make extensive use of a JavaTM-based open source search engine called Galago. Galago was designed both for this book and to incorporate lessons learned from experience with the Lemur and Indri projects. In other words, this is a fully functional search engine that can be used to support real applications. Many of the programming exercises require the use, modi cation, and extension of Galago components.
1 In keeping with common usage, most uses of the word “web” in this book are not cap- italized, except when we refer to the World Wide Web as a separate entity.
 
VI Preface
Contents
In the  rst chapter, we provide a high-level review of the  eld of information re- trieval and its relationship to search engines. In the second chapter, we describe the architecture of a search engine.  is is done to introduce the entire range of search engine components without getting stuck in the details of any particular aspect. In Chapter 3, we focus on crawling, document feeds, and other techniques for acquiring the information that will be searched. Chapter 4 describes the sta- tistical nature of text and the techniques that are used to process it, recognize im- portant features, and prepare it for indexing. Chapter 5 describes how to create indexes for efficient search and how those indexes are used to process queries. In Chapter 6, we describe the techniques that are used to process queries and trans- form them into better representations of the user’s information need.
Ranking algorithms and the retrieval models they are based on are covered in Chapter 7.  is chapter also includes an overview of machine learning tech- niques and how they relate to information retrieval and search engines. Chapter 8 describes the evaluation and performance metrics that are used to compare and tune search engines. Chapter 9 covers the important classes of techniques used for classi cation,  ltering, clustering, and dealing with spam. Social search is a term used to describe search applications that involve communities of people in tag- ging content or answering questions. Search techniques for these applications and peer-to-peer search are described in Chapter 10. Finally, in Chapter 11, we give an overview of advanced techniques that capture more of the content of documents than simple word-based approaches.  is includes techniques that use linguistic features, the document structure, and the content of nontextual media, such as images or music.
Information retrieval theory and the design, implementation, evaluation, and use of search engines cover too many topics to describe them all in depth in one book. We have tried to focus on the most important topics while giving some coverage to all aspects of this challenging and rewarding subject.
Supplements
A range of supplementary material is provided for the book.  is material is de- signed both for those taking a course based on the book and for those giving the course. Speci cally, this includes:
• Extensive lecture slides (in PDF and PPT format)
Preface VII
• Solutions to selected end–of–chapter problems (instructors only)
• Test collections for exercises
• Galago search engine
 e supplements are available at www.search-engines-book.com. Acknowledgments
First and foremost, this book would not have happened without the tremen- dous support and encouragement from our wives, Pam Aselton, Anne-Marie Strohman, and Shelley Wang.  e University of Massachusetts Amherst provided material support for the preparation of the book and awarded a Conti Faculty Fel- lowship to Cro , which sped up our progress signi cantly.  e staff at the Center for Intelligent Information Retrieval (Jean Joyce, Kate Moruzzi, Glenn Stowell, and Andre Gauthier) made our lives easier in many ways, and our colleagues and students in the Center provided the stimulating environment that makes work- ing in this area so rewarding. A number of people reviewed parts of the book and we appreciated their comments. Finally, we have to mention our children, Doug, Eric, Evan, and Natalie, or they would never forgive us.
B C D M T S
2015 Update
 is version of the book is being made available for free download. It has been edited to correct the minor errors noted in the 5 years since the book’s publica- tion.  e authors, meanwhile, are working on a second edition.
 Contents
1 SearchEnginesandInformationRetrieval ....................... 1 1.1 WhatIsInformationRetrieval?.............................. 1 1.2  eBigIssues............................................. 4 1.3 SearchEngines............................................ 6 1.4 SearchEngineers .......................................... 9
2 ArchitectureofaSearchEngine................................. 13
2.1 WhatIsanArchitecture? ................................... 13
2.2 BasicBuildingBlocks ...................................... 14
2.3 BreakingItDown ......................................... 17
2.3.1 TextAcquisition..................................... 17
2.3.2 TextTransformation ................................. 19
2.3.3 IndexCreation...................................... 22
2.3.4 UserInteraction..................................... 23
2.3.5 Ranking............................................ 25
2.3.6 Evaluation.......................................... 27
2.4 HowDoesItReallyWork?.................................. 28
3 CrawlsandFeeds.............................................. 31
3.1 DecidingWhattoSearch................................... 31
3.2 CrawlingtheWeb ......................................... 32
3.2.1 RetrievingWebPages ................................ 33 3.2.2  eWebCrawler.................................... 35 3.2.3 Freshness........................................... 37 3.2.4 FocusedCrawling ................................... 41 3.2.5 DeepWeb.......................................... 41
X
Contents
3.2.6 Sitemaps ........................................... 43
3.2.7 DistributedCrawling ................................ 44
3.3 CrawlingDocumentsandEmail............................. 46
3.4 DocumentFeeds .......................................... 47
3.5  eConversionProblem ................................... 49
3.5.1 CharacterEncodings................................. 50
3.6 StoringtheDocuments..................................... 52 3.6.1 UsingaDatabaseSystem.............................. 53 3.6.2 RandomAccess ..................................... 53 3.6.3 CompressionandLargeFiles.......................... 54 3.6.4 Update............................................. 56 3.6.5 BigTable............................................ 57
3.7 DetectingDuplicates ...................................... 60
3.8 RemovingNoise........................................... 63
4 ProcessingText ............................................... 73
4.1 FromWordstoTerms...................................... 73
4.2 TextStatistics............................................. 75
4.2.1 VocabularyGrowth.................................. 80
4.2.2 EstimatingCollectionandResultSetSizes .............. 83
4.3 DocumentParsing......................................... 86 4.3.1 Overview........................................... 86 4.3.2 Tokenizing ......................................... 87 4.3.3 Stopping ........................................... 90 4.3.4 Stemming .......................................... 91 4.3.5 PhrasesandN-grams................................. 97
4.4 DocumentStructureandMarkup............................ 101
4.5 LinkAnalysis ............................................. 104 4.5.1 AnchorText ........................................ 105 4.5.2 PageRank .......................................... 105 4.5.3 LinkQuality........................................ 111
4.6 InformationExtraction..................................... 113 4.6.1 HiddenMarkovModelsforExtraction ................. 115
4.7 Internationalization........................................ 118
Contents XI
5 RankingwithIndexes.......................................... 125
5.1 Overview................................................. 125
5.2 AbstractModelofRanking ................................. 126
5.3 InvertedIndexes........................................... 129
5.3.1 Documents......................................... 131 5.3.2 Counts............................................. 133 5.3.3 Positions ........................................... 134 5.3.4 FieldsandExtents ................................... 136 5.3.5 Scores.............................................. 138 5.3.6 Ordering ........................................... 139
5.4 Compression ............................................. 140 5.4.1 EntropyandAmbiguity .............................. 142 5.4.2 DeltaEncoding ..................................... 144 5.4.3 Bit-AlignedCodes................................... 145 5.4.4 Byte-AlignedCodes ................................. 148 5.4.5 CompressioninPractice.............................. 149 5.4.6 LookingAhead...................................... 151 5.4.7 SkippingandSkipPointers ........................... 151
5.5 AuxiliaryStructures........................................ 154
5.6 IndexConstruction........................................ 156 5.6.1 SimpleConstruction................................. 156 5.6.2 Merging............................................ 157 5.6.3 ParallelismandDistribution .......................... 158 5.6.4 Update............................................. 164
5.7 QueryProcessing.......................................... 165 5.7.1 Document-at-a-timeEvaluation ....................... 166 5.7.2 Term-at-a-timeEvaluation............................168 5.7.3 OptimizationTechniques............................. 170 5.7.4 StructuredQueries .................................. 178 5.7.5 DistributedEvaluation ............................... 180 5.7.6 Caching............................................ 181
6 QueriesandInterfaces ......................................... 187
6.1 InformationNeedsandQueries ............................. 187
6.2 QueryTransformationandRe nement....................... 190
6.2.1 StoppingandStemmingRevisited ..................... 190 6.2.2 SpellCheckingandSuggestions ....................... 193
XII
Contents
6.2.3 QueryExpansion.................................... 199 6.2.4 RelevanceFeedback.................................. 208 6.2.5 ContextandPersonalization .......................... 211
6.3 ShowingtheResults ....................................... 215 6.3.1 ResultPagesandSnippets ............................ 215 6.3.2 AdvertisingandSearch ............................... 218 6.3.3 ClusteringtheResults................................ 221
6.4 Cross-LanguageSearch..................................... 226
RetrievalModels .............................................. 233
7.1 OverviewofRetrievalModels............................... 233 7.1.1 BooleanRetrieval.................................... 235 7.1.2  eVectorSpaceModel.............................. 237
7.2 ProbabilisticModels ....................................... 243 7.2.1 InformationRetrievalasClassi cation ................. 244 7.2.2  eBM25RankingAlgorithm........................ 250
7.3 RankingBasedonLanguageModels ......................... 252 7.3.1 QueryLikelihoodRanking ........................... 254 7.3.2 Relevance Models and Pseudo-Relevance Feedback . . . . . . 261
7.4 ComplexQueriesandCombiningEvidence................... 267 7.4.1  eInferenceNetworkModel ........................ 268 7.4.2  eGalagoQueryLanguage .......................... 273
7.5 WebSearch............................................... 279
7.6 MachineLearningandInformationRetrieval.................. 283 7.6.1 LearningtoRank.................................... 284 7.6.2 TopicModelsandVocabularyMismatch................ 288
7.7 Application-BasedModels.................................. 291
EvaluatingSearchEngines...................................... 297
8.1 WhyEvaluate? ............................................ 297
8.2  eEvaluationCorpus..................................... 299
8.3 Logging.................................................. 305
8.4 EffectivenessMetrics....................................... 308
8.4.1 RecallandPrecision ................................. 308 8.4.2 AveragingandInterpolation .......................... 313 8.4.3 FocusingontheTopDocuments ...................... 318 8.4.4 UsingPreferences.................................... 321
7
8
Contents XIII
8.5 EfficiencyMetrics ......................................... 322
8.6 Training,Testing,andStatistics.............................. 325 8.6.1 Signi canceTests.................................... 325 8.6.2 SettingParameterValues ............................. 330 8.6.3 OnlineTesting ...................................... 332
8.7  eBottomLine.......................................... 333
9 Classi cationandClustering ................................... 339
9.1 Classi cationandCategorization............................ 340 9.1.1 NaïveBayes......................................... 342 9.1.2 SupportVectorMachines............................. 351 9.1.3 Evaluation.......................................... 359 9.1.4 Classi erandFeatureSelection........................ 359 9.1.5 Spam,Sentiment,andOnlineAdvertising .............. 364
9.2 Clustering................................................ 373 9.2.1 HierarchicalandK-MeansClustering.................. 375 9.2.2 KNearestNeighborClustering ....................... 384 9.2.3 Evaluation.......................................... 386 9.2.4 HowtoChooseK................................... 387 9.2.5 ClusteringandSearch................................ 389
10 SocialSearch ................................................. 397 10.1WhatIsSocialSearch? ..................................... 397 10.2UserTagsandManualIndexing ............................. 400
10.2.1SearchingTags ...................................... 402 10.2.2InferringMissingTags................................ 404 10.2.3BrowsingandTagClouds............................. 406
10.3SearchingwithCommunities ............................... 408 10.3.1WhatIsaCommunity? .............................. 408 10.3.2FindingCommunities................................ 409 10.3.3Community-BasedQuestionAnswering................ 415 10.3.4CollaborativeSearching .............................. 420
10.4FilteringandRecommending ............................... 423 10.4.1DocumentFiltering.................................. 423 10.4.2CollaborativeFiltering ............................... 432
10.5Peer-to-PeerandMetasearch ................................ 438 10.5.1DistributedSearch................................... 438
XIV Contents
10.5.2P2PNetworks ...................................... 442
11 BeyondBagofWords.......................................... 451 11.1Overview................................................. 451 11.2Feature-BasedRetrievalModels ............................. 452 11.3TermDependenceModels .................................. 454 11.4StructureRevisited ........................................ 459
11.4.1XMLRetrieval...................................... 461
11.4.2EntitySearch ....................................... 464 11.5LongerQuestions,BetterAnswers ........................... 466 11.6Words,Pictures,andMusic ................................. 470 11.7OneSearchFitsAll? ....................................... 479
References........................................................ 487 Index ............................................................ 511
 List of Figures
1.1 Search engine design and the core information retrieval issues . . . 9
2.1  eindexingprocess ...................................... 15
2.2  equeryprocess......................................... 16
3.1 A uniform resource locator (URL), split into three parts . . . . . . . 33
3.2 Crawling the Web.  e web crawler connects to web servers to  nd pages. Pages may link to other pages on the same server or
ondifferentservers. ....................................... 34
3.3 Anexamplerobots.txt le.................................. 36
3.4 Asimplecrawlingthreadimplementation .................... 37
3.5 AnHTTPHEADrequestandserverresponse ............... 38
3.6 Ageandfreshnessofasinglepageovertime .................. 39
3.7 Expected age of a page with mean change frequency λ = 1/7
(oneweek)............................................... 40
3.8 Anexamplesitemap le.................................... 43
3.9 AnexampleRSS2.0feed .................................. 48
3.10 An example of text in the TREC Web compound document
format................................................... 55
3.11 Anexamplelinkwithanchortext ........................... 56
3.12 BigTable stores data in a single logical table, which is split into
manysmallertablets....................................... 57
3.13 ABigTablerow ........................................... 58
3.14 Exampleof ngerprintingprocess ........................... 62
3.15 Exampleofsimhash ngerprintingprocess ................... 64
3.16 Maincontentblockinawebpage ........................... 65
XVI List of Figures
3.17 Tagcountsusedtoidentifytextblocksinawebpage........... 66
3.18 PartoftheDOMstructurefortheexamplewebpage .......... 67
4.1 Rank versus probability of occurrence for words assuming Zipf’slaw(rank×probability=0.1) ........................ 76
4.2 A log-log plot of Zipf ’s law compared to real data from AP89.
 e predicted relationship between probability of occurrence andrankbreaksdownbadlyathighranks. ................... 79
4.3 Vocabulary growth for the TREC AP89 collection compared toHeaps’law............................................. 81
4.4 Vocabulary growth for the TREC GOV2 collection compared toHeaps’law............................................. 82
4.5 Resultsizeestimateforwebsearch .......................... 83
4.6 Comparison of stemmer output for a TREC query. Stopwords havealsobeenremoved. ................................... 95
4.7 OutputofaPOStaggerforaTRECquery ................... 98
4.8 PartofawebpagefromWikipedia .......................... 102
4.9 HTMLsourceforexampleWikipediapage .................. 103
4.10 A sample “Internet” consisting of just three web pages.  e arrowsdenotelinksbetweenthepages. ...................... 108
4.11 Pseudocode for the iterative PageRank algorithm. . . . . . . . . . . . . . 110
4.12 Trackbacklinksinblogpostings ............................ 112
4.13 Texttaggedbyinformationextraction ....................... 114
4.14 Sentencemodelforstatisticalentityextractor ................. 116
4.15 Chinesesegmentationandbigrams.......................... 119
5.1  e components of the abstract model of ranking: documents, features,queries,theretrievalfunction,anddocumentscores.... 127
5.2 A more concrete model of ranking. Notice how both the query
and the document have feature functions in this model. . . . . . . . . 128
5.3 An inverted index for the documents (sentences) in Table 5.1 . . . 132
5.4 An inverted index, with word counts, for the documents in Table5.1................................................. 134
5.5 An inverted index, with word positions, for the documents in Table5.1................................................. 135
5.6 Aligning posting lists for “tropical” and “ sh” to  nd the phrase “tropical sh”............................................. 136
List of Figures XVII
5.7 Aligning posting lists for “ sh” and title to  nd matches of the word“ sh”inthetitle eldofadocument. ................... 138
5.8 Pseudocodeforasimpleindexer ............................ 157
5.9 An example of index merging.  e  rst and second indexes are mergedtogethertoproducethecombinedindex. ............. 158
5.10 MapReduce.............................................. 161
5.11 Mapperforacreditcardsummingalgorithm.................. 162
5.12 Reducerforacreditcardsummingalgorithm ................. 162
5.13 Mapperfordocuments .................................... 163
5.14 Reducerforwordpostings ................................. 164
5.15 Document-at-a-time query evaluation.  e numbers (x:y)
represent a document number (x) and a word count (y). . . . . . . . 166
5.16 A simple document-at-a-time retrieval algorithm . . . . . . . . . . . . . . 167
5.17 Term-at-a-timequeryevaluation ............................ 168
5.18 Asimpleterm-at-a-timeretrievalalgorithm................... 169
5.19 Skip pointers in an inverted list.  e gray boxes show skip
pointers, which point into the white boxes, which are inverted listpostings............................................... 170
5.20 A term-at-a-time retrieval algorithm with conjunctive processing 173
5.21 A document-at-a-time retrieval algorithm with conjunctive
processing ............................................... 174
5.22 MaxScore retrieval with the query “eucalyptus tree”.  e gray
boxesindicatepostingsthatcanbesafelyignoredduringscoring. 176
5.23 Evaluation tree for the structured query #combine(#od:1(tropical
fish)#od:1(aquariumfish)fish)................................ 179
6.1 Toptenresultsforthequery“tropical sh” ................... 209
6.2 Geographic representation of Cape Cod using bounding
rectangles................................................ 214
6.3 Typicaldocumentsummaryforawebsearch.................. 215
6.4 An example of a text span of words (w) bracketed by signi cant
words(s)usingLuhn’salgorithm............................ 216
6.5 Advertisements displayed by a search engine for the query “ sh
tanks”................................................... 220
6.6 Clusters formed by a search engine from top-ranked documents
for the query “tropical  sh”. Numbers in brackets are the numberofdocumentsinthecluster. ......................... 222
XVIII ListofFigures
6.7 Categories returned for the query “tropical  sh” in a popular onlineretailer ............................................ 225
6.8 Subcategories and facets for the “Home & Garden” category . . . . 225
6.9 Cross-languagesearch ..................................... 226
6.10 A French web page in the results list for the query “pecheur
france” .................................................. 228
7.1 Term-document matrix for a collection of four documents . . . . . . 239
7.2 Vectorrepresentationofdocumentsandqueries............... 240
7.3 Classifying a document as relevant or non-relevant . . . . . . . . . . . . 245
7.4 Exampleinferencenetworkmodel .......................... 269
7.5 Inferencenetworkwiththreenodes ......................... 271
7.6 Galagoqueryforthedependencemodel ..................... 282
7.7 Galagoqueryforwebdata.................................. 282
8.1 ExampleofaTRECtopic.................................. 302
8.2 Recall and precision values for two rankings of six relevant
documents............................................... 311
8.3 Recall and precision values for rankings from two different queries 314
8.4 Recall-precisiongraphsfortwoqueries....................... 315
8.5 Interpolated recall-precision graphs for two queries . . . . . . . . . . . . 316
8.6 Average recall-precision graph using standard recall levels . . . . . . . 317
8.7 Typical recall-precision graph for 50 queries from TREC . . . . . . . 318
8.8 Probability distribution for test statistic values assuming the
null hypothesis.  e shaded area is the region of rejection for a
one-sidedtest............................................. 327
8.9 Example distribution of query effectiveness improvements . . . . . . 335
9.1 Illustration of how documents are represented in the multiple- Bernoulli event space. In this example, there are 10 documents
(each with a unique id), two classes (spam and not spam), and a vocabulary that consists of the terms “cheap”, “buy”, “banking”, “dinner”,and“the”. ........................................ 346
9.2 Illustration of how documents are represented in the
multinomial event space. In this example, there are 10
documents (each with a unique id), two classes (spam and not
spam), and a vocabulary that consists of the terms “cheap”, “buy”,“banking”,“dinner”,and“the”.......................... 349
List of Figures XIX
9.3 Data set that consists of two classes (pluses and minuses).  e
data set on the le  is linearly separable, whereas the one on the rightisnot. .............................................. 352
9.4 Graphical illustration of Support Vector Machines for the
linearly separable case. Here, the hyperplane de ned by w is
shown, as well as the margin, the decision regions, and the supportvectors,whichareindicatedbycircles................. 353
9.5 Generative process used by the Naïve Bayes model. First, a class
is chosen according to P (c), and then a document is chosen accordingtoP(d|c). ...................................... 360
9.6 Example data set where non-parametric learning algorithms,
such as a nearest neighbor classi er, may outperform parametric algorithms.  e pluses and minuses indicate positive and
negative training examples, respectively.  e solid gray line
shows the actual decision boundary, which is highly non-linear. . 361
9.7 ExampleoutputofSpamAssassinemailspam lter ............ 365
9.8 Example of web page spam, showing the main page and some
oftheassociatedtermandlinkspam......................... 367
9.9 Example product review incorporating sentiment . . . . . . . . . . . . . 370
9.10 Example semantic class match between a web page about
rainbow  sh (a type of tropical  sh) and an advertisement
for tropical  sh food.  e nodes “Aquariums”, “Fish”, and
“Supplies” are example nodes within a semantic hierarchy.
 e web page is classi ed as “Aquariums - Fish” and the ad is classi ed as “Supplies - Fish”. Here, “Aquariums” is the least
common ancestor. Although the web page and ad do not share
any terms in common, they can be matched because of their semanticsimilarity......................................... 372
9.11 Example of divisive clustering with K = 4.  e clustering
proceeds from le  to right and top to bottom, resulting in four clusters................................................... 376
9.12 Example of agglomerative clustering with K = 4.  e
clustering proceeds from le  to right and top to bottom, resultinginfourclusters. ................................... 377
9.13 Dendrogram that illustrates the agglomerative clustering of the pointsfromFigure9.12.................................... 377
XX
List of Figures
9.14 Examples of clusters in a graph formed by connecting nodes representing instances. A link represents a distance between the twoinstancesthatislessthansomethresholdvalue............. 379
9.15 Illustration of how various clustering cost functions are computed 381
9.16 Example of overlapping clustering using nearest neighbor
clustering with K = 5.  e overlapping clusters for the black
points (A, B, C, and D) are shown.  e  ve nearest neighbors foreachblackpointareshadedgrayandlabeledaccordingly..... 385
9.17 Example of overlapping clustering using Parzen windows.  e
clusters for the black points (A, B, C, and D) are shown.  e
shaded circles indicate the windows used to determine cluster membership.  e neighbors for each black point are shaded grayandlabeledaccordingly. ............................... 388
9.18 Cluster hypothesis tests on two TREC collections.  e top
two compare the distributions of similarity values between relevant-relevant and relevant-nonrelevant pairs (light gray) of documents.  e bottom two show the local precision of the relevantdocuments........................................ 390
10.1 Search results used to enrich a tag representation. In this
example, the tag being expanded is “tropical  sh”.  e query
“tropical  sh” is run against a search engine, and the snippets returned are then used to generate a distribution over related
terms. ................................................... 403
10.2 Example of a tag cloud in the form of a weighted list.  e
tags are in alphabetical order and weighted according to some criteria,suchaspopularity. ................................. 407
10.3 Illustration of the HITS algorithm. Each row corresponds to a
single iteration of the algorithm and each column corresponds toaspeci cstepofthealgorithm. ........................... 412
10.4 Example of how nodes within a directed graph can be
represented as vectors. For a given node p, its vector representationhascomponentqsetto1ifp→q.............. 413
List of Figures XXI
10.5 Overview of the two common collaborative search scenarios.
On the le  is co-located collaborative search, which involves
multiple participants in the same location at the same time.
On the right is remote collaborative search, where participants
are in different locations and not necessarily all online and searchingatthesametime. ................................. 421
10.6 Example of a static  ltering system. Documents arrive over time
and are compared against each pro le. Arrows from documents
to pro les indicate the document matches the pro le and is
retrieved. ................................................ 425
10.7 Example of an adaptive  ltering system. Documents arrive
over time and are compared against each pro le. Arrows from documents to pro les indicate the document matches the
pro le and is retrieved. Unlike static  ltering, where pro les are
static over time, pro les are updated dynamically (e.g., when a newmatchoccurs)......................................... 428
10.8 A set of users within a recommender system. Users and their
ratings for some item are given. Users with question marks
above their heads have not yet rated the item. It is the goal of
the recommender system to  ll in these question marks. . . . . . . . . 434
10.9 Illustration of collaborative  ltering using clustering. Groups
of similar users are outlined with dashed lines. Users and their
ratings for some item are given. In each group, there is a single
user who has not judged the item. For these users, the unjudged
item is assigned an automatic rating based on the ratings of similarusers. ............................................. 435
10.10 Metasearch engine architecture.  e query is broadcast to
multiple web search engines and result lists are merged. . . . . . . . . 439
10.11 Network architectures for distributed search: (a) central hub;
(b) pure P2P; and (c) hierarchical P2P. Dark circles are hub
or superpeer nodes, gray circles are provider nodes, and white circlesareconsumernodes.................................. 443
10.12 Neighborhoods (Ni) of a hub node (H) in a hierarchical P2P network ................................................. 445
XXII List of Figures
11.1 Example Markov Random Field model assumptions, including
full independence (top le ), sequential dependence (top
right), full dependence (bottom le ), and general dependence (bottomright) ........................................... 455
11.2 Graphical model representations of the relevance model
technique (top) and latent concept expansion (bottom) used
for pseudo-relevance feedback with the query “hubble telescope achievements” ............................................ 459
11.3 Functions provided by a search engine interacting with a simple databasesystem........................................... 461
11.4 Example of an entity search for organizations using the TREC WallStreetJournal1987Collection ......................... 464
11.5 Questionansweringsystemarchitecture...................... 467
11.6 ExamplesofOCRerrors................................... 472
11.7 Examplesofspeechrecognizererrors ........................ 473
11.8 Two images (a  sh and a  ower bed) with color histograms.  ehorizontalaxisishuevalue.............................. 474
11.9  ree examples of content-based image retrieval.  e collection
for the  rst two consists of 1,560 images of cars, faces, apes,
and other miscellaneous subjects.  e last example is from a collection of 2,048 trademark images. In each case, the le most imageisthequery. ........................................ 475
11.10KeyframesextractedfromaTRECvideoclip ................ 476 11.11Examplesofautomatictextannotationofimages.............. 477 11.12  ree representations of Bach’s “Fugue #10”: audio, MIDI, and
conventionalmusicnotation ............................... 478
 List of Tables
1.1 Somedimensionsofinformationretrieval .................... 4 3.1 UTF-8encoding.......................................... 51
4.1 StatisticsfortheAP89collection............................ 77
4.2 Mostfrequent50wordsfromAP89 ......................... 78
4.3 Low-frequencywordsfromAP89 ........................... 78
4.4 Examplewordfrequencyranking ........................... 79
4.5 Proportions of words occurring n times in 336,310 documents
from the TREC Volume 3 corpus.  e total vocabulary size
(numberofuniquewords)is508,209. ....................... 80
4.6 Document frequencies and estimated frequencies for word combinations (assuming independence) in the GOV2 Web
collection.Collectionsize(N)is25,205,179.................. 84
4.7 Examples of errors made by the original Porter stemmer. False
positives are pairs of words that have the same stem. False
negativesarepairsthathavedifferentstems. .................. 93
4.8 ExamplesofwordswiththeArabicrootktb .................. 96
4.9 High-frequency noun phrases from a TREC collection and
U.S.patentsfrom1996 .................................... 99
4.10 StatisticsfortheGooglen-gramsample ...................... 101
5.1 Four sentences from the Wikipedia entry for tropical  sh . . . . . . . 132
5.2 Elias-γcodeexamples ..................................... 146
5.3 Elias-δcodeexamples...................................... 147
5.4 Spacerequirementsfornumbersencodedinv-byte ............ 149
XXIV ListofTables
5.5 Sampleencodingsforv-byte................................ 149
5.6 Skiplengths(k)andexpectedprocessingsteps ................ 152
6.1 Partial entry for the Medical Subject (MeSH) Heading “Neck Pain”.................................................... 200
6.2 Termassociationmeasures ................................. 203
6.3 Most strongly associated words for “tropical” in a collection of
TREC news stories. Co-occurrence counts are measured at the documentlevel. .......................................... 204
6.4 Most strongly associated words for “ sh” in a collection of
TREC news stories. Co-occurrence counts are measured at the documentlevel. .......................................... 205
6.5 Most strongly associated words for “ sh” in a collection of
TREC news stories. Co-occurrence counts are measured in windowsof vewords. .................................... 205
7.1 Contingency table of term occurrences for a particular query . . . 248
7.2 BM25scoresforanexampledocument ...................... 252
7.3 Querylikelihoodscoresforanexampledocument............. 260
7.4 Highest-probability terms from relevance model for four
example queries (estimated using top 10 documents) . . . . . . . . . . . 266
7.5 Highest-probability terms from relevance model for four
example queries (estimated using top 50 documents) . . . . . . . . . . . 267
7.6 Conditionalprobabilitiesforexamplenetwork ............... 272
7.7 Highest-probability terms from four topics in LDA model . . . . . 290
8.1 Statistics for three example text collections.  e average number
of words per document is calculated without stemming. . . . . . . . . 301
8.2 Statistics for queries from example text collections . . . . . . . . . . . . . 301
8.3 Sets of documents de ned by a simple search with binary
relevance ................................................ 309
8.4 Precision values at standard recall levels calculated using interpolation ............................................. 317
8.5 De nitionsofsomeimportantefficiencymetrics .............. 323
8.6 Arti cial effectiveness data for two retrieval algorithms (A and
B) over 10 queries.  e column B – A gives the difference in effectiveness. ............................................. 328
List of Tables XXV
9.1 A list of kernels that are typically used with SVMs. For each kernel,thename,value,andimplicitdimensionalityaregiven. .. 357
10.1 ExamplequestionssubmittedtoYahoo!Answers.............. 416
10.2 Translations automatically learned from a set of question and
answer pairs.  e 10 most likely translations for the terms
“everest”,“xp”,and“search”aregiven.......................... 419
10.3 Summary of static and adaptive  ltering models. For each, the
pro lerepresentationandpro leupdatingalgorithmaregiven. . 430
10.4 Contingency table for the possible outcomes of a  ltering system. Here, TP (true positive) is the number of relevant
documents retrieved, FN (false negative) is the number of
relevant documents not retrieved, FP (false positive) is the
number of non-relevant documents retrieved, and TN (true negative) is the number of non-relevant documents not retrieved. 431
11.1 Most likely one- and two-word concepts produced using latent concept expansion with the top 25 documents retrieved for
the query “hubble telescope achievements” on the TREC ROBUSTcollection ...................................... 460
11.2 Example TREC QA questions and their corresponding questioncategories........................................ 469
1
Search Engines and Information Retrieval
“Mr. Helpmann, I’m keen to get into Information Retrieval.”
Sam Lowry, Brazil
1.1 What Is Information Retrieval?
 is book is designed to help people understand search engines, evaluate and compare them, and modify them for speci c applications. Searching for infor- mation on the Web is, for most people, a daily activity. Search and communi- cation are by far the most popular uses of the computer. Not surprisingly, many people in companies and universities are trying to improve search by coming up with easier and faster ways to  nd the right information.  ese people, whether they call themselves computer scientists, so ware engineers, information scien- tists, search engine optimizers, or something else, are working in the  eld of In- formation Retrieval.1 So, before we launch into a detailed journey through the internals of search engines, we will take a few pages to provide a context for the rest of the book.
Gerard Salton, a pioneer in information retrieval and one of the leading  gures from the 1960s to the 1990s, proposed the following de nition in his classic 1968 textbook (Salton, 1968):
Information retrieval is a  eld concerned with the structure, analysis, or- ganization, storage, searching, and retrieval of information.
Despite the huge advances in the understanding and technology of search in the past 40 years, this de nition is still appropriate and accurate.  e term “informa-
1 Information retrieval is o en abbreviated as IR. In this book, we mostly use the full term.  is has nothing to do with the fact that many people think IR means “infrared” or something else.
  
2 1 SearchEnginesandInformationRetrieval
tion” is very general, and information retrieval includes work on a wide range of types of information and a variety of applications related to search.
 e primary focus of the  eld since the 1950s has been on text and text docu- ments. Web pages, email, scholarly papers, books, and news stories are just a few of the many examples of documents. All of these documents have some amount of structure, such as the title, author, date, and abstract information associated with the content of papers that appear in scienti c journals.  e elements of this structure are called attributes, or  elds, when referring to database records.  e important distinction between a document and a typical database record, such as a bank account record or a  ight reservation, is that most of the information in the document is in the form of text, which is relatively unstructured.
Toillustratethisdifference,considertheinformationcontainedintwotypical attributes of an account record, the account number and current balance. Both are very well de ned, both in terms of their format (for example, a six-digit integer for an account number and a real number with two decimal places for balance) and their meaning. It is very easy to compare values of these attributes, and conse- quently it is straightforward to implement algorithms to identify the records that satisfy queries such as “Find account number 321456” or “Find accounts with balances greater than $50,000.00”.
Now consider a news story about the merger of two banks.  e story will have some attributes, such as the headline and source of the story, but the primary con- tent is the story itself. In a database system, this critical piece of information would typically be stored as a single large attribute with no internal structure. Most of the queries submitted to a web search engine such as Google2 that relate to this story will be of the form “bank merger” or “bank takeover”. To do this search, we must design algorithms that can compare the text of the queries with the text of the story and decide whether the story contains the information that is being sought. De ning the meaning of a word, a sentence, a paragraph, or a whole news story is much more difficult than de ning an account number, and consequently comparing text is not easy. Understanding and modeling how people compare texts, and designing computer algorithms to accurately perform this comparison, is at the core of information retrieval.
Increasingly, applications of information retrieval involve multimedia docu- ments with structure, signi cant text content, and other media. Popular infor- mation media include pictures, video, and audio, including music and speech. In
2 http://www.google.com
 
1.1 WhatIsInformationRetrieval? 3
some applications, such as in legal support, scanned document images are also important.  ese media have content that, like text, is difficult to describe and compare.  e current technology for searching non-text documents relies on text descriptions of their content rather than the contents themselves, but progress is being made on techniques for direct comparison of images, for example.
In addition to a range of media, information retrieval involves a range of tasks and applications.  e usual search scenario involves someone typing in a query to a search engine and receiving answers in the form of a list of documents in ranked order. Although searching the World Wide Web (web search) is by far the most common application involving information retrieval, search is also a crucial part of applications in corporations, government, and many other domains. Vertical search is a specialized form of web search where the domain of the search is re- stricted to a particular topic. Enterprise search involves  nding the required infor- mation in the huge variety of computer  les scattered across a corporate intranet. Web pages are certainly a part of that distributed information store, but most information will be found in sources such as email, reports, presentations, spread- sheets, and structured data in corporate databases. Desktop search is the personal version of enterprise search, where the information sources are the  les stored on an individual computer, including email messages and web pages that have re- cently been browsed. Peer-to-peer search involves  nding information in networks of nodes or computers without any centralized control.  is type of search began as a  le sharing tool for music but can be used in any community based on shared interests, or even shared locality in the case of mobile devices. Search and related information retrieval techniques are used for advertising, for intelligence analy- sis, for scienti c discovery, for health care, for customer support, for real estate, and so on. Any application that involves a collection3of text or other unstructured information will need to organize and search that information.
Search based on a user query (sometimes called ad hoc search because the range of possible queries is huge and not prespeci ed) is not the only text-based task that is studied in information retrieval. Other tasks include  ltering, classi cation, and question answering. Filtering or tracking involves detecting stories of interest based on a person’s interests and providing an alert using email or some other mechanism. Classi cation or categorization uses a de ned set of labels or classes
3  e term database is o en used to refer to a collection of either structured or unstruc- tured data. To avoid confusion, we mostly use the term document collection (or just collection) for text. However, the terms web database and search engine database are so common that we occasionally use them in this book.
 
4 1 SearchEnginesandInformationRetrieval
(such as the categories listed in the Yahoo! Directory4) and automatically assigns those labels to documents. Question answering is similar to search but is aimed at more speci c questions, such as “What is the height of Mt. Everest?”.  e goal of question answering is to return a speci c answer found in the text, rather than a list of documents. Table 1.1 summarizes some of these aspects or dimensions of the  eld of information retrieval.
 Examples of Content
Text
Images Video Scanned documents Audio
Music
Examples of Applications Web search Vertical search Enterprise search Desktop search Peer-to-peer search
Examples of Tasks
Ad hoc search Filtering Classi cation Question answering
  Table 1.1. Some dimensions of information retrieval
1.2 The Big Issues
Information retrieval researchers have focused on a few key issues that remain just as important in the era of commercial web search engines working with billions of web pages as they were when tests were done in the 1960s on document col- lections containing about 1.5 megabytes of text. One of these issues is relevance. Relevance is a fundamental concept in information retrieval. Loosely speaking, a relevant document contains the information that a person was looking for when she submitted a query to the search engine. Although this sounds simple, there are many factors that go into a person’s decision as to whether a particular document is relevant.  ese factors must be taken into account when designing algorithms for comparing text and ranking documents. Simply comparing the text of a query with the text of a document and looking for an exact match, as might be done in a database system or using the grep utility in Unix, produces very poor results in terms of relevance. One obvious reason for this is that language can be used to ex-
4 http://dir.yahoo.com/
 
1.2  eBigIssues 5
press the same concepts in many different ways, o en with very different words.  is is referred to as the vocabulary mismatch problem in information retrieval.
It is also important to distinguish between topical relevance and user relevance. A text document is topically relevant to a query if it is on the same topic. For ex- ample, a news story about a tornado in Kansas would be topically relevant to the query “severe weather events”.  e person who asked the question (o en called the user) may not consider the story relevant, however, if she has seen that story before, or if the story is  ve years old, or if the story is in Chinese from a Chi- nese news agency. User relevance takes these additional features of the story into account.
To address the issue of relevance, researchers propose retrieval models and test how well they work. A retrieval model is a formal representation of the process of matching a query and a document. It is the basis of the ranking algorithm that is used in a search engine to produce the ranked list of documents. A good retrieval model will  nd documents that are likely to be considered relevant by the person who submitted the query. Some retrieval models focus on topical relevance, but a search engine deployed in a real environment must use ranking algorithms that incorporate user relevance.
An interesting feature of the retrieval models used in information retrieval is that they typically model the statistical properties of text rather than the linguistic structure.  is means, for example, that the ranking algorithms are typically far more concerned with the counts of word occurrences than whether the word is a noun or an adjective. More advanced models do incorporate linguistic features, but they tend to be of secondary importance.  e use of word frequency infor- mation to represent text started with another information retrieval pioneer, H.P. Luhn, in the 1950s.  is view of text did not become popular in other  elds of computer science, such as natural language processing, until the 1990s.
Another core issue for information retrieval is evaluation. Since the quality of a document ranking depends on how well it matches a person’s expectations, it was necessary early on to develop evaluation measures and experimental proce- dures for acquiring this data and using it to compare ranking algorithms. Cyril Cleverdon led the way in developing evaluation methods in the early 1960s, and two of the measures he used, precision and recall, are still popular. Precision is a very intuitive measure, and is the proportion of retrieved documents that are rel- evant. Recall is the proportion of relevant documents that are retrieved. When the recall measure is used, there is an assumption that all the relevant documents for a given query are known. Such an assumption is clearly problematic in a web
6 1 SearchEnginesandInformationRetrieval
search environment, but with smaller test collections of documents, this measure can be useful. A test collection5 for information retrieval experiments consists of a collection of text documents, a sample of typical queries, and a list of relevant documents for each query (the relevance judgments).  e best-known test collec- tions are those associated with the TREC6 evaluation forum.
Evaluation of retrieval models and search engines is a very active area, with much of the current focus on using large volumes of log data from user interac- tions, such as clickthrough data, which records the documents that were clicked on during a search session. Clickthrough and other log data is strongly correlated with relevance so it can be used to evaluate search, but search engine companies still use relevance judgments in addition to log data to ensure the validity of their results.
 e third core issue for information retrieval is the emphasis on users and their information needs.  is should be clear given that the evaluation of search is user- centered.  at is, the users of a search engine are the ultimate judges of quality.  is has led to numerous studies on how people interact with search engines and, in particular, to the development of techniques to help people express their in- formation needs. An information need is the underlying cause of the query that a person submits to a search engine. In contrast to a request to a database system, such as for the balance of a bank account, text queries are o en poor descriptions of what the user actually wants. A one-word query such as “cats” could be a request for information on where to buy cats or for a description of the Broadway musi- cal. Despite their lack of speci city, however, one-word queries are very common inwebsearch.Techniquessuchasquerysuggestion,queryexpansion,andrelevance feedback use interaction and context to re ne the initial query in order to produce better ranked lists.
 ese issues will come up throughout this book, and will be discussed in con- siderably more detail. We now have sufficient background to start talking about the main product of research in information retrieval—namely, search engines.
1.3 Search Engines
A search engine is the practical application of information retrieval techniques to large-scale text collections. A web search engine is the obvious example, but as
5 Also known as an evaluation corpus (plural corpora). 6 Text REtrieval Conference—http://trec.nist.gov/
 
1.3 SearchEngines 7
has been mentioned, search engines can be found in many different applications, such as desktop search or enterprise search. Search engines have been around for many years. For example, MEDLINE, the online medical literature search sys- tem, started in the 1970s.  e term “search engine” was originally used to refer to specialized hardware for text search. From the mid-1980s onward, however, it gradually came to be used in preference to “information retrieval system” as the name for the so ware system that compares queries to documents and produces ranked result lists of documents.  ere is much more to a search engine than the ranking algorithm, of course, and we will discuss the general architecture of these systems in the next chapter.
Search engines come in a number of con gurations that re ect the applica- tions they are designed for. Web search engines, such as Google and Yahoo!,7 must be able to capture, or crawl, many terabytes of data, and then provide subsecond response times to millions of queries submitted every day from around the world. Enterprise search engines—for example, Autonomy8—must be able to process the large variety of information sources in a company and use company-speci c knowledge as part of search and related tasks, such as data mining. Data mining refers to the automatic discovery of interesting structure in data and includes tech- niques such as clustering. Desktop search engines, such as the Microso  VistaTM search feature, must be able to rapidly incorporate new documents, web pages, and email as the person creates or looks at them, as well as provide an intuitive interface for searching this very heterogeneous mix of information.  ere is over- lap between these categories with systems such as Google, for example, which is available in con gurations for enterprise and desktop search.
Open source search engines are another important class of systems that have somewhat different design goals than the commercial search engines.  ere are a number of these systems, and the Wikipedia page for information retrieval9 pro- vides links to many of them.  ree systems of particular interest are Lucene,10 Lemur,11 and the system provided with this book, Galago.12 Lucene is a popular Java-based search engine that has been used for a wide range of commercial ap- plications.  e information retrieval techniques that it uses are relatively simple.
7 http://www.yahoo.com
8 http://www.autonomy.com
9 http://en.wikipedia.org/wiki/Information_retrieval
10 http://lucene.apache.org
11 http://www.lemurproject.org
12 http://www.search-engines-book.com
 
8 1 SearchEnginesandInformationRetrieval
Lemur is an open source toolkit that includes the Indri C++-based search engine. Lemur has primarily been used by information retrieval researchers to compare advanced search techniques. Galago is a Java-based search engine that is based on the Lemur and Indri projects.  e assignments in this book make extensive use of Galago. It is designed to be fast, adaptable, and easy to understand, and incorpo- rates very effective information retrieval techniques.
 e “big issues” in the design of search engines include the ones identi ed for information retrieval: effective ranking algorithms, evaluation, and user interac- tion.  ere are, however, a number of additional critical features of search engines that result from their deployment in large-scale, operational environments. Fore- most among these features is the performance of the search engine in terms of mea- sures such as response time, query throughput, and indexing speed. Response time is the delay between submitting a query and receiving the result list, throughput measures the number of queries that can be processed in a given time, and index- ing speed is the rate at which text documents can be transformed into indexes for searching. An index is a data structure that improves the speed of search.  e design of indexes for search engines is one of the major topics in this book.
Another important performance measure is how fast new data can be incorpo- rated into the indexes. Search applications typically deal with dynamic, constantly changing information. Coverage measures how much of the existing information in, say, a corporate information environment has been indexed and stored in the search engine, and recency or  eshness measures the “age” of the stored informa- tion.
Search engines can be used with small collections, such as a few hundred emails and documents on a desktop, or extremely large collections, such as the entire Web.  ere may be only a few users of a given application, or many thousands. Scalability is clearly an important issue for search engine design. Designs that work for a given application should continue to work as the amount of data and the number of users grow. In section 1.1, we described how search engines are used in many applications and for many tasks. To do this, they have to be customizable or adaptable.  is means that many different aspects of the search engine, such as the ranking algorithm, the interface, or the indexing strategy, must be able to be tuned and adapted to the requirements of the application.
Practical issues that impact search engine design also occur for speci c appli- cations.  e best example of this is spam in web search. Spam is generally thought of as unwanted email, but more generally it could be de ned as misleading, inap- propriate, or non-relevant information in a document that is designed for some
1.4 SearchEngineers 9
commercial bene t.  ere are many kinds of spam, but one type that search en- gines must deal with is spam words put into a document to cause it to be retrieved in response to popular queries.  e practice of “spamdexing” can signi cantly de- grade the quality of a search engine’s ranking, and web search engine designers have to develop techniques to identify the spam and remove those documents. Figure 1.1 summarizes the major issues involved in search engine design.
,QIRUPDWLRQ 5HWULHYDO 6HDUFK (QJLQHV
  3HUIRUPDQFH
 (IILFLHQW VHDUFK DQG LQGH[LQJ 
,QFRUSRUDWLQJ QHZ GDWD
 &RYHUDJH DQG IUHVKQHVV
6FDODELOLW\
 *URZLQJ ZLWK GDWD DQG XVHUV
$GDSWDELOLW\
 7XQLQJ IRU DSSOLFDWLRQV
6SHFLILF SUREOHPV
 ( J   VSDP
5HOHYDQFH
 (IIHFWLYH UDQNLQJ 
(YDOXDWLRQ
 7HVWLQJ DQG PHDVXULQJ
,QIRUPDWLRQ QHHGV
 8VHU LQWHUDFWLRQ
 Fig. 1.1. Search engine design and the core information retrieval issues
Based on this discussion of the relationship between information retrieval and search engines, we now consider what roles computer scientists and others play in the design and use of search engines.
1.4 Search Engineers
Information retrieval research involves the development of mathematical models of text and language, large-scale experiments with test collections or users, and a lot of scholarly paper writing. For these reasons, it tends to be done by aca- demics or people in research laboratories.  ese people are primarily trained in computer science, although information science, mathematics, and, occasionally, social science and computational linguistics are also represented. So who works
10 1 SearchEnginesandInformationRetrieval
with search engines? To a large extent, it is the same sort of people but with a more practical emphasis.  e computing industry has started to use the term search engineer to describe this type of person. Search engineers are primarily people trained in computer science, mostly with a systems or database background. Sur- prisingly few of them have training in information retrieval, which is one of the major motivations for this book.
What is the role of a search engineer? Certainly the people who work in the major web search companies designing and implementing new search engines are search engineers, but the majority of search engineers are the people who modify, extend, maintain, or tune existing search engines for a wide range of commercial applications. People who design or “optimize” content for search engines are also search engineers, as are people who implement techniques to deal with spam.  e search engines that search engineers work with cover the entire range mentioned in the last section: they primarily use open source and enterprise search engines for application development, but also get the most out of desktop and web search engines.
 e importance and pervasiveness of search in modern computer applications has meant that search engineering has become a crucial profession in the com- puter industry.  ere are, however, very few courses being taught in computer science departments that give students an appreciation of the variety of issues that are involved, especially from the information retrieval perspective.  is book is in- tended to give potential search engineers the understanding and tools they need.
References and Further Reading
In each chapter, we provide some pointers to papers and books that give more detail on the topics that have been covered.  is additional reading should not be necessary to understand material that has been presented, but instead will give more background, more depth in some cases, and, for advanced topics, will de- scribe techniques and research results that are not covered in this book.
 e classic references on information retrieval, in our opinion, are the books by Salton (1968; 1983) and van Rijsbergen (1979). Van Rijsbergen’s book remains popular, since it is available on the Web.13 All three books provide excellent de- scriptions of the research done in the early years of information retrieval, up to the late 1970s. Salton’s early book was particularly important in terms of de ning
13 http://www.dcs.gla.ac.uk/Keith/Preface.html
 
1.4 SearchEngineers 11
the  eld of information retrieval for computer science. More recent books include Baeza-Yates and Ribeiro-Neto (1999) and Manning et al. (2008).
Research papers on all the topics covered in this book can be found in the Proceedings of the Association for Computing Machinery (ACM) Special In- terest Group on Information Retrieval (SIGIR) Conference.  ese proceedings are available on the Web as part of the ACM Digital Library.14 Good papers on information retrieval and search also appear in the European Conference on Information Retrieval (ECIR), the Conference on Information and Knowl- edge Management (CIKM), and the Web Search and Data Mining Conference (WSDM).  e WSDM conference is a spin-off of the World Wide Web Confer- ence (WWW), which has included some important papers on web search.  e proceedings from the TREC workshops are available online and contain useful descriptions of new research techniques from many different academic and indus- try groups. An overview of the TREC experiments can be found in Voorhees and Harman (2005). An increasing number of search-related papers are beginning to appear in database conferences, such as VLDB and SIGMOD. Occasional papers also show up in language technology conferences, such as ACL and HLT (As- sociation for Computational Linguistics and Human Language Technologies), machine learning conferences, and others.
Exercises
1.1.  ink up and write down a small number of queries for a web search engine. Make sure that the queries vary in length (i.e., they are not all one word). Try to specify exactly what information you are looking for in some of the queries. Run these queries on two commercial web search engines and compare the top 10 results for each query by doing relevance judgments. Write a report that an- swers at least the following questions: What is the precision of the results? What is the overlap between the results for the two search engines? Is one search engine clearly better than the other? If so, by how much? How do short queries perform compared to long queries?
1.2. Site search is another common application of search engines. In this case, search is restricted to the web pages at a given website. Compare site search to web search, vertical search, and enterprise search.
14 http://www.acm.org/dl
 
12 1 SearchEnginesandInformationRetrieval
1.3. Use the Web to  nd as many examples as you can of open source search en- gines, information retrieval systems, or related technology. Give a brief descrip- tion of each search engine and summarize the similarities and differences between them.
1.4. List  ve web services or sites that you use that appear to use search, not includ- ing web search engines. Describe the role of search for that service. Also describe whetherthesearchisbasedonadatabaseorgrepstyleofmatching,orifthesearch is using some type of ranking.
2
Architecture of a Search Engine
“While your  rst question may be the most per- tinent, you may or may not realize it is also the most irrelevant.”
 e Architect, Matrix Reloaded
2.1 What Is an Architecture?
In this chapter, we describe the basic so ware architecture of a search engine. Al- though there is no universal agreement on the de nition, a so ware architecture generally consists of so ware components, the interfaces provided by those com- ponents, and the relationships between them. An architecture is used to describe a system at a particular level of abstraction. An example of an architecture used to provide a standard for integrating search and related language technology compo- nents is UIMA (Unstructured Information Management Architecture).1 UIMA de nes interfaces for components in order to simplify the addition of new tech- nologies into systems that handle text and other unstructured data.
Our search engine architecture is used to present high-level descriptions of the important components of the system and the relationships between them. It is not a code-level description, although some of the components do correspond to so ware modules in the Galago search engine and other systems. We use this architecture in this chapter and throughout the book to provide context to the discussion of speci c techniques.
An architecture is designed to ensure that a system will satisfy the application requirements or goals.  e two primary goals of a search engine are:
• Effectiveness (quality): We want to be able to retrieve the most relevant set of documents possible for a query.
• Efficiency (speed): We want to process queries from users as quickly as possi- ble.
1 http://www.research.ibm.com/UIMA
  
14 2 ArchitectureofaSearchEngine
We may have more speci c goals, too, but usually these fall into the categories of effectiveness or efficiency (or both). For instance, the collection of documents we want to search may be changing; making sure that the search engine immedi- ately reacts to changes in documents is both an effectiveness issue and an efficiency issue.
 e architecture of a search engine is determined by these two requirements. Because we want an efficient system, search engines employ specialized data struc- tures that are optimized for fast retrieval. Because we want high-quality results, search engines carefully process text and store text statistics that help improve the relevance of results.
Many of the components we discuss in the following sections have been used for decades, and this general design has been shown to be a useful compromise between the competing goals of effective and efficient retrieval. In later chapters, we will discuss these components in more detail.
2.2 Basic Building Blocks
Search engine components support two major functions, which we call the index- ing process and the query process.  e indexing process builds the structures that enable searching, and the query process uses those structures and a person’s query to produce a ranked list of documents. Figure 2.1 shows the high-level “building blocks” of the indexing process.  ese major components are text acquisition, text transformation, and index creation.
 e task of the text acquisition component is to identify and make available the documents that will be searched. Although in some cases this will involve sim- ply using an existing collection, text acquisition will more o en require building a collection by crawling or scanning the Web, a corporate intranet, a desktop, or other sources of information. In addition to passing documents to the next com- ponent in the indexing process, the text acquisition component creates a docu- ment data store, which contains the text and metadata for all the documents. Metadata is information about a document that is not part of the text content, such the document type (e.g., email or web page), document structure, and other features, such as document length.
 e text transformation component transforms documents into index terms or features. Index terms, as the name implies, are the parts of a document that are stored in the index and used in searching.  e simplest index term is a word, but not every word may be used for searching. A “feature” is more o en used in
2.2 BasicBuildingBlocks 15
 Document data store
       Text Acquisition
Index Creation
  Email, web pages,
news articles, memos, letters
Index
 Text Transformation
Fig. 2.1.  e indexing process
the  eld of machine learning to refer to a part of a text document that is used to represent its content, which also describes an index term. Examples of other types of index terms or features are phrases, names of people, dates, and links in a web page. Index terms are sometimes simply referred to as “terms.”  e set of all the terms that are indexed for a document collection is called the index vocabulary.
 e index creation component takes the output of the text transformation component and creates the indexes or data structures that enable fast searching. Given the large number of documents in many search applications, index creation must be efficient, both in terms of time and space. Indexes must also be able to be efficiently updated when new documents are acquired. Inverted indexes, or some- times inverted  les, are by far the most common form of index used by search engines. An inverted index, very simply, contains a list for every index term of the documents that contain that index term. It is inverted in the sense of being the opposite of a document  le that lists, for every document, the index terms they contain.  ere are many variations of inverted indexes, and the particular form of index used is one of the most important aspects of a search engine.
Figure 2.2 shows the building blocks of the query process.  e major compo- nents are user interaction, ranking, and evaluation.
 e user interaction component provides the interface between the person doing the searching and the search engine. One task for this component is accept- ing the user’s query and transforming it into index terms. Another task is to take the ranked list of documents from the search engine and organize it into the re-
16 2 ArchitectureofaSearchEngine
 Document data store
        User Interaction
Ranking
    Log data
Evaluation
Fig. 2.2.  e query process
sults shown to the user.  is includes, for example, generating the snippets used to summarize documents.  e document data store is one of the sources of informa- tion used in generating the results. Finally, this component also provides a range of techniques for re ning the query so that it better represents the information need.
 e ranking component is the core of the search engine. It takes the trans- formed query from the user interaction component and generates a ranked list of documents using scores based on a retrieval model. Ranking must be both effi- cient, since many queries may need to be processed in a short time, and effective, since the quality of the ranking determines whether the search engine accom- plishes the goal of  nding relevant information.  e efficiency of ranking depends on the indexes, and the effectiveness depends on the retrieval model.
 e task of the evaluation component is to measure and monitor effectiveness and efficiency. An important part of that is to record and analyze user behavior using log data.  e results of evaluation are used to tune and improve the ranking component. Most of the evaluation component is not part of the online search engine, apart from logging user and system data. Evaluation is primarily an offline activity, but it is a critical part of any search application.
Index
 
2.3 Breaking It Down
We now look in more detail at the components of each of the basic building blocks. Not all of these components will be part of every search engine, but to- gether they cover what we consider to be the most important functions for a broad range of search applications.
2.3.1 Text Acquisition
Crawler
In many applications, the crawler component has the primary responsibility for identifying and acquiring documents for the search engine.  ere are a number of different types of crawlers, but the most common is the general web crawler. A web crawler is designed to follow the links on web pages to discover and download new pages. Although this sounds deceptively simple, there are signi cant challenges in designing a web crawler that can efficiently handle the huge volume of new pages on the Web, while at the same time ensuring that pages that may have changed since the last time a crawler visited a site are kept “fresh” for the search engine. A web crawler can be restricted to a single site, such as a university, as the basis for site search. Focused, or topical, web crawlers use classi cation techniques to restrict the pages that are visited to those that are likely to be about a speci c topic.  is type of crawler may be used by a vertical or topical search application, such as a search engine that provides access to medical information on web pages.
For enterprise search, the crawler is adapted to discover and update all docu- ments and web pages related to a company’s operation. An enterprise document crawler follows links to discover both external and internal (i.e., restricted to the corporate intranet) pages, but also must scan both corporate and personal di- rectories to identify email, word processing documents, presentations, database records, and other company information. Document crawlers are also used for desktop search, although in this case only the user’s personal directories need to be scanned.
Feeds
Document feeds are a mechanism for accessing a real-time stream of documents. For example, a news feed is a constant stream of news stories and updates. In con- trast to a crawler, which must discover new documents, a search engine acquires
2.3 BreakingItDown 17
 Text Acquisition
Crawler
Feeds
Conversion Document data store
18 2 ArchitectureofaSearchEngine
new documents from a feed simply by monitoring it. RSS2 is a common standard used for web feeds for content such as news, blogs, or video. An RSS “reader” is used to subscribe to RSS feeds, which are formatted using XML.3 XML is a language for describing data formats, similar to HTML.4  e reader monitors those feeds and provides new content when it arrives. Radio and television feeds are also used in some search applications, where the “documents” contain auto- matically segmented audio and video streams, together with associated text from closed captions or speech recognition.
Conversion
 e documents found by a crawler or provided by a feed are rarely in plain text. Instead, they come in a variety of formats, such as HTML, XML, Adobe PDF, Microso  WordTM, Microso  PowerPoint®, and so on. Most search engines require that these documents be converted into a consistent text plus metadata format. In this conversion, the control sequences and non-content data associated with a particular format are either removed or recorded as metadata. In the case of HTML and XML, much of this process can be described as part of the text trans- formation component. For other formats, the conversion process is a basic step that prepares the document for further processing. PDF documents, for example, must be converted to text. Various utilities are available that perform this conver- sion, with varying degrees of accuracy. Similarly, utilities are available to convert the various Microso  Office® formats into text.
Another common conversion problem comes from the way text is encoded in a document. ASCII5 is a common standard single-byte character encoding scheme used for text. ASCII uses either 7 or 8 bits (extended ASCII) to represent either 128 or 256 possible characters. Some languages, however, such as Chinese, have many more characters than English and use a number of other encoding schemes. Unicode is a standard encoding scheme that uses 16 bits (typically) to represent most of the world’s languages. Any application that deals with documents in dif- ferent languages has to ensure that they are converted into a consistent encoding scheme before further processing.
2 RSS actually refers to a family of standards with similar names (and the same initials), such as Really Simple Syndication or Rich Site Summary.
3 eXtensible Markup Language
4 HyperText Markup Language
5 American Standard Code for Information Interchange
 
Document data store
 e document data store is a database used to manage large numbers of docu- ments and the structured data that is associated with them.  e document con- tents are typically stored in compressed form for efficiency.  e structured data consists of document metadata and other information extracted from the docu- ments, such as links and anchor text (the text associated with a link). A relational database system can be used to store the documents and metadata. Some applica- tions, however, use a simpler, more efficient storage system to provide very fast retrieval times for very large document stores.
Although the original documents are available on the Web, in the enterprise database, the document data store is necessary to provide fast access to the doc- ument contents for a range of search engine components. Generating summaries of retrieved documents, for example, would take far too long if the search engine had to access the original documents and reprocess them.
2.3.2 Text Transformation
Parser
 e parsing component is responsible for processing the sequence of text tokens in the document to recognize structural elements such as titles,  gures, links, and headings. Tokenizing the text is an important  rst step in this process. In many cases, tokens are the same as words. Both document and query text must be trans- formed into tokens in the same manner so that they can be easily compared.  ere are a number of decisions that potentially affect retrieval that make tokenizing non-trivial. For example, a simple de nition for tokens could be strings of al- phanumeric characters that are separated by spaces.  is does not tell us, however, how to deal with special characters such as capital letters, hyphens, and apostro- phes. Should we treat “apple” the same as “Apple”? Is “on-line” two words or one word? Should the apostrophe in “O’Connor” be treated the same as the one in “owner’s”? In some languages, tokenizing gets even more interesting. Chinese, for example, has no obvious word separator like a space in English.
Document structure is o en speci ed by a markup language such as HTML or XML. HTML is the default language used for specifying the structure of web pages. XML has much more  exibility and is used as a data interchange format for many applications.  e document parser uses knowledge of the syntax of the markup language to identify the structure.
2.3 BreakingItDown 19
 Text Transformation
Parser
Stopping
Stemming
Link Analysis Information Extraction Classifier
20 2 ArchitectureofaSearchEngine
Both HTML and XML use tags to de ne document elements. For example, <h2> Search </h2> de nes “Search” as a second-level heading in HTML. Tags and other control sequences must be treated appropriately when tokenizing. Other types of documents, such as email and presentations, have a speci c syntax and methods for specifying structure, but much of this may be be removed or simpli-  ed by the conversion component.
Stopping
 e stopping component has the simple task of removing common words from the stream of tokens that become index terms.  e most common words are typ- ically function words that help form sentence structure but contribute little on their own to the description of the topics covered by the text. Examples are “the”, “of ”, “to”, and “for”. Because they are so common, removing them can reduce the size of the indexes considerably. Depending on the retrieval model that is used as the basis of the ranking, removing these words usually has no impact on the search engine’s effectiveness, and may even improve it somewhat. Despite these potential advantages, it can be difficult to decide how many words to include on the stop- word list. Some stopword lists used in research contain hundreds of words.  e problem with using such lists is that it becomes impossible to search with queries like “to be or not to be” or “down under”. To avoid this, search applications may use very small stopword lists (perhaps just containing “the”) when processing doc- ument text, but then use longer lists for the default processing of query text.
Stemming
Stemming is another word-level transformation.  e task of the stemming com- ponent (or stemmer) is to group words that are derived from a common stem. Grouping “ sh”, “ shes”, and “ shing” is one example. By replacing each member of a group with one designated word (for example, the shortest, which in this case is “ sh”), we increase the likelihood that words used in queries and documents will match. Stemming, in fact, generally produces small improvements in ranking effectiveness. Similar to stopping, stemming can be done aggressively, conserva- tively, or not at all. Aggressive stemming can cause search problems. It may not be appropriate, for example, to retrieve documents about different varieties of  sh in response to the query “ shing”. Some search applications use more conservative stemming, such as simply identifying plural forms using the letter “s”, or they may
2.3 BreakingItDown 21
do no stemming when processing document text and focus on adding appropriate word variants to the query.
Some languages, such as Arabic, have more complicated morphology than En- glish, and stemming is consequently more important. An effective stemming com- ponent in Arabic has a huge impact on search effectiveness. In contrast, there is little word variation in other languages, such as Chinese, and for these languages stemming is not effective.
Link extraction and analysis
Links and the corresponding anchor text in web pages can readily be identi ed and extracted during document parsing. Extraction means that this information is recorded in the document data store, and can be indexed separately from the general text content. Web search engines make extensive use of this information through link analysis algorithms such as PageRank (Brin & Page, 1998). Link analysis provides the search engine with a rating of the popularity, and to some extent, the authority of a page (in other words, how important it is). Anchor text, which is the clickable text of a web link, can be used to enhance the text content of a page that the link points to.  ese two factors can signi cantly improve the effectiveness of web search for some types of queries.
Information extraction
Information extraction is used to identify index terms that are more complex than single words.  is may be as simple as words in bold or words in headings, but in general may require signi cant additional computation. Extracting syntactic fea- tures such as noun phrases, for example, requires some form of syntactic analysis or part-of-speech tagging. Research in this area has focused on techniques for ex- tracting features with speci c semantic content, such as named entity recognizers, which can reliably identify information such as person names, company names, dates, and locations.
Classifier
 e classi er component identi es class-related metadata for documents or parts of documents.  is covers a range of functions that are o en described separately. Classi cation techniques assign prede ned class labels to documents.  ese labels typically represent topical categories such as “sports”, “politics”, or “business”. Two
22 2 ArchitectureofaSearchEngine
important examples of other types of classi cation are identifying documents as spam, and identifying the non-content parts of documents, such as advertising. Clustering techniques are used to group related documents without prede ned categories.  ese document groups can be used in a variety of ways during ranking or user interaction.
2.3.3 Index Creation
Document statistics
 e task of the document statistics component is simply to gather and record statistical information about words, features, and documents.  is information is used by the ranking component to compute scores for documents.  e types of data generally required are the counts of index term occurrences (both words and more complex features) in individual documents, the positions in the doc- uments where the index terms occurred, the counts of occurrences over groups of documents (such as all documents labeled “sports” or the entire collection of documents), and the lengths of documents in terms of the number of tokens.  e actual data required is determined by the retrieval model and associated rank- ing algorithm.  e document statistics are stored in lookup tables, which are data structures designed for fast retrieval.
Weighting
Index term weights re ect the relative importance of words in documents, and are used in computing scores for ranking.  e speci c form of a weight is deter- mined by the retrieval model.  e weighting component calculates weights using the document statistics and stores them in lookup tables. Weights could be calcu- lated as part of the query process, and some types of weights require information about the query, but by doing as much calculation as possible during the indexing process, the efficiency of the query process will be improved.
One of the most common types used in older retrieval models is known as tf.idf weighting.  ere are many variations of these weights, but they are all based on a combination of the frequency or count of index term occurrences in a document (the term  equency, or tf ) and the frequency of index term occurrence over the entire collection of documents (inverse document  equency, or idf ).  e idf weight is called inverse document frequency because it gives high weights to terms that occur in very few documents. A typical formula for idf is log N /n, where N is the
 Index Creation
Document Statistics Weighting
Inversion Distribution
2.3 BreakingItDown 23
total number of documents indexed by the search engine and n is the number of documents that contain a particular term.
Inversion
 e inversion component is the core of the indexing process. Its task is to change the stream of document-term information coming from the text transformation component into term-document information for the creation of inverted indexes.  e challenge is to do this efficiently, not only for large numbers of documents when the inverted indexes are initially created, but also when the indexes are up- dated with new documents from feeds or crawls.  e format of the inverted in- dexes is designed for fast query processing and depends to some extent on the ranking algorithm used.  e indexes are also compressed to further enhance effi- ciency.
Index distribution
 e index distribution component distributes indexes across multiple computers and potentially across multiple sites on a network. Distribution is essential for efficient performance with web search engines. By distributing the indexes for a subset of the documents (document distribution), both indexing and query pro- cessing can be done in parallel. Distributing the indexes for a subset of terms (term distribution) can also support parallel processing of queries. Replication is a form of distribution where copies of indexes or parts of indexes are stored in multiple sites so that query processing can be made more efficient by reducing communi- cation delays. Peer-to-peer search involves a less organized form of distribution where each node in a network maintains its own indexes and collection of docu- ments.
2.3.4 User Interaction
Query input
 e query input component provides an interface and a parser for a query lan- guage.  e simplest query languages, such as those used in most web search in- terfaces, have only a small number of operators. An operator is a command in the query language that is used to indicate text that should be treated in a special way. In general, operators help to clarify the meaning of the query by constraining how
 User Interaction
Query input
Query transformation Results Output
24 2 ArchitectureofaSearchEngine
text in the document can match text in the query. An example of an operator in a simple query language is the use of quotes to indicate that the enclosed words should occur as a phrase in the document, rather than as individual words with no relationship. A typical web query, however, consists of a small number of keywords with no operators. A keyword is simply a word that is important for specifying the topic of a query. Because the ranking algorithms for most web search engines are designed for keyword queries, longer queries that may contain a lower proportion of keywords typically do not work well. For example, the query “search engines” may produce a better result with a web search engine than the query “what are typical implementation techniques and data structures used in search engines”. One of the challenges for search engine design is to give good results for a range of queries, and better results for more speci c queries.
More complex query languages are available, either for people who want to have a lot of control over the search results or for applications using a search en- gine. In the same way that the SQL query language (Elmasri & Navathe, 2006) is not designed for the typical user of a database application (the end user), these query languages are not designed for the end users of search applications. Boolean query languages have a long history in information retrieval.  e operators in this languageincludeBooleanAND,OR,andNOT,andsomeformofproximityopera- tor that speci es that words must occur together within a speci c distance (usually in terms of word count). Other query languages include these and other operators in a probabilistic framework designed to allow speci cation of features related to both document structure and content.
Query transformation
 e query transformation component includes a range of techniques that are de- signed to improve the initial query, both before and a er producing a document ranking.  e simplest processing involves some of the same text transformation techniquesusedondocumenttext.Tokenizing,stopping,andstemmingmustbe done on the query text to produce index terms that are comparable to the docu- ment terms.
Spell checking and query suggestion are query transformation techniques that produce similar output. In both cases, the user is presented with alternatives to the initial query that are likely to either correct spelling errors or be more spe- ci c descriptions of their information needs.  ese techniques o en leverage the extensive query logs collected for web applications. Query expansion techniques
2.3 BreakingItDown 25
also suggest or add additional terms to the query, but usually based on an analy- sis of term occurrences in documents.  is analysis may use different sources of information, such as the whole document collection, the retrieved documents, or documents on the user’s computer. Relevance feedback is a technique that expands queries based on term occurrences in documents that are identi ed as relevant by the user.
Results output
 e results output component is responsible for constructing the display of ranked documents coming from the ranking component.  is may include tasks such as generating snippets to summarize the retrieved documents, highlighting impor- tant words and passages in documents, clustering the output to identify related groups of documents, and  nding appropriate advertising to add to the results display. In applications that involve documents in multiple languages, the results may be translated into a common language.
2.3.5 Ranking
Scoring
 e scoring component, also called query processing, calculates scores for docu- ments using the ranking algorithm, which is based on a retrieval model.  e de- signers of some search engines explicitly state the retrieval model they use. For other search engines, only the ranking algorithm is discussed (if any details at all are revealed), but all ranking algorithms are based implicitly on a retrieval model.  e features and weights used in a ranking algorithm, which may have been de- rived empirically (by testing and evaluation), must be related to topical and user relevance, or the search engine would not work.
Many different retrieval models and methods of deriving ranking algorithms have been proposed.  e basic form of the document score calculated by many of
 these models is
∑
qi .di i
where the summation is over all of the terms in the vocabulary of the collection, qi is the query term weight of the ith term, and di is the document term weight.  e term weights depend on the particular retrieval model being used, but are generallysimilartotf.idf weights.InChapter7,wediscusstherankingalgorithms
Ranking
Scoring Optimization Distribution
26 2 ArchitectureofaSearchEngine
based on the BM25 and query likelihood retrieval models (as well as others) in more detail.
 e document scores must be calculated and compared very rapidly in order to determine the ranked order of the documents that are given to the results output component.  is is the task of the performance optimization component.
Performance optimization
Performance optimization involves the design of ranking algorithms and the as- sociated indexes to decrease response time and increase query throughput. Given a particular form of document scoring, there are a number of ways to calculate those scores and produce the ranked document output. For example, scores can be computed by accessing the index for a query term, computing the contribution for that term to a document’s score, adding this contribution to a score accumula- tor, and then accessing the next index.  is is referred to as term-at-a-time scoring. Another alternative is to access all the indexes for the query terms simultaneously, and compute scores by moving pointers through the indexes to  nd the terms present in a document. In this document-at-a-time scoring, the  nal document score is calculated immediately instead of being accumulated one term at a time. In both cases, further optimizations are possible that signi cantly decrease the time required to compute the top-ranked documents. Safe optimizations guaran- tee that the scores calculated will be the same as the scores without optimization. Unsafe optimizations, which do not have this property, can in some cases be faster, so it is important to carefully evaluate the impact of the optimization.
Distribution
Given some form of index distribution, ranking can also be distributed. A query broker decides how to allocate queries to processors in a network and is responsi- ble for assembling the  nal ranked list for the query.  e operation of the broker depends on the form of index distribution. Caching is another form of distribu- tion where indexes or even ranked document lists from previous queries are le  in local memory. If the query or index term is popular, there is a signi cant chance that this information can be reused with substantial time savings.
2.3.6 Evaluation
Logging
Logs of the users’ queries and their interactions with the search engine are one of the most valuable sources of information for tuning and improving search ef- fectiveness and efficiency. Query logs can be used for spell checking, query sug- gestions, query caching, and other tasks, such as helping to match advertising to searches. Documents in a result list that are clicked on and browsed tend to be relevant.  is means that logs of user clicks on documents (clickthrough data) and information such as the dwell time (time spent looking at a document) can be used to evaluate and train ranking algorithms.
Ranking analysis
Given either log data or explicit relevance judgments for a large number of (query, document) pairs, the effectiveness of a ranking algorithm can be measured and compared to alternatives.  is is a critical part of improving a search engine and selecting values for parameters that are appropriate for the application. A variety of evaluation measures are commonly used, and these should also be selected to measure outcomes that make sense for the application. Measures that emphasize the quality of the top-ranked documents, rather than the whole list, for example, are appropriate for many types of web queries.
Performance analysis
 e performance analysis component involves monitoring and improving overall system performance, in the same way that the ranking analysis component mon- itors effectiveness. A variety of performance measures are used, such as response time and throughput, but the measures used also depend on the application. For example, a distributed search application should monitor network usage and ef-  ciency in addition to other measures. For ranking analysis, test collections are o en used to provide a controlled experimental environment.  e equivalent for performance analysis is simulations, where actual networks, processors, storage devices, and data are replaced with mathematical models that can be adjusted us- ing parameters.
2.3 BreakingItDown 27
 Evaluation
Logging
Ranking Analysis Performance Analysis
28 2 ArchitectureofaSearchEngine
2.4 How Does It Really Work?
Now you know the names and the basic functions of the components of a search engine, but we haven’t said much yet about how these components actually per- form these functions.  at’s what the rest of the book is about. Each chapter de- scribes, in depth, how one or more components work. If you still don’t understand a component a er  nishing the appropriate chapter, you can study the Galago code, which is one implementation of the ideas presented, or the references de- scribed at the end of each chapter.
References and Further Reading
Detailed references on the techniques and models mentioned in the compo- nent descriptions will be given in the appropriate chapters.  ere are a few gen- eral references for search architectures. A database textbook, such as Elmasri and Navathe (2006), provides descriptions of database system architecture and the associated query languages that are interesting to compare with the search en- gine architecture discussed here.  ere are some similarities at the high level, but database systems focus on structured data and exact match rather than on text and ranking, so most of the components are very different.
 e classic research paper on web search engine architecture, which gives an overview of an early version of Google, is Brin and Page (1998). Another system overview for an earlier general-purpose search engine (Inquery) is found in Callan et al. (1992). A comprehensive description of the Lucene architecture and com- ponents can be found in Hatcher and Gospodnetic (2004).
Exercises
2.1. Find some examples of the search engine components described in this chap- ter in the Galago code.
2.2. A more-like-this query occurs when the user can click on a particular docu- ment in the result list and tell the search engine to  nd documents that are similar to this one. Describe which low-level components are used to answer this type of query and the sequence in which they are used.
2.4 HowDoesItReallyWork? 29
2.3. Document  ltering is an application that stores a large number of queries or user pro les and compares these pro les to every incoming document on a feed. Documents that are sufficiently similar to the pro le are forwarded to that person via email or some other mechanism. Describe the architecture of a  ltering engine and how it may differ from a search engine.
3
Crawls and Feeds
“You’ve stuck your webs into my business for the last time.”
Doc Ock, Spider Man 2
3.1 Deciding What to Search
 is book is about the details of building a search engine, from the mathematics behind ranking to the algorithms of query processing. Although we focus heav- ily on the technology that makes search engines work, and great technology can make a good search engine even better, it is the information in the document col- lection that makes search engines useful. In other words, if the right documents are not stored in the search engine, no search technique will be able to  nd rele- vant information.
 e title of this section implies the question, “What should we search?”  e simple answer is everything you possibly can. Every document answers at least one question (i.e., “Now where was that document again?”), although the best doc- uments answer many more. Every time a search engine adds another document, the number of questions it can answer increases. On the other hand, adding many poor-quality documents increases the burden on the ranking process to  nd only the best documents to show to the user. Web search engines, however, show how successful search engines can be, even when they contain billions of low-quality documents with little useful content.
Even useful documents can become less useful over time.  is is especially true of news and  nancial information where, for example, many people want to know about today’s stock market report, but only a few care about what happened yes- terday.  e frustration of  nding out-of-date web pages and links in a search re- sult list is, unfortunately, a common experience. Search engines are most effective when they contain the most recent information in addition to archives of older material.
 
32 3 CrawlsandFeeds
 is chapter introduces techniques for  nding documents to search, whether on the Web, on a  le server, on a computer’s hard disk, or in an email program. We will discuss strategies for storing documents and keeping those documents up-to-date. Along the way, we will discuss how to pull data out of  les, navigating through issues of character encodings, obsolete  le formats, duplicate documents, and textual noise. By the end of this chapter you will have a solid grasp on how to get document data into a search engine, ready to be indexed.
3.2 Crawling the Web
Tobuildasearchenginethatsearcheswebpages,you rstneedacopyofthepages that you want to search. Unlike some of the other sources of text we will consider later, web pages are particularly easy to copy, since they are meant to be retrieved over the Internet by browsers.  is instantly solves one of the major problems of getting information to search, which is how to get the data from the place it is stored to the search engine.
Finding and downloading web pages automatically is called crawling, and a program that downloads pages is called a web crawler.1  ere are some unique challenges to crawling web pages.  e biggest problem is the sheer scale of the Web.  ere are at least tens of billions of pages on the Internet.  e “at least” in the last sentence is there because nobody is sure how many pages there are. Even if the number of pages in existence today could be measured exactly, that number would be immediately wrong, because pages are constantly being created. Every time a user adds a new blog post or uploads a photo, another web page is created. Most organizations do not have enough storage space to store even a large fraction of the Web, but web search providers with plenty of resources must still constantly download new content to keep their collections current.
Another problem is that web pages are usually not under the control of the people building the search engine database. Even if you know that you want to copy all the pages from www.company.com, there is no easy way to  nd out how many pages there are on the site.  e owners of that site may not want you to copy some of the data, and will probably be angry if you try to copy it too quickly or too frequently. Some of the data you want to copy may be available only by typing a request into a form, which is a difficult process to automate.
1 Crawling is also occasionally referred to as spidering, and a crawler is sometimes called a spider.
 
3.2.1 Retrieving Web Pages
Each web page on the Internet has its own unique uniform resource locator, or URL. Any URL used to describe a web page has three parts: the scheme, the host- name, and the resource name (Figure 3.1). Web pages are stored on web servers, which use a protocol called Hypertext Transfer Protocol, or HTTP, to exchange information with client so ware.  erefore, most URLs used on the Web start with the scheme http, indicating that the URL represents a resource that can be retrieved using HTTP.  e hostname follows, which is the name of the com- puter that is running the web server that holds this web page. In the  gure, the computer’s name is www.cs.umass.edu, which is a computer in the University of Massachusetts Computer Science department.  is URL refers to a page on that computer called /csinfo/people.html.
http://www.cs.umass.edu/csinfo/people.html
http www.cs.umass.edu /csinfo/people.html
scheme hostname resource
Fig. 3.1. A uniform resource locator (URL), split into three parts
Webbrowsersandwebcrawlersaretwodifferentkindsofwebclients,butboth fetch web pages in the same way. First, the client program connects to a domain name system (DNS) server.  e DNS server translates the hostname into an inter- net protocol (IP) address.  is IP address is a number that is typically 32 bits long, but some networks now use 128-bit IP addresses.  e program then attempts to connect to a server computer with that IP address. Since that server might have many different programs running on it, with each one listening to the network for new connections, each program listens on a different port. A port is just a 16-bit number that identi es a particular service. By convention, requests for web pages are sent to port 80 unless speci ed otherwise in the URL.
Once the connection is established, the client program sends an HTTP re- quest to the web server to request a page.  e most common HTTP request type is a GET request, for example:
                  GET /csinfo/people.html HTTP/1.0
 is simple request asks the server to send the page called /csinfo/people.html back to the client, using version 1.0 of the HTTP protocol speci cation. A er
3.2 CrawlingtheWeb 33
   
34 3 CrawlsandFeeds
sending a short header, the server sends the contents of that  le back to the client. If the client wants more pages, it can send additional requests; otherwise, the client closes the connection.
A client can also fetch web pages using POST requests. A POST request is like a GET request, except that it can send additional request information to the server. By convention, GET requests are used for retrieving data that already exists on the server, whereas POST requests are used to tell the server something. A POST request might be used when you click a button to purchase something or to edit a web page.  is convention is useful if you are running a web crawler, since sending only GET requests helps make sure your crawler does not inadvertently order a product.
crawler.searchengine.com
Fig. 3.2. Crawling the Web.  e web crawler connects to web servers to  nd pages. Pages may link to other pages on the same server or on different servers.
   /index.html /courses
/news
          /index.html
/today
     /2005/story.html
 /index.html /news.html
/about.html
     www.bbc.co.uk
   www.whitehouse.gov
      www.cs.umass.edu
/index.html /2006/09/story.html
/2003/04/story.html
          www.cnn.com
    
3.2.2 The Web Crawler
Figure 3.2 shows a diagram of the Web from a simple web crawler’s perspective.  e web crawler has two jobs: downloading pages and  nding URLs.
 e crawler starts with a set of seeds, which are a set of URLs given to it as parameters.  ese seeds are added to a URL request queue.  e crawler starts fetching pages from the request queue. Once a page is downloaded, it is parsed to  nd link tags that might contain other useful URLs to fetch. If the crawler  nds a new URL that it has not seen before, it is added to the crawler’s request queue, or  ontier.  e frontier may be a standard queue, or it may be ordered so that important pages move to the front of the list.  is process continues until the crawler either runs out of disk space to store pages or runs out of useful links to add to the request queue.
If a crawler used only a single thread, it would not be very efficient. Notice that the web crawler spends a lot of its time waiting for responses: it waits for the DNS server response, then it waits for the connection to the web server to be acknowledged, and then it waits for the web page data to be sent from the server. During this waiting time, the CPU of the web crawler machine is idle and the network connection is unused. To reduce this inefficiency, web crawlers use threads and fetch hundreds of pages at once.
Fetching hundreds of pages at once is good for the person running the web crawler, but not necessarily good for the person running the web server on the other end. Just imagine how the request queue works in practice. When a web page like www.company.com is fetched, it is parsed and all of the links on that page are added to the request queue.  e crawler will then attempt to fetch all of those pages at once. If the web server for www.company.com is not very powerful, it might spend all of its time handling requests from the crawler instead of handling requests from real users.  is kind of behavior from web crawlers tends to make web server administrators very angry.
To avoid this problem, web crawlers use politeness policies. Reasonable web crawlers do not fetch more than one page at a time from a particular web server. In addition, web crawlers wait at least a few seconds, and sometimes minutes, be- tween requests to the same web server.  is allows web servers to spend the bulk of their time processing real user requests. To support this, the request queue is logically split into a single queue per web server. At any one time, most of these per-server queues are off-limits for crawling, because the crawler has fetched a page from that server recently.  e crawler is free to read page requests only from queues that haven’t been accessed within the speci ed politeness window.
3.2 CrawlingtheWeb 35
36 3 CrawlsandFeeds
When using a politeness window, the request queue must be very large in order to achieve good performance. Suppose a web crawler can fetch 100 pages each second, and that its politeness policy dictates that it cannot fetch more than one page each 30 seconds from a particular web server.  e web crawler needs to have URLs from at least 3,000 different web servers in its request queue in order to achieve high throughput. Since many URLs will come from the same servers, the request queue needs to have tens of thousands of URLs in it before a crawler can reach its peak throughput.
                 User-agent: *
                 Disallow: /private/
                 Disallow: /confidential/
                 Disallow: /other/
                 Allow: /other/public/
                 User-agent: FavoredCrawler
                 Disallow:
                 Sitemap: http://mysite.com/sitemap.xml.gz
Fig. 3.3. An example robots.txt  le
Even crawling a site slowly will anger some web server administrators who ob- ject to any copying of their data. Web server administrators who feel this way can store a  le called /robots.txt on their web servers. Figure 3.3 contains an ex- ample robots.txt  le.  e  le is split into blocks of commands that start with a User-agent: speci cation.  e User-agent: line identi es a crawler, or group of crawlers, affected by the following rules. Following this line are Allow and Disallow rules that dictate which resources the crawler is allowed to access. In the  gure, the  rst block indicates that all crawlers need to ignore resources that begin with /private/, /confidential/, or /other/, except for those that begin with /other/public/.  e second block indicates that a crawler named Favored- Crawler gets its own set of rules: it is allowed to copy everything.
 e  nal block of the example is an optional Sitemap: directive, which will be discussed later in this section.
Figure 3.4 shows an implementation of a crawling thread, using the crawler building blocks we have seen so far. Assume that the frontier has been initialized
procedure CT(frontier) while not frontier.done() do
website ← frontier.nextSite()
url ← website.nextURL()
if website.permitsCrawl(url) then
text ← retrieveURL(url) storeDocument(url, text) for each url in parse(text) do
frontier.addURL(url)
end for end if
frontier.releaseSite(website)
end while end procedure
Fig. 3.4. A simple crawling thread implementation
with a few URLs that act as seeds for the crawl.  e crawling thread  rst retrieves a website from the frontier.  e crawler then identi es the next URL in the web- site’s queue. In permitsCrawl, the crawler checks to see if the URL is okay to crawl according to the website’s robots.txt  le. If it can be crawled, the crawler uses re- trieveURL to fetch the document contents.  is is the most expensive part of the loop, and the crawler thread may block here for many seconds. Once the text has been retrieved, storeDocument stores the document text in a document database (discussed later in this chapter).  e document text is then parsed so that other URLs can be found.  ese URLs are added to the frontier, which adds them to the appropriate website queues. When all this is  nished, the website object is returned to the frontier, which takes care to enforce its politeness policy by not giving the website to another crawler thread until an appropriate amount of time has passed. In a real crawler, the timer would start immediately a er the document was retrieved, since parsing and storing the document could take a long time.
3.2.3 Freshness
Web pages are constantly being added, deleted, and modi ed. To keep an accu- rate view of the Web, a web crawler must continually revisit pages it has already crawled to see if they have changed in order to maintain the  eshness of the docu- ment collection.  e opposite of a fresh copy is a stale copy, which means a copy that no longer re ects the real content of the web page.
3.2 CrawlingtheWeb 37
38
3 CrawlsandFeeds
Client request:
Server response:
HEAD /csinfo/people.html HTTP/1.1
Host: www.cs.umass.edu
 HTTP/1.1 200 OK
 Date: Thu, 03 Apr 2008 05:17:54 GMT
 Server: Apache/2.0.52 (CentOS)
 Last-Modified: Fri, 04 Jan 2008 15:28:39 GMT
 ETag: "239c33-2576-2a2837c0"
 Accept-Ranges: bytes
 Content-Length: 9590
 Connection: close
 Content-Type: text/html; charset=ISO-8859-1
Fig. 3.5. An HTTP HEAD request and server response
 e HTTP protocol has a special request type called HEAD that makes it easy to check for page changes.  e HEAD request returns only header information about the page, but not the page itself. Figure 3.5 contains an example HEAD request and response.  e Last-Modified value indicates the last time the page content was changed. Notice that the date is also sent along with the response, as well as in response to a GET request.  is allows the web crawler to compare the date it received from a previous GET request with the Last-Modified value from a HEAD request.
A HEAD request reduces the cost of checking on a page, but does not elimi- nate it. It simply is not possible to check every page every minute. Not only would that attract more negative reactions from web server administrators, but it would cause enormous load on the web crawler and the incoming network connection.
 ankfully, most web pages are not updated every few minutes. Some of them, like news websites, do change frequently. Others, like a person’s home page, change much less o en. Even within a page type there can be huge variations in the modi cation rate. For example, some blogs are updated many times a day, whereas others go months between updates. It does little good to continuously check sites that are rarely updated.  erefore, one of the crawler’s jobs is to measure the rate at which each page changes. Over time, this data can be used to estimate how frequently each page changes.
Given that a web crawler can’t update every page immediately as it changes, the crawler needs to have some metric for measuring crawl freshness. In this chapter, we’ve used freshness as a general term, but freshness is also the name of a metric.
3.2 CrawlingtheWeb 39
        freshness
age
    crawl updates crawl update crawl
Fig. 3.6. Age and freshness of a single page over time
Under the  eshness metric, a page is  esh if the crawl has the most recent copy of a web page, but stale otherwise. Freshness is then the fraction of the crawled pages that are currently fresh.
Keeping freshness high seems like exactly what you’d want to do, but optimiz- ing for freshness can have unintended consequences. Suppose that http://www.ex- ample.com is a popular website that changes its front page slightly every minute. Unless your crawler continually polls http://www.example.com, you will almost al- ways have a stale copy of that page. Notice that if you want to optimize for fresh- ness, the appropriate strategy is to stop crawling this site completely! If it will never be fresh, it can’t help your freshness value. Instead, you should allocate your crawler’s resources to pages that change less frequently.
Of course, users will revolt if you decide to optimize your crawler for freshness.  ey will look at http://www.example.com and wonder why your indexed copy is months out of date.
Age is a better metric to use. You can see the difference between age and fresh- ness in Figure 3.6. In the top part of the  gure, you can see that pages become fresh immediately when they are crawled, but once the page changes, the crawled page becomes stale. Under the age metric, the page has age 0 until it is changed, and then its age grows until the page is crawled again.
Suppose we have a page with change frequency λ, meaning that we expect it to change λ times in a one-day period. We can calculate the expected age of a page t days a er it was last crawled:
∫t 0
Age(λ, t) =
P (page changed at time x)(t − x)dx
                     40 3 CrawlsandFeeds
                                     Fig. 3.7. Expected age of a page with mean change frequency λ = 1/7 (one week)
 e (t − x) expression is an age: we assume the page is crawled at time t, but that it changed at time x. We multiply that by the probability that the page actu- ally changed at time x. Studies have shown that, on average, web page updates fol- low the Poisson distribution, meaning that the time until the next update is gov- erned by an exponential distribution (Cho & Garcia-Molina, 2003).  is gives us a formula to plug into the P (page changed at time x) expression:
∫t
0
Figure 3.7 shows the result of plotting this expression for a  xed λ = 1/7, indicating roughly one change a week. Notice how the expected age starts at zero, and rises slowly at  rst.  is is because the page is unlikely to have changed in the  rst day. As the days go by, the probability that the page has changed increases. By the end of the week, the expected age of the page is about 2.6 days.  is means that if your crawler crawls each page once a week, and each page in your collection has a mean update time of once a week, the pages in your index will be 2.6 days old on average just before the crawler runs again.
Notice that the second derivative of the Age function is always positive.  at is, the graph is not only increasing, but its rate of increase is always increasing.  is positive second derivative means that the older a page gets, the more it costs you to not crawl it. Optimizing this metric will never result in the conclusion that optimizing for freshness does, where sometimes it is economical to not crawl a page at all.
Age(λ, t) =
λe−λx(t − x)dx
3.2.4 Focused Crawling
Some users would like a search engine that focuses on a speci c topic of informa- tion. For instance, at a website about movies, users might want access to a search engine that leads to more information about movies. If built correctly, this type of vertical search can provide higher accuracy than general search because of the lack of extraneous information in the document collection.  e computational cost of running a vertical search will also be much less than a full web search, simply because the collection will be much smaller.
 e most accurate way to get web pages for this kind of engine would be to crawlafullcopyoftheWebandthenthrowoutallunrelatedpages. isstrategy requires a huge amount of disk space and bandwidth, and most of the web pages will be discarded at the end.
A less expensive approach is focused, or topical, crawling. A focused crawler attempts to download only those pages that are about a particular topic. Focused crawlers rely on the fact that pages about a topic tend to have links to other pages on the same topic. If this were perfectly true, it would be possible to start a crawl at one on-topic page, then crawl all pages on that topic just by following links from a single root page. In practice, a number of popular pages for a speci c topic are typically used as seeds.
Focused crawlers require some automatic means for determining whether a page is about a particular topic. Chapter 9 will introduce text classi ers, which are tools that can make this kind of distinction. Once a page is downloaded, the crawler uses the classi er to decide whether the page is on topic. If it is, the page is kept, and links from the page are used to  nd other related sites.  e anchor text in the outgoing links is an important clue of topicality. Also, some pages have more on-topic links than others. As links from a particular web page are visited, the crawler can keep track of the topicality of the downloaded pages and use this to determine whether to download other similar pages. Anchor text data and page link topicality data can be combined together in order to determine which pages should be crawled next.
3.2.5 Deep Web
Not all parts of the Web are easy for a crawler to navigate. Sites that are difficult for a crawler to  nd are collectively referred to as the deep Web (also called the hidden Web). Some studies have estimated that the deep Web is over a hundred
3.2 CrawlingtheWeb 41
42 3 CrawlsandFeeds
times larger than the traditionally indexed Web, although it is very difficult to measure this accurately.
Most sites that are a part of the deep Web fall into three broad categories:
• Private sites are intentionally private.  ey may have no incoming links, or may require you to log in with a valid account before using the rest of the site.  ese sites generally want to block access from crawlers, although some news pub- lishers may still want their content indexed by major search engines.
• Form results are sites that can be reached only a er entering some data into a form. For example, websites selling airline tickets typically ask for trip infor- mation on the site’s entry page. You are shown  ight information only a er submitting this trip information. Even though you might want to use a search engine to  nd  ight timetables, most crawlers will not be able to get through this form to get to the timetable information.
• Scripted pages are pages that use JavaScriptTM, Flash®, or another client-side lan- guage in the web page. If a link is not in the raw HTML source of the web page, but is instead generated by JavaScript code running on the browser, the crawler will need to execute the JavaScript on the page in order to  nd the link. Although this is technically possible, executing JavaScript can slow down the crawler signi cantly and adds complexity to the system.
Sometimes people make a distinction between static pages and dynamic pages. Static pages are  les stored on a web server and displayed in a web browser un- modi ed, whereas dynamic pages may be the result of code executing on the web server or the client. Typically it is assumed that static pages are easy to crawl, while dynamic pages are hard.  is is not quite true, however. Many websites have dy- namically generated web pages that are easy to crawl; wikis are a good example of this. Other websites have static pages that are impossible to crawl because they
can be accessed only through web forms.
Web administrators of sites with form results and scripted pages o en want
their sites to be indexed, unlike the owners of private sites. Of these two categories, scripted pages are easiest to deal with.  e site owner can usually modify the pages slightly so that links are generated by code on the server instead of by code in the browser.  e crawler can also run page JavaScript, or perhaps Flash as well, although these can take a lot of time.
 e most difficult problems come with form results. Usually these sites are repositories of changing data, and the form submits a query to a database system. In the case where the database contains millions of records, the site would need to
3.2 CrawlingtheWeb 43
expose millions of links to a search engine’s crawler. Adding a million links to the front page of such a site is clearly infeasible. Another option is to let the crawler guess what to enter into forms, but it is difficult to choose good form input. Even with good guesses, this approach is unlikely to expose all of the hidden data.
3.2.6 Sitemaps
As you can see from the last two sections, the biggest problems in crawling arise because site owners cannot adequately tell crawlers about their sites. In section 3.2.3, we saw how crawlers have to make guesses about when pages will be updated because polling is costly. In section 3.2.5, we saw that site owners sometimes have data that they would like to expose to a search engine, but they can’t because there is no reasonable place to store the links. Sitemaps solve both of these problems.
           <?xml version="1.0" encoding="UTF-8"?>
           <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
             <url>
               <loc>http://www.company.com/</loc>
               <lastmod>2008-01-15</lastmod>
               <changefreq>monthly</changefreq>
               <priority>0.7</priority>
</url> <url>
               <loc>http://www.company.com/items?item=truck</loc>
               <changefreq>weekly</changefreq>
             </url>
             <url>
               <loc>http://www.company.com/items?item=bicycle</loc>
               <changefreq>daily</changefreq>
             </url>
           </urlset>
Fig. 3.8. An example sitemap  le
A robots.txt  le can contain a reference to a sitemap, like the one shown in Figure 3.8. A sitemap contains a list of URLs and data about those URLs, such as modi cation time and modi cation frequency.
44 3 CrawlsandFeeds
 ere are three URL entries shown in the example sitemap. Each one contains a URL in a loc tag.  e changefreq tag indicates how o en this resource is likely to change.  e  rst entry includes a lastmod tag, which indicates the last time it was changed.  e  rst entry also includes a priority tag with a value of 0.7, which is higher than the default of 0.5.  is tells crawlers that this page is more important than other pages on this site.
Why would a web server administrator go to the trouble to create a sitemap? One reason is that it tells search engines about pages it might not otherwise  nd. Look at the second and third URLs in the sitemap. Suppose these are two prod- uct pages.  ere may not be any links on the website to these pages; instead, the user may have to use a search form to get to them. A simple web crawler will not attempt to enter anything into a form (although some advanced crawlers do), and so these pages would be invisible to search engines. A sitemap allows crawlers to  nd this hidden content.
 e sitemap also exposes modi cation times. In the discussion of page fresh- ness, we mentioned that a crawler usually has to guess when pages are likely to change.  e changefreq tag gives the crawler a hint about when to check a page again for changes, and the lastmod tag tells the crawler when a page has changed.  is helps reduce the number of requests that the crawler sends to a website with- out sacri cing page freshness.
3.2.7 Distributed Crawling
For crawling individual websites, a single computer is sufficient. However, crawl- ing the entire Web requires many computers devoted to crawling. Why would a single crawling computer not be enough? We will consider three reasons.
One reason to use multiple computers is to put the crawler closer to the sites it crawls. Long-distance network connections tend to have lower throughput (fewer bytes copied per second) and higher latency (bytes take longer to cross the net- work). Decreased throughput and increased latency work together to make each page request take longer. As throughput drops and latency rises, the crawler has to open more connections to copy pages at the same rate.
For example, suppose a crawler has a network connection that can transfer 1MB each second. With an average web page size of 20K, it can copy 50 pages each second. If the sites that are being crawled are close, the data transfer rate from them may be 1MB a second. However, it can take 80ms for the site to start sending data, because there is some transmission delay in opening the connection
3.2 CrawlingtheWeb 45
and sending the request. Let’s assume each request takes 100ms (80ms of latency and 20ms of data transfer). Multiplying 50 by 100ms, we see that there is 5 seconds of waiting involved in transferring 50 pages.  is means that  ve connections will be needed to transfer 50 pages in one second. If the sites are farther away, with an average throughput of 100K per second and 500ms of latency, then each request would now take 600ms. Since 50 × 600ms = 30 seconds, the crawler would need to keep 30 connections open to transfer pages at the same rate.
Another reason for multiple crawling computers is to reduce the number of sites the crawler has to remember. A crawler has to remember all of the URLs it has already crawled, and all of the URLs that it has queued to crawl.  ese URLs must be easy to access, because every page that is crawled contains new links that need to be added to the crawl queue. Since the crawler’s queue should not contain duplicates or sites that have already been crawled, each new URL must be checked against everything in the queue and everything that has been crawled.  e data structure for this lookup needs to be in RAM; otherwise, the computer’s crawl speed will be severely limited. Spreading crawling duties among many computers reduces this bookkeeping load.
Yet another reason is that crawling can use a lot of computing resources, includ- ing CPU resources for parsing and network bandwidth for crawling pages. Crawl- ing a large portion of the Web is too much work for a single computer to handle.
A distributed crawler is much like a crawler on a single computer, except in- stead of a single queue of URLs, there are many queues.  e distributed crawler uses a hash function to assign URLs to crawling computers. When a crawler sees a new URL, it computes a hash function on that URL to decide which crawl- ing computer is responsible for it.  ese URLs are gathered in batches, then sent periodically to reduce the network overhead of sending a single URL at a time.
 e hash function should be computed on just the host part of each URL.  is assigns all the URLs for a particular host to a single crawler. Although this may promote imbalance since some hosts have more pages than others, politeness rules require a time delay between URL fetches to the same host. It is easier to maintain that kind of delay by using the same crawling computers for all URLs for the same host. In addition, we would expect that sites from domain.com will have lots of links to other pages on domain.com. By assigning domain.com to a single crawl host, we minimize the number of URLs that need to be exchanged between crawling computers.
46 3 CrawlsandFeeds
3.3 Crawling Documents and Email
Even though the Web is a tremendous information resource, a huge amount of digital information is not stored on websites. In this section, we will consider in- formation that you might  nd on a normal desktop computer, such as email, word processing documents, presentations, or spreadsheets.  is information can be searched using a desktop search tool. In companies and organizations, enterprise search will make use of documents on  le servers, or even on employee desktop computers, in addition to local web pages.
Many of the problems of web crawling change when we look at desktop data. In web crawling, just  nding the data can be a struggle. On a desktop computer, the interesting data is stored in a  le system with familiar semantics. Finding all the  les on a hard disk is not particularly difficult, since  le systems have directo- ries that are easy to discover. In some ways, a  le system is like a web server, but with an automatically generated sitemap.
 ere are unique challenges in crawling desktop data, however.  e  rst con- cerns update speed. In desktop search applications, users demand search results based on the current content of their  les.  is means, for example, being able to search for an email the instant it is received, and being able to search for a docu- ment as soon as it has been saved. Notice that this is a much different expectation than with web search, where users can tolerate crawling delays of hours or days. Crawling the  le system every second is impractical, but modern  le systems can send change noti cations directly to the crawler process so that it can copy new  les immediately. Remote  le systems from  le servers usually do not provide this kind of change noti cation, and so they must be crawled just like a web server.
Disk space is another concern. With a web crawler, we assume that we need to keep a copy of every document that is found.  is is less true on a desktop system, where the documents are already stored locally, and where users will be unhappy if a large proportion of the hard disk is taken by the indexer. A desktop crawler instead may need to read documents into memory and send them directly to the indexer. We will discuss indexing more in Chapter 5.
Since websites are meant to be viewed with web browsers, most web content is stored in HTML. On the other hand, each desktop program—the word pro- cessor, presentation tool, email program, etc.—has its own  le format. So, just  nding these  les is not enough; eventually they will need to be converted into a format that the indexer can understand. In section 3.5 we will revisit this conver- sion issue.
3.4 DocumentFeeds 47
Finally, and perhaps most importantly, crawling desktop data requires a focus on data privacy. Desktop systems can have multiple users with different accounts, and user A should not be able to  nd emails from user B’s account through the search feature.  is is especially important when we consider crawling shared net- work  le systems, as in a corporate network.  e  le access permissions of each  le must be recorded along with the crawled data, and must be kept up-to-date.
3.4 Document Feeds
In general Web or desktop crawling, we assume that any document can be created or modi ed at any time. However, many documents are published, meaning that they are created at a  xed time and rarely updated again. News articles, blog posts, press releases, and email are some of the documents that  t this publishing model. Most information that is time-sensitive is published.
Since each published document has an associated time, published documents from a single source can be ordered in a sequence called a document feed. A docu- ment feed is particularly interesting for crawlers, since the crawler can easily  nd all the new documents by examining only the end of the feed.
We can distinguish two kinds of document feeds, push and pull. A push feed alerts the subscriber to new documents.  is is like a telephone, which alerts you to an incoming phone call; you don’t need to continually check the phone to see if someone is calling. A pull feed requires the subscriber to check periodically for new documents; this is like checking your mailbox for new mail to arrive. News feeds from commercial news agencies are o en push feeds, but pull feeds are over- whelmingly popular for free services. We will focus primarily on pull feeds in this section.
 e most common format for pull feeds is called RSS. RSS has at least three de nitions: Really Simple Syndication, RDF Site Summary, or Rich Site Sum- mary. Not surprisingly, RSS also has a number of slightly incompatible imple- mentations, and a similar competing format exists called the Atom Syndication Format.  e proliferation of standards is the result of an idea that gained popu- larity too quickly for developers to agree on a single standard.
Figure 3.9 shows an RSS 2.0 feed from an example site called http://www.search- engine-news.org.  is feed contains two articles: one is about an upcoming SIGIR conference, and the other is about a textbook. Notice that each entry contains a time indicating when it was published. In addition, near the top of the RSS feed there is an tag named ttl, which means time to live, measured in minutes.  is
48 3 CrawlsandFeeds
           <?xml version="1.0"?>
           <rss version="2.0">
             <channel>
               <title>Search Engine News</title>
               <link>http://www.search-engine-news.org/</link>
               <description>News about search engines.</description>
               <language>en-us</language>
               <pubDate>Tue, 19 Jun 2008 05:17:00 GMT</pubDate>
               <ttl>60</ttl>
               <item>
                 <title>Upcoming SIGIR Conference</title>
                 <link>http://www.sigir.org/conference</link>
                 <description>The annual SIGIR conference is coming!
                   Mark your calendars and check for cheap
                   flights.</description>
                 <pubDate>Tue, 05 Jun 2008 09:50:11 GMT</pubDate>
                 <guid>http://search-engine-news.org#500</guid>
</item>
               <item>
                 <title>New Search Engine Textbook</title>
                 <link>http://www.cs.umass.edu/search-book</link>
                 <description>A new textbook about search engines
                   will be published soon.</description>
                 <pubDate>Tue, 05 Jun 2008 09:33:01 GMT</pubDate>
                 <guid>http://search-engine-news.org#499</guid>
               </item>
             </channel>
</rss>
Fig. 3.9. An example RSS 2.0 feed
3.5  eConversionProblem 49
feed states that its contents should be cached only for 60 minutes, and informa- tion more than an hour old should be considered stale.  is gives a crawler an indication of how o en this feed  le should be crawled.
RSS feeds are accessed just like a traditional web page, using HTTP GET re- quests to web servers that host them.  erefore, some of the crawling techniques we discussed before apply here as well, such as using HTTP HEAD requests to detect when RSS feeds change.
From a crawling perspective, document feeds have a number of advantages over traditional pages. Feeds give a natural structure to data; even more than with a sitemap, a web feed implies some relationship between the data items. Feeds are easy to parse and contain detailed time information, like a sitemap, but also include a description  eld about each page (and this description  eld sometimes contains the entire text of the page referenced in the URL). Most importantly, like a sitemap, feeds provide a single location to look for new data, instead of hav- ing to crawl an entire site to  nd a few new documents.
3.5 The Conversion Problem
Search engines are built to search through text. Unfortunately, text is stored on computers in hundreds of incompatible  le formats. Standard text  le formats include raw text, RTF, HTML, XML, Microso  Word, ODF (Open Document Format) and PDF (Portable Document Format).  ere are tens of other less com- mon word processors with their own  le formats. But text documents aren’t the only kind of document that needs to be searched; other kinds of  les also contain important text, such as PowerPoint slides and Excel®spreadsheets. In addition to all of these formats, people o en want to search old documents, which means that search engines may need to support obsolete  le formats. It is not uncommon for a commercial search engine to support more than a hundred  le types.
 e most common way to handle a new  le format is to use a conversion tool that converts the document content into a tagged text format such as HTML or XML.  ese formats are easy to parse, and they retain some of the important formatting information (font size, for example). You can see this on any major web search engine. Search for a PDF document, but then click on the “Cached” link at the bottom of a search result. You will be taken to the search engine’s view of the page, which is usually an HTML rendition of the original document. For some document types, such as PowerPoint, this cached version can be nearly un- readable. Fortunately, readability isn’t the primary concern of the search engine.
50 3 CrawlsandFeeds
 e point is to copy this data into the search engine so that it can be indexed and retrieved. However, translating the data into HTML has an advantage: the user does not need to have an application that can read the document’s  le format in order to view it.  is is critical for obsolete  le formats.
Documents could be converted to plain text instead of HTML or XML. However, doing this would strip the  le of important information about head- ings and font sizes that could be useful to the indexer. As we will see later, headings and bold text tend to contain words that describe the document content well, so we want to give these words preferential treatment during scoring. Accurate con- version of formatting information allows the indexer to extract these important features.
3.5.1 Character Encodings
Even HTML  les are not necessarily compatible with each other because of char- acter encoding issues.  e text that you see on this page is a series of little pictures we call letters or glyphs. Of course, a computer  le is a stream of bits, not a collec- tion of pictures. A character encoding is a mapping between bits and glyphs. For English, the basic character encoding that has been around since 1963 is ASCII. ASCII encodes 128 letters, numbers, special characters, and control characters in 7 bits, extended with an extra bit for storage in bytes.  is scheme is  ne for the English alphabet of 26 letters, but there are many other languages, and some of those have many more glyphs.  e Chinese language, for example, has more than 40,000 characters, with over 3,000 in common use. For the CJK (Chinese- Japanese-Korean) family of East Asian languages, this led to the development of a number of different 2-byte standards. Other languages, such as Hindi or Arabic, also have a range of different encodings. Note that not all encodings even agree on English.  e EBCDIC encoding used on mainframes, for example, is completely different than the ASCII encoding used by personal computers.
 e computer industry has moved slowly in handling complicated character sets such as Chinese and Arabic. Until recently, the typical approach was to use different language-speci c encodings, sometimes called code pages.  e  rst 128 values of each encoding are reserved for typical English characters, punctuation, and numbers. Numbers above 128 are mapped to glyphs in the target language, from Hebrew to Arabic. However, if you use a different encoding for each lan- guage, you can’t write in Hebrew and Japanese in the same document. Addition- ally, the text itself is no longer self-describing. It’s not enough to just store data in a text  le; you must also record what encoding was used.
3.5  eConversionProblem 51
To solve this mess of encoding issues, Unicode was developed. Unicode is a single mapping from numbers to glyphs that attempts to include all glyphs in common use in all known languages.  is solves the problem of using multiple languages in a single  le. Unfortunately, it does not fully solve the problems of bi- nary encodings, because Unicode is a mapping between numbers and glyphs, not bits and glyphs. It turns out that there are many ways to translate Unicode num- bers to glyphs! Some of the most popular include UTF-8, UTF-16, UTF-32, and UCS-2 (which is deprecated).
 e proliferation of encodings comes from a need for compatibility and to save space. Encoding English text in UTF-8 is identical to the ASCII encod- ing. Each ASCII letter requires just one byte. However, some traditional Chinese characters can require as many as 4 bytes.  e trade-off for compactness for West- ern languages is that each character requires a variable number of bytes, which makes it difficult to quickly compute the number of characters in a string or to jump to a random location in a string. By contrast, UTF-32 (also known as UCS- 4) uses exactly 4 bytes for every character. Jumping to the twentieth character in a UTF-32 string is easy: just jump to the eightieth byte and start reading. Un- fortunately, UTF-32 strings are incompatible with all old ASCII so ware, and UTF-32  les require four times as much space as UTF-8. Because of this, many applications use UTF-32 as their internal text encoding (where random access is important), but use UTF-8 to store text on disk.
Decimal
0–127 128–2047 2048–55295 55296–57343 57344–65535 65536–1114111
Hexadecimal
0–7F
80–7FF
800–D7FF
D800–DFFF
E000–FFFF
10000–10FFFF 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
Table 3.1. UTF-8 encoding
Encoding
 0xxxxxxx
110xxxxx 10xxxxxx 1110xxxx 10xxxxxx 10xxxxxx Unde ned
1110xxxx 10xxxxxx 10xxxxxx
Table 3.1 shows an encoding table for UTF-8.  e le  columns represent ranges of decimal values, and the rightmost column shows how these values are encoded in binary.  e x characters represent binary digits. For example, the Greek letter pi (π) is Unicode symbol number 960. In binary, that number is 00000011 11000000 (3C0 in hexadecimal).  e second row of the table tells us
52 3 CrawlsandFeeds
that this letter will require 2 bytes to encode in UTF-8.  e high 5 bits of the character go in the  rst byte, and the next 6 bits go in the second byte.  e  nal encoding is 11001111 10000000 (CF80 in hexadecimal).  e bold binary digits are the same as the digits from the table, while the x letters from the table have been  lled in by binary digits from the Unicode number.
3.6 Storing the Documents
A er documents have been converted to some common format, they need to be stored in preparation for indexing.  e simplest document storage is no docu- ment storage, and for some applications this is preferable. In desktop search, for example, the documents are already stored in the  le system and do not need to be copied elsewhere. As the crawling process runs, it can send converted documents immediately to an indexing process. By not storing the intermediate converted documents, desktop search systems can save disk space and improve indexing la- tency.
Most other kinds of search engines need to store documents somewhere. Fast access to the document text is required in order to build document snippets2 for each search result.  ese snippets of text give the user an idea of what is inside the retrieved document without actually needing to click on a link.
Even if snippets are not necessary, there are other reasons to keep a copy of each document. Crawling for documents can be expensive in terms of both CPU and network load. It makes sense to keep copies of the documents around instead of trying to fetch them again the next time you want to build an index. Keep- ing old documents allows you to use HEAD requests in your crawler to save on bandwidth, or to crawl only a subset of the pages in your index.
Finally, document storage systems can be a starting point for information ex- traction (described in Chapter 4).  e most pervasive kind of information ex- traction happens in web search engines, which extract anchor text from links to store with target web documents. Other kinds of extraction are possible, such as identifying names of people or places in documents. Notice that if information extraction is used in the search application, the document storage system should support modi cation of the document data.
Wenowdiscusssomeofthebasicrequirementsforadocumentstoragesystem, including random access, compression, and updating, and consider the relative
2 We discuss snippet generation in Chapter 6.
 
3.6 StoringtheDocuments 53
bene ts of using a database system or a customized storage system such as Google’s BigTable.
3.6.1 Using a Database System
If you have used a relational database before, you might be thinking that a database would be a good place to store document data. For many applications, in fact, a database is an excellent place to store documents. A database takes care of the difficult details of storing small pieces of data, such as web pages, and makes it easy to update them later. Most databases also run as a network server, so that the documents are easily available on the network.  is could support, for example, a single computer serving documents for snippets while many other computers handle queries. Databases also tend to come with useful import and analysis tools that can make it easier to manage the document collection.
Many companies that run web search engines are reluctant to talk about their internal technologies. However, it appears that few, if any, of the major search engines use conventional relational databases to store documents. One problem is the sheer volume of document data, which can overwhelm traditional database systems. Database vendors also tend to expect that database servers will use the most expensive disk systems, which is impractical given the collection size. We discuss an alternative to a relational database at the end of this section that ad- dresses some of these concerns.
3.6.2 Random Access
To retrieve documents quickly in order to compute a snippet for a search result, the document store needs to support random access. Compared to a full relational database, however, only a relatively simple lookup criterion is needed. We want a data store such that we can request the content of a document based on its URL.
 e easiest way to handle this kind of lookup is with hashing. Using a hash function on the URL gives us a number we can use to  nd the data. For small installations, the hash function can tell us which  le contains the document. For larger installations, the hash function tells us which server contains the document. Once the document location has been narrowed down to a single  le, a B-Tree or sorted data structure can be used to  nd the offset of the document data within the  le.
54 3 CrawlsandFeeds
3.6.3 Compression and Large Files
Regardless of whether the application requires random access to documents, the document storage system should make use of large  les and compression.
Even a document that seems long to a person is small by modern computer standards. For example, this chapter is approximately 10,000 words, and those words require about 70K of disk space to store.  at is far bigger than the average web page, but a modern hard disk can transfer 70K of data in about a millisecond. However, the hard disk might require 10 milliseconds to seek to that  le in order to start reading.  is is why storing each document in its own  le is not a very good idea; reading these small  les requires a substantial overhead to open them. A better solution is to store many documents in a single  le, and for that  le to be large enough that transferring the  le contents takes much more time than seeking to the beginning. A good size choice might be in the hundreds of megabytes. By storing documents close together, the indexer can spend most of its time reading data instead of seeking for it.
 e Galago search engine includes parsers for three compound document for- mats: ARC, TREC Text, and TREC Web. In each format, many text documents are stored in the same  le, with short regions of document metadata separating the documents. Figure 3.10 shows an example of the TREC Web format. Notice that each document block begins with a <DOC> tag and ends with a </DOC> tag. At the beginning of the document, the <DOCHDR> tag marks a section containing the information about the page request, such as its URL, the date it was crawled, and the HTTP headers returned by the web server. Each document record also contains a <DOCNO>  eld that includes a unique identi er for the document.
Even though large  les make sense for data transfer from disk, reducing the total storage requirements for document collections has obvious advantages. For- tunately, text written by people is highly redundant. For instance, the letter q is al- most always followed by the letter u. Shannon (1951) showed that native English speakers are able to guess the next letter of a passage of English text with 69% ac- curacy. HTML and XML tags are even more redundant. Compression techniques exploit this redundancy to make  les smaller without losing any of the content. We will cover compression as it is used for document indexing in Chapter 5, in part because compression for indexing is rather specialized. While research continues into text compression, popular algorithms like DEFLATE (Deutsch, 1996) and LZW (Welch, 1984) can compress HTML and XML text by 80%.  is space savings reduces the cost of storing a lot of documents, and also reduces
3.6 StoringtheDocuments 55
<DOC>
<DOCNO>WTX001-B01-10</DOCNO>
<DOCHDR>
http://www.example.com/test.html 204.244.59.33 19970101013145 text/html 440
HTTP/1.0 200 OK
Date: Wed, 01 Jan 1997 01:21:13 GMT
Server: Apache/1.0.3
Content-type: text/html
Content-length: 270
Last-modified: Mon, 25 Nov 1996 05:31:24 GMT
</DOCHDR>
<HTML>
<TITLE>Tropical Fish Store</TITLE>
Coming soon!
</HTML>
</DOC>
<DOC>
<DOCNO>WTX001-B01-109</DOCNO>
<DOCHDR>
http://www.example.com/fish.html 204.244.59.33 19970101013149 text/html 440
HTTP/1.0 200 OK
Date: Wed, 01 Jan 1997 01:21:19 GMT
Server: Apache/1.0.3
Content-type: text/html
Content-length: 270
Last-modified: Mon, 25 Nov 1996 05:31:24 GMT
</DOCHDR>
<HTML>
<TITLE>Fish Information</TITLE>
This page will soon contain interesting
information about tropical fish.
</HTML>
</DOC>
Fig. 3.10. An example of text in the TREC Web compound document format
56 3 CrawlsandFeeds
the amount of time it takes to read a document from the disk since there are fewer bytes to read.
Compression works best with large blocks of data, which makes it a good  t for big  les with many documents in them. However, it is not necessarily a good idea to compress the entire  le as a single block. Most compression methods do not allow random access, so each block can only be decompressed sequentially. If you want random access to the data, it is better to consider compressing in smaller blocks, perhaps one block per document, or one block for a few documents. Small blocks reduce compression ratios (the amount of space saved) but improve re- quest latency.
3.6.4 Update
As new versions of documents come in from the crawler, it makes sense to update the document store.  e alternative is to create an entirely new document store by merging the new, changed documents from the crawler with document data from the old document store for documents that did not change. If the document data does not change very much, this merging process will be much more expensive than updating the data in place.
             <a href="http://example.com" >Example website</a>
Fig. 3.11. An example link with anchor text
Another important reason to support update is to handle anchor text. Fig- ure 3.11 shows an example of anchor text in an HTML link tag.  e HTML code in the  gure will render in the web browser as a link, with the text Example website that, when clicked, will direct the user to http://example.com. Anchor text is an important feature because it provides a concise summary of what the target page is about. If the link comes from a different website, we may also believe that the summary is unbiased, which also helps us rank documents (see Chapters 4 and 7).
Collecting anchor text properly is difficult because the anchor text needs to be associated with the target page. A simple way to approach this is to use a data store that supports update. When a document is found that contains anchor text, we  nd the record for the target page and update the anchor text portion of the record. When it is time to index the document, the anchor text is all together and ready for indexing.
3.6.5 BigTable
Although a database can perform the duties of a document data store, the very largest document collections demand custom document storage systems. BigTable is the most well known of these systems (Chang et al., 2006). BigTable is a working system in use internally at Google, although at least two open source projects are taking a similar approach. In the next few paragraphs, we will look at the BigTable architecture to see how the problem of document storage in uenced its design.
BigTable is a distributed database system originally built for the task of storing web pages. A BigTable instance really is a big table; it can be over a petabyte in size, but each database contains only one table.  e table is split into small pieces, called tablets, which are served by thousands of machines (Figure 3.12).
3.6 StoringtheDocuments 57
             logical table
tablets
Fig. 3.12. BigTable stores data in a single logical table, which is split into many smaller tablets
If you are familiar with relational databases, you will have encountered SQL (Structured Query Language). SQL allows users to write complex and computa- tionally expensive queries, and one of the tasks of the database system is to opti- mize the processing of these queries to make them as fast as possible. Because some of these queries could take a very long time to complete, a large relational database requires a complex locking system to ensure that the many users of the database do not corrupt it by reading or writing data simultaneously. Isolating users from each other is a difficult job, and many papers and books have been written about how to do it well.
58 3 CrawlsandFeeds
 e BigTable approach is quite different.  ere is no query language, and therefore no complex queries, and it includes only row-level transactions, which would be considered rather simple by relational database standards. However, the simplicity of the model allows BigTable to scale up to very large database sizes while using inexpensive computers, even though they may be prone to failure.
Most of the engineering in BigTable involves failure recovery.  e tablets, which are the small sections of the table, are stored in a replicated  le system that is accessible by all BigTable tablet servers. Any changes to a BigTable tablet are recorded to a transaction log, which is also stored in a shared  le system. If any tablet server crashes, another server can immediately read the tablet data and transaction log from the  le system and take over.
Most relational databases store their data in  les that are constantly modi ed. In contrast, BigTable stores its data in immutable (unchangeable)  les. Once  le data is written to a BigTable  le, it is never changed.  is also helps in failure recovery. In relational database systems, failure recovery requires a complex series of operations to make sure that  les were not corrupted because only some of the outstanding writes completed before the computer crashed. In BigTable, a  le is either incomplete (in which case it can be thrown away and re-created from other BigTable  les and the transaction log ), or it is complete and therefore is not corrupt. To allow for table updates, the newest data is stored in RAM, whereas older data is stored in a series of  les. Periodically the  les are merged together to reduce the total number of disk  les.
anchor:other.com anchor:null.com
Fig. 3.13. A BigTable row
title
  text
   document text
 example
 click here
 example site
 www.example.com
 BigTables are logically organized by rows (Figure 3.13). In the  gure, the row stores the data for a single web page.  e URL, www.example.com, is the row key, which can be used to  nd this row.  e row has many columns, each with a unique name. Each column can have many different timestamps, although that is not shown in the  gure.  e combination of a row key, a column key, and a times-
3.6 StoringtheDocuments 59
tamp point to a single cell in the row.  e cell holds a series of bytes, which might be a number, a string, or some other kind of data.
In the  gure, notice that there is a text column for the full text of the docu- ment as well as a title column, which makes it easy to quickly  nd the document title without parsing the full document text.  ere are two columns for anchor text. One, called anchor:other.com, includes anchor text from a link from the site other.com to example.com; the text of the link is “example”, as shown in the cell.  e anchor:null.com describes a link from null.com to example.com with anchor text “click here”. Both of these columns are in the anchor column group. Other columns could be added to this column group to add information about more links.
BigTable can have a huge number of columns per row, and while all rows have the same column groups, not all rows have the same columns.  is is a major de- parture from traditional database systems, but this  exibility is important, in part because of the lack of tables. In a relational database system, the anchor columns would be stored in one table and the document text in another. Because BigTable has just one table, all the anchor information needs to be packed into a single record. With all the anchor data stored together, only a single disk read is neces- sary to read all of the document data. In a two-table relational database, at least two reads would be necessary to retrieve this data.
Rows are partitioned into tablets based on their row keys. For instance, all URLs beginning with a could be located in one tablet, while all those starting with b could be in another tablet. Using this kind of range-based partitioning makes it easy for a client of BigTable to determine which server is serving each row. To look up a particular row, the client consults a list of row key ranges to determine which tablet would hold the desired row.  e client then contacts the appropriate tablet server to fetch the row.  e row key ranges are cached in the client, so that most of the network traffic is between clients and tablet servers.
BigTable’s architecture is designed for speed and scale through massive num- bers of servers, and for economy by using inexpensive computers that are expected to fail. In order to achieve these goals, BigTable sacri ces some key relational database features, such as a complex query language and multiple-table databases. However, this architecture is well suited for the task of storing and  nding web pages, where the primary task is efficient lookups and updates on individual rows.
60 3 CrawlsandFeeds
3.7 Detecting Duplicates
Duplicate and near-duplicate documents occur in many situations. Making copies and creating new versions of documents is a constant activity in offices, and keep- ing track of these is an important part of information management. On the Web, however, the situation is more extreme. In addition to the normal sources of dupli- cation, plagiarism and spam are common, and the use of multiple URLs to point to the same web page and mirror sites can cause a crawler to generate large num- bers of duplicate pages. Studies have shown that about 30% of the web pages in a large crawl are exact or near duplicates of pages in the other 70% (e.g., Fetterly et al., 2003).
Documents with very similar content generally provide little or no new infor- mation to the user, but consume signi cant resources during crawling, indexing, and search. In response to this problem, algorithms for detecting duplicate doc- uments have been developed so that they can be removed or treated as a group during indexing and ranking.
Detecting exact duplicates is a relatively simple task that can be done using checksumming techniques. A checksum is a value that is computed based on the content of the document.  e most straightforward checksum is a sum of the bytes in the document  le. For example, the checksum for a  le containing the text “Tropical  sh” would be computed as follows (in hex):
T r o p i c a l f i s h Sum 54726F706963616C2066697368 508
Any document  le containing the same text would have the same checksum. Of course, any document  le containing text that happened to have the same check- sum would also be treated as a duplicate. A  le containing the same characters in a different order would have the same checksum, for example. More sophis- ticated functions, such as a cyclic redundancy check (CRC), have been developed that consider the positions of the bytes.
 e detection of near-duplicate documents is more difficult. Even de ning a near-duplicate is challenging. Web pages, for example, could have the same text content but differ in the advertisements, dates, or formatting. Other pages could have small differences in their content from revisions or updates. In general, a near-duplicate is de ned using a threshold value for some similarity measure be- tween pairs of documents. For example, a document D1 could be de ned as a near-duplicate of document D2 if more than 90% of the words in the documents were the same.
3.7 DetectingDuplicates 61
 ere are two scenarios for near-duplicate detection. One is the search sce- nario, where the goal is to  nd near-duplicates of a given document D.  is, like all search problems, conceptually involves the comparison of the query document to all other documents. For a collection containing N documents, the number of comparisons required will be O(N ).  e other scenario, discovery, involves  nd- ing all pairs of near-duplicate documents in the collection.  is process requires O(N2) comparisons. Although information retrieval techniques that measure similarity using word-based representations of documents have been shown to be effective for identifying near-duplicates in the search scenario, the computa- tional requirements of the discovery scenario have meant that new techniques have been developed for deriving compact representations of documents.  ese compact representations are known as  ngerprints.
 e basic process of generating  ngerprints is as follows:
1.  edocumentisparsedintowords.Non-wordcontent,suchaspunctuation, HTML tags, and additional whitespace, is removed (see section 4.3).
2.  ewordsaregroupedintocontiguousn-gramsforsomen. eseareusually overlapping sequences of words (see section 4.3.5), although some techniques use non-overlapping sequences.
3. Some of the n-grams are selected to represent the document.
4.  e selected n-grams are hashed to improve retrieval efficiency and further
reduce the size of the representation.
5.  e hash values are stored, typically in an inverted index.
 ere are a number of  ngerprinting algorithms that use this general approach,
and they differ mainly in how subsets of the n-grams are selected. Selecting a  xed number of n-grams at random does not lead to good performance in terms of  nding near-duplicates. Consider two near-identical documents, D1 and D2.  e  ngerprints generated from n-grams selected randomly from document D1 are unlikely to have a high overlap with the  ngerprints generated from a different set of n-grams selected randomly from D2. A more effective technique uses pre- speci ed combinations of characters, and selects n-grams that begin with those characters. Another popular technique, called 0 mod p, is to select all n-grams whose hash value modulo p is zero, where p is a parameter.
Figure 3.14 illustrates the  ngerprinting process using overlapping 3-grams, hypothetical hash values, and the 0 mod p selection method with a p value of 4. Note that a er the selection process, the document (or sentence in this case) is represented by  ngerprints for the n-grams “ sh include  sh”, “found in tropical”,
62
3 CrawlsandFeeds
Tropical fish include fish found in tropical environments around the world,  including both freshwater and salt water species. 
(a) Original text
 
tropical fish include, fish include fish, include fish found, fish found in, found in  tropical, in tropical environments, tropical environments around, environments  around the, around the world, the world including, world including both, including  both freshwater, both freshwater and, freshwater and salt, and salt water, salt  water species 
(b) 3-grams
938  664  463  822  492  798  78  969  143  236  913  908  694  553  870  779 
(c) Hash values
                                                      664  492  236  908 
(d) Selected hash values using 0 mod 4
Fig. 3.14. Example of  ngerprinting process
“theworldincluding”,and“includingbothfreshwater”.Inlarge-scaleapplications, such as  nding near-duplicates on the Web, the n-grams are typically 5–10 words long and the hash values are 64 bits.3
Near-duplicate documents are found by comparing the  ngerprints that repre- sent them. Near-duplicate pairs are de ned by the number of shared  ngerprints or the ratio of shared  ngerprints to the total number of  ngerprints used to rep- resent the pair of documents. Fingerprints do not capture all of the information in the document, however, and consequently this leads to errors in the detection of near-duplicates. Appropriate selection techniques can reduce these errors, but not eliminate them. As we mentioned, evaluations have shown that comparing word-based representations using a similarity measure such as the cosine correla- tion (see section 7.1.2) is generally signi cantly more effective than  ngerprinting methods for  nding near-duplicates.  e problem with these methods is their ef-  ciency.
3  e hash values are usually generated using Rabin  ngerprinting (Broder et al., 1997), named a er the Israeli computer scientist Michael Rabin.
 
3.8 RemovingNoise 63
A recently developed  ngerprinting technique called simhash (Charikar, 2002) combines the advantages of the word-based similarity measures with the effi- ciency of  ngerprints based on hashing. It has the unusual property for a hashing function that similar documents have similar hash values. More precisely, the sim- ilarity of two pages as measured by the cosine correlation measure is proportional to the number of bits that are the same in the  ngerprints generated by simhash.
 e procedure for calculating a simhash  ngerprint is as follows:
1. Process the document into a set of features with associated weights. We will assume the simple case where the features are words weighted by their fre- quency. Other weighting schemes are discussed in Chapter 7.
2. Generate a hash value with b bits (the desired size of the  ngerprint) for each word.  e hash value should be unique for each word.
3. In b-dimensional vector V , update the components of the vector by adding the weight for a word to every component for which the corresponding bit in the word’s hash value is 1, and subtracting the weight if the value is 0.
4. A erallwordshavebeenprocessed,generateab-bit ngerprintbysettingthe ith bit to 1 if the ith component of V is positive, or 0 otherwise.
Figure 3.15 shows an example of this process for an 8-bit  ngerprint. Note
that common words (stopwords) are removed as part of the text processing. In practice, much larger values of b are used. Henzinger (2006) describes a large-scale Web-based evaluation where the  ngerprints had 384 bits. A web page is de ned as a near-duplicate of another page if the simhash  ngerprints agree on more than 372 bits.  is study showed signi cant effectiveness advantages for the simhash approach compared to  ngerprints based on n-grams.
3.8 Removing Noise
Many web pages contain text, links, and pictures that are not directly related to the main content of the page. For example, Figure 3.16 shows a web page contain- ing a news story.  e main content of the page (the story) is outlined in black.  is content block takes up less than 20% of the display area of the page, and the rest is made up of banners, advertisements, images, general navigation links, ser- vices (such as search and alerts), and miscellaneous information, such as copy- right. From the perspective of the search engine, this additional material in the web page is mostly noise that could negatively affect the ranking of the page. A major component of the representation of a page used in a search engine is based
64
3 CrawlsandFeeds
Tropical fish include fish found in tropical environments around the world,  including both freshwater and salt water species. 
(a) Original text
 
tropical 2  fish 2  include 1  found 1  environments 1  around 1  world 1    including 1  both 1  freshwater 1  salt 1  water 1  species 1 
tropical    found  world  freshwater  species 
(b) Words with weights
01100001 fish  10101011 include  11100110 00011110 environments 00101101 around  10001011
00101010 including 11000000 both 00111111 salt  10110101 water 11101110  
(c) 8 bit hash values
10101110
00100101
                                             1   5  9   9  3  1  3  3 
(d) Vector V formed by summing weights                                               1  0  1  0  1  1  1  1 
(e) 8-bit fingerprint formed from V
Fig. 3.15. Example of simhash  ngerprinting process
on word counts, and the presence of a large number of words unrelated to the main topic can be a problem. For this reason, techniques have been developed to detect the content blocks in a web page and either ignore the other material or reduce its importance in the indexing process.
Finn et al. (2001) describe a relatively simple technique based on the obser- vation that there are less HTML tags in the text of the main content of typical web pages than there is in the additional material. Figure 3.17 (also known as a document slope curve) shows the cumulative distribution of tags in the example web page from Figure 3.16, as a function of the total number of tokens (words or other non-tag strings) in the page.  e main text content of the page corresponds to the “plateau” in the middle of the distribution.  is  at area is relatively small because of the large amount of formatting and presentation information in the HTML source for the page.
3.8 RemovingNoise 65
Content  block
Fig. 3.16. Main content block in a web page
One way to detect the largest  at area of the distribution is to represent a web pageasasequenceofbits,wherebn =1indicatesthatthenthtokenisatag,and bn =0otherwise.Certaintagsthataremostlyusedtoformattext,suchasfont changes, headings, and table tags, are ignored (i.e., are represented by a 0 bit).  e detection of the main content can then be viewed as an optimization problem where we  nd values of i and j to maximize both the number of tags below i and above j and the number of non-tag tokens between i and j.  is corresponds to maximizing the corresponding objective function:
i−1 j N−1 ∑∑∑
    n=0
n=i
bn +
(1−bn)+ bn n=j+1
66 3 CrawlsandFeeds
              1000
               900
               800
               700
               600
               500
               400
               300
               200
               100
                 0
Text area
   Tag count
  0 500
1000     1500     2000     2500     3000     3500
Token count
Fig. 3.17. Tag counts used to identify text blocks in a web page
where N is the number of tokens in the page.  is can be done simply by scanning the possible values for i and j and computing the objective function. Note that this procedure will only work when the proportion of text tokens in the non- content section is lower than the proportion of tags, which is not the case for the web page in Figure 3.17. Pinto et al. (2002) modi ed this approach to use a text window to search for low-slope sections of the document slope curve.
 e structure of the web page can also be used more directly to identify the content blocks in the page. To display a web page using a browser, an HTML parser interprets the structure of the page speci ed using the tags, and creates a Document Object Model (DOM) representation.  e tree-like structure repre- sented by the DOM can be used to identify the major components of the web page. Figure 3.18 shows part of the DOM structure4 for the example web page in Figure 3.16.  e part of the structure that contains the text of the story is indicated by the comment cnnArticleContent. Gupta et al. (2003) describe an approach that
4  is was generated using the DOM Inspector tool in the Firefox browser.
 
3.8 RemovingNoise 67
navigates the DOM tree recursively, using a variety of  ltering techniques to re- move and modify nodes in the tree and leave only content. HTML elements such as images and scripts are removed by simple  lters. More complex  lters remove advertisements, lists of links, and tables that do not have “substantive” content.
 
Fig. 3.18. Part of the DOM structure for the example web page
 e DOM structure provides useful information about the components of a web page, but it is complex and is a mixture of logical and layout components. In Figure 3.18, for example, the content of the article is buried in a table cell (TD tag) in a row (TR tag) of an HTML table (TABLE tag).  e table is being used in this case to specify layout rather than semantically related data. Another approach to
 
68 3 CrawlsandFeeds
identifying the content blocks in a page focuses on the layout and presentation of the web page. In other words, visual features—such as the position of the block, the size of the font used, the background and font colors, and the presence of separators (such as lines and spaces)—are used to de ne blocks of information that would be apparent to the user in the displayed web page. Yu et al. (2003) describe an algorithm that constructs a hierarchy of visual blocks from the DOM tree and visual features.
 e  rst algorithm we discussed, based on the distribution of tags, is quite effective for web pages with a single content block. Algorithms that use the DOM structure and visual analysis can deal with pages that may have several content blocks. In the case where there are several content blocks, the relative importance of each block can be used by the indexing process to produce a more effective representation. One approach to judging the importance of the blocks in a web page is to train a classi er that will assign an importance category based on visual and content features (R. Song et al., 2004).
References and Further Reading
Cho and Garcia-Molina (2002, 2003) wrote a series of in uential papers on web crawler design. Our discussion of page refresh policies is based heavily on Cho and Garcia-Molina (2003), and section 3.2.7 draws from Cho and Garcia-Molina (2002).
 ere are many open source web crawlers.  e Heritrix crawler,5 developed for the Internet Archive project, is a capable and scalable example.  e system is de- veloped in modules that are highly con gurable at runtime, making it particularly suitable for experimentation.
Focused crawling attracted much attention in the early days of web search. Menczer and Belew (1998) and Chakrabarti et al. (1999) wrote two of the most in uential papers. Menczer and Belew (1998) envision a focused crawler made of autonomous so ware agents, principally for a single user.  e user enters a list of both URLs and keywords.  e agent then attempts to  nd web pages that would be useful to the user, and the user can rate those pages to give feedback to the system. Chakrabarti et al. (1999) focus on crawling for specialized topical indexes.  eir crawler uses a classi er to determine the topicality of crawled pages, as well as a distiller, which judges the quality of a page as a source of links to other topical
5 http://crawler.archive.org
 
3.8 RemovingNoise 69
pages.  ey evaluate their system against a traditional, unfocused crawler to show that an unfocused crawler seeded with topical links is not sufficient to achieve a topical crawl.  e broad link structure of the Web causes the unfocused crawler to quickly dri  to other topics, while the focused crawler successfully stays on topic.
 e Unicode speci cation is an incredibly detailed work, covering tens of thousands of characters (Unicode Consortium, 2006). Because of the nature of some non-Western scripts, many glyphs are formed from grouping a number of Unicode characters together, so the speci cation must detail not just what the characters are, but how they can be joined together. Characters are still being added to Unicode periodically.
Bergman (2001) is an extensive study of the deep Web. Even though this study is old by web standards, it shows how sampling through search engines can be used to help estimate the amount of unindexed content on the Web.  is study esti- mated that 550 billion web pages existed in the deep Web, compared to 1 billion in the accessible Web. He et al. (2007) describe a more recent survey that shows that the deep Web has continued to expand rapidly in recent years. An example of a technique for generating searchable representations of deep Web databases, called query probing, is described by Ipeirotis and Gravano (2004).
Sitemaps, robots.txt  les, RSS feeds, and Atom feeds each have their own spec- i cations, which are available on the Web.6  ese formats show that successful web standards are o en quite simple.
As we mentioned, database systems can be used to store documents from a web crawl for some applications. Our discussion of database systems was, how- ever, limited mostly to a comparison with BigTable.  ere are a number of text- books, such as Garcia-Molina et al. (2008), that provide much more informa- tion on how databases work, including details about important features such as query languages, locking, and recovery. BigTable, which we referenced frequently, was described in Chang et al. (2006). Other large Internet companies have built their own database systems with similar goals: large-scale distribution and high throughput, but without an expressive query language or detailed transaction sup- port.  e Dynamo system from Amazon has low latency guarantees (DeCandia et al., 2007), and Yahoo! uses their UDB system to store large datasets (Baeza- Yates & Ramakrishnan, 2008).
6 http://www.sitemaps.org http://www.robotstxt.org http://www.rssboard.org/rss-speci cation http://www.rfc-editor.org/rfc/rfc5023.txt
 
70 3 CrawlsandFeeds
We mentioned DEFLATE (Deutsch, 1996) and LZW (Welch, 1984) as spe- ci c document compression algorithms in the text. DEFLATE is the basis for the popular Zip, gzip, and zlib compression tools. LZW is the basis of the Unix com- press command, and is also found in  le formats such as GIF, PostScript, and PDF.  e text by Witten et al. (1999) provides detailed discussions about text and image compression algorithms.
Hoad and Zobel (2003) provide both a review of  ngerprinting techniques and a comparison to word-based similarity measures for near-duplicate detection.  eir evaluation focused on  nding versions of documents and plagiarized docu- ments. Bernstein and Zobel (2006) describe a technique for using full  ngerprint- ing (no selection) for the task of  nding co-derivatives, which are documents de- rived from the same source. Bernstein and Zobel (2005) examined the impact of duplication on evaluations of retrieval effectiveness.  ey showed that about 15% of the relevant documents for one of the TREC tracks were redundant, which could signi cantly affect the impact of the results from a user’s perspective.
Henzinger (2006) describes a large-scale evaluation of near-duplicate detec- tion on the Web.  e two techniques compared were a version of Broder’s “shin- gling” algorithm (Broder et al., 1997; Fetterly et al., 2003) and simhash (Charikar, 2002). Henzinger’s study, which used 1.6 billion pages, showed that neither meth- od worked well for detecting redundant documents on the same site because of the frequent use of “boilerplate” text that makes different pages look similar. For pages on different sites, the simhash algorithm achieved a precision of 50% (meaning that of those pages that were declared “near-duplicate” based on the similarity threshold, 50% were correct), whereas the Broder algorithm produced a precision of 38%.
A number of papers have been written about techniques for extracting content from web pages. Yu et al. (2003) and Gupta et al. (2003) are good sources for references to these papers.
Exercises
3.1. Suppose you have two collections of documents.  e smaller collection is full of useful, accurate, high-quality information.  e larger collection contains a few high-quality documents, but also contains lower-quality text that is old, out-of- date, or poorly written. What are some reasons for building a search engine for only the small collection? What are some reasons for building a search engine that covers both collections?
3.8 RemovingNoise 71
3.2. Suppose you have a network connection that can transfer 10MB per second. If each web page is 10K and requires 500 milliseconds to transfer, how many threads does your web crawler need to fully utilize the network connection? If your crawler needs to wait 10 seconds between requests to the same web server, what is the minimum number of distinct web servers the system needs to contact each minute to keep the network connection fully utilized?
3.3. What is the advantage of using HEAD requests instead of GET requests dur- ing crawling? When would a crawler use a GET request instead of a HEAD re- quest?
3.4. Why do crawlers not use POST requests?
3.5. Name the three types of sites mentioned in the chapter that compose the
deepWeb.
3.6. How would you design a system to automatically enter data into web forms in order to crawl deep Web pages? What measures would you use to make sure your crawler’s actions were not destructive (for instance, so that it doesn’t add random blog comments).
3.7. Write a program that can create a valid sitemap based on the contents of a directory on your computer’s hard disk. Assume that the  les are accessible from a website at the URL http://www.example.com. For instance, if there is a  le in your directory called homework.pdf, this would be available at http://www.exam- ple.com/homework.pdf. Use the real modi cation date on the  le as the last modi-  ed time in the sitemap, and to help estimate the change frequency.
3.8. Suppose that, in an effort to crawl web pages faster, you set up two crawl- ing machines with different starting seed URLs. Is this an effective strategy for distributed crawling? Why or why not?
3.9. Write a simple single-threaded web crawler. Starting from a single input URL (perhaps a professor’s web page), the crawler should download a page and then wait at least  ve seconds before downloading the next page. Your program should  nd other pages to crawl by parsing link tags found in previously crawled docu- ments.
3.10. UTF-16 is used in Java and Windows®. Compare it to UTF-8. 3.11. How does BigTable handle hardware failure?
72 3 CrawlsandFeeds
3.12. Design a compression algorithm that compresses HTML tags. Your algo- rithm should detect tags in an HTML  le and replace them with a code of your own design that is smaller than the tag itself. Write an encoder and decoder pro- gram.
3.13. Generate checksums for a document by adding the bytes of the document and by using the Unix command cksum. Edit the document and see if both check- sums change. Can you change the document so that the simple checksum does not change?
3.14. Write a program to generate simhash  ngerprints for documents. You can use any reasonable hash function for the words. Use the program to detect du- plicates on your home computer. Report on the accuracy of the detection. How does the detection accuracy vary with  ngerprint size?
3.15. Plot the document slope curves for a sample of web pages.  e sample should include at least one page containing a news article. Test the accuracy of the simple optimization algorithm for detecting the main content block. Write your own program or use the code from http://www.aidanf.net/software/bte-body- text-extraction. Describe the cases where the algorithm fails. Would an algorithm that searched explicitly for low-slope areas of the document slope curve be suc- cessful in these cases?
3.16. Give a high-level outline of an algorithm that would use the DOM structure to identify content information in a web page. In particular, describe heuristics you would use to identify content and non-content elements of the structure.
4
Processing Text
4.1 From Words to Terms
“I was trying to comprehend the meaning of the words.”
Spock, Star Trek:  e Final Frontier
 A er gathering the text we want to search, the next step is to decide whether it should be modi ed or restructured in some way to simplify searching.  e types of changes that are made at this stage are called text transformation or, more o en, text processing.  e goal of text processing is to convert the many forms in which words can occur into more consistent index terms. Index terms are the represen- tation of the content of a document that are used for searching.
 e simplest decision about text processing would be to not do it at all. A good example of this is the “ nd” feature in your favorite word processor. By the time you use the  nd command, the text you wish to search has already been gathered: it’s on the screen. A er you type the word you want to  nd, the word processor scans the document and tries to  nd the exact sequence of letters that you just typed.  is feature is extremely useful, and nearly every text editing program can do this because users demand it.
 e trouble is that exact text search is rather restrictive.  e most annoying restriction is case-sensitivity: suppose you want to  nd “computer hardware”, and there is a sentence in the document that begins with “Computer hardware”. Your search query does not exactly match the text in the sentence, because the  rst letter of the sentence is capitalized. Fortunately, most word processors have an option forignoringcaseduringsearches.Youcanthinkofthisasaveryrudimentaryform of online text processing. Like most text processing techniques, ignoring case in- creases the probability that you will  nd a match for your query in the document.
Many search engines do not distinguish between uppercase and lowercase let- ters. However, they go much further. As we will see in this chapter, search engines
74 4 ProcessingText
can strip punctuation from words to make them easier to  nd. Words are split apart in a process called tokenization. Some words may be ignored entirely in or- der to make query processing more effective and efficient; this is called stopping.  e system may use stemming to allow similar words (like “run” and “running”) to match each other. Some documents, such as web pages, may have formatting changes (like bold or large text), or explicit structure (like titles, chapters, and cap- tions) that can also be used by the system. Web pages also contain links to other web pages, which can be used to improve document ranking. All of these tech- niques are discussed in this chapter.
 ese text processing techniques are fairly simple, even though their effects on search results can be profound. None of these techniques involves the com- puter doing any kind of complex reasoning or understanding of the text. Search engines work because much of the meaning of text is captured by counts of word occurrences and co-occurrences,1 especially when that data is gathered from the huge text collections available on the Web. Understanding the statistical nature of text is fundamental to understanding retrieval models and ranking algorithms, so we begin this chapter with a discussion of text statistics. More sophisticated tech- niques for natural language processing that involve syntactic and semantic analysis of text have been studied for decades, including their application to information retrieval, but to date have had little impact on ranking algorithms for search en- gines.  ese techniques are, however, being used for the task of question answer- ing, which is described in Chapter 11. In addition, techniques involving more complex text processing are being used to identify additional index terms or fea- tures for search. Information extraction techniques for identifying people’s names, organization names, addresses, and many other special types of features are dis- cussed here, and classi cation, which can be used to identify semantic categories, is discussed in Chapter 9.
Finally, even though this book focuses on retrieving English documents, in- formation retrieval techniques can be used with text in many different languages. In this chapter, we show how different languages require different types of text representation and processing.
1 Word co-occurrence measures the number of times groups of words (usually pairs) oc- cur together in documents. A collocation is the name given to a pair, group, or sequence of words that occur together more o en than would be expected by chance.  e term association measures that are used to  nd collocations are discussed in Chapter 6.
 
4.2 Text Statistics
Although language is incredibly rich and varied, it is also very predictable.  ere are many ways to describe a particular topic or event, but if the words that occur in many descriptions of an event are counted, then some words will occur much more frequently than others. Some of these frequent words, such as “and” or “the,” will be common in the description of any event, but others will be characteristic of that particular event.  is was observed as early as 1958 by Luhn, when he proposed that the signi cance of a word depended on its frequency in the docu- ment. Statistical models of word occurrences are very important in information retrieval, and are used in many of the core components of search engines, such as the ranking algorithms, query transformation, and indexing techniques.  ese models will be discussed in later chapters, but we start here with some of the basic models of word occurrence.
One of the most obvious features of text from a statistical point of view is that the distribution of word frequencies is very skewed.  ere are a few words that have very high frequencies and many words that have low frequencies. In fact, the two most frequent words in English (“the” and “of ”) account for about 10% of all word occurrences.  e most frequent six words account for 20% of occurrences, and the most frequent 50 words are about 40% of all text! On the other hand, given a large sample of text, typically about one half of all the unique words in that sample occur only once.  is distribution is described by Zipf ’s law,2 which states that the frequency of the rth most common word is inversely proportional to r or, alternatively, the rank of a word times its frequency (f ) is approximately a constant (k):
r·f=k
We o en want to talk about the probability of occurrence of a word, which is just the frequency of the word divided by the total number of word occurrences in the text. In this case, Zipf ’s law is:
r·Pr =c
where Pr is the probability of occurrence for the rth ranked word, and c is a con- stant. For English, c ≈ 0.1. Figure 4.1 shows the graph of Zipf ’s law with this constant.  is clearly shows how the frequency of word occurrence falls rapidly a er the  rst few most common words.
2 Named a er the American linguist George Kingsley Zipf.
4.2 TextStatistics 75
 
76 4 ProcessingText
0.1
0.09
0.08
0.07
0.06
Probability 0.05
(of occurrence)
0.04
0.03 0.02 0.01
0
0 10 20 30 40 50 60 70 80 90 100
Rank  (by decreasing frequency)
Fig. 4.1. Rank versus probability of occurrence for words assuming Zipf ’s law (rank × probability = 0.1)
To see how well Zipf ’s law predicts word occurrences in actual text collec- tions, we will use the Associated Press collection of news stories from 1989 (called AP89) as an example.  is collection was used in TREC evaluations for several years. Table 4.1 shows some statistics for the word occurrences in AP89.  e vo- cabulary size is the number of unique words in the collection. Even in this rela- tively small collection, the vocabulary size is quite large (nearly 200,000 unique words). A large proportion of these words (70,000) occur only once. Words that occur once in a text corpus or book have long been regarded as important in text analysis, and have been given the special name of Hapax Legomena.3
Table 4.2 shows the 50 most frequent words from the AP89 collection, to- gether with their frequencies, ranks, probability of occurrence (converted to a percentage of total occurrences), and the r.Pr value. From this table, we can see
3  e name was created by scholars studying the Bible. Since the 13th century, people have studied the word occurrences in the Bible and, of particular interest, created con- cordances, which are indexes of where words occur in the text. Concordances are the ancestors of the inverted  les that are used in modern search engines.  e  rst concor- dance was said to have required 500 monks to create.
     
Total documents
Total word occurrences Vocabulary size
Words occurring > 1000 times Words occurring once
4.2 TextStatistics 77
84,678 39,749,179 198,763 4,169 70,064
Table 4.1. Statistics for the AP89 collection
that Zipf ’s law is quite accurate, in that the value of r.Pr is approximately con- stant, and close to 0.1.  e biggest variations are for some of the most frequent words. In fact, it is generally observed that Zipf ’s law is inaccurate for low and high ranks (high-frequency and low-frequency words). Table 4.3 gives some ex- amples for lower-frequency words from AP89.
Figure 4.2 shows a log-log plot4 of the r.Pr values for all words in the AP89 collection. Zipf ’s law is shown as a straight line on this graph since log Pr = log(c · r−1) = log c − log r.  is  gure clearly shows how the predicted re- lationship breaks down at high ranks (approximately rank 10,000 and above). A number of modi cations to Zipf ’s law have been proposed,5 some of which have interesting connections to cognitive models of language.
It is possible to derive a simple formula for predicting the proportion of words with a given frequency from Zipf ’s law. A word that occurs n times has rank rn = k/n. In general, more than one word may have the same frequency. We assume that the rank rn is associated with the last of the group of words with the same frequency. In that case, the number of words with the same frequency n will be given by rn − rn+1, which is the rank of the last word in the group minus the rank of the last word of the previous group of words with a higher frequency (remember that higher-frequency words have lower ranks). For example, Table 4.4 has an example of a ranking of words in decreasing order of their frequency.  e number of words with frequency 5,099 is the rank of the last member of that
4  e x and y axes of a log-log plot show the logarithm of the values of x and y, not the values themselves.
5  e most well-known is the derivation by the mathematician Benoit Mandelbrot (the same person who developed fractal geometry), which is (r + β)α · Pr = γ, where β, α, and γ are parameters that can be tuned for a particular text. In the case of the AP89 collection, however, the  t for the frequency data is not noticeably better than the Zipf distribution.
 
78
4 ProcessingText
Word Freq.
the 2,420,778 of 1,045,733 to 968,882 a 892,429 and 865,644 in 847,825 said 504,593 for 363,865 that 347,072 was 293,027 on 291,947 he 250,919 is 245,843 with 223,846 at 210,064 by 209,586 it 195,621 from 189,451 as 181,714 be 157,300 were 153,913 an 152,576 have 149,749 his 142,285 but 140,880
r Pr (%)
1 6.49
2 2.80
3 2.60
4 2.39
5 2.32
6 2.27
7 1.35
8 0.98
9 0.93
10 0.79
11 0.78
12 0.67
13 0.65
14 0.60
15 0.56
16 0.56
17 0.52
18 0.51
19 0.49
20 0.42
21 0.41
22 0.41
23 0.40
24 0.38
25 0.38
r.Pr   Word 0.065   has 0.056   are 0.078   not 0.096   who 0.120   they 0.140   its 0.095   had 0.078   will 0.084   would 0.079   about 0.086   i
0.081   been 0.086   this 0.084   their 0.085   new 0.090   or 0.089   which 0.091   we 0.093   more 0.084   a er 0.087   us 0.090   percent 0.092   up 0.092   one 0.094   people
Freq r
136,007 26 130,322 27 127,493 28 116,364 29 111,024 30 111,021 31 103,943 32 102,949 33
99,503 34 92,983 35 92,005 36 88,786 37 87,286 38 84,638 39 83,449 40 81,796 41 80,385 42 80,245 43 76,388 44 75,165 45 72,045 46 71,956 47 71,082 48 70,266 49 68,988 50
Pr (%) 0.37 0.35 0.34 0.31 0.30 0.30 0.28 0.28 0.27 0.25 0.25 0.24 0.23 0.23 0.22 0.22 0.22 0.22 0.21 0.20 0.19 0.19 0.19 0.19 0.19
r.Pr 0.095 0.094 0.096 0.090 0.089 0.092 0.089 0.091 0.091 0.087 0.089 0.088 0.089 0.089 0.090 0.090 0.091 0.093 0.090 0.091 0.089 0.091 0.092 0.092 0.093
  Table 4.2. Most frequent 50 words from AP89
Word Freq.
assistant 5,095 sewers 100 toothbrush 10 hazmat 1
r
1,021 17,110 51,555
Pr (%) r.Pr .013 0.13 .000256 0.04 .000025 0.01 .000002 0.04
 166,945
Table 4.3. Low-frequency words from AP89
 
 1
0.1
0.01
0.001
Pr 0.0001
1e-005
1e-006
1e-007
1e-008
Zipf
AP89
1 10
100 1000
Rank
10000
100000
1e+006
Fig. 4.2. A log-log plot of Zipf ’s law compared to real data from AP89.  e predicted relationship between probability of occurrence and rank breaks down badly at high ranks.
group (“chemical”) minus the rank of the last member of the previous group with higher frequency (“summit”), which is 1006 − 1002 = 4.
Rank
1000
1001
1002
1003
1004
1005
1006
1007
Word
concern spoke summit bring star immediate chemical african
Frequency
5,100 5,100 5,100 5,099 5,099 5,099 5,099 5,098
Table 4.4. Example word frequency ranking
4.2 TextStatistics 79
80 4 ProcessingText
Given that the number of words with frequency n is rn − rn+1 = k/n − k/(n + 1) = k/n(n + 1), then the proportion of words with this frequency can be found by dividing this number by the total number of words, which will be the rank of the last word with frequency 1.  e rank of the last word in the vocabulary is k/1 = k.  e proportion of words with frequency n, therefore, is given by 1/n(n + 1).  is formula predicts, for example, that 1/2 of the words in thevocabularywilloccuronce.Table4.5comparesthepredictionsofthisformula with real data from a different TREC collection.
Number of Predicted Actual Occurrences Proportion Proportion
(n) (1/n(n+1))
1 0.500 0.402 2 0.167 0.132 3 0.083 0.069 4 0.050 0.046 5 0.033 0.032 6 0.024 0.024 7 0.018 0.019 8 0.014 0.016 9 0.011 0.014
10 0.009 0.012
Table 4.5. Proportions of words occurring n times
TREC Volume 3 corpus.  e total vocabulary size (number of unique words) is 508,209.
4.2.1 Vocabulary Growth
Another useful prediction related to word occurrence is vocabulary growth. As the size of the corpus grows, new words occur. Based on the assumption of a Zipf dis- tribution for words, we would expect that the number of new words that occur in a given amount of new text would decrease as the size of the corpus increases. New words will, however, always occur due to sources such as invented words (think of all those drug names and start-up company names), spelling errors, product numbers, people’s names, email addresses, and many others.  e relationship be- tween the size of the corpus and the size of the vocabulary was found empirically by Heaps (1978) to be:
Actual Number of Words
204,357 67,082 35,083 23,271 16,332 12,421 9,766 8,200 6,907 5,893
  in 336,310 documents from the
v = k · nβ
where v is the vocabulary size for a corpus of size n words, and k and β are pa- rameters that vary for each collection.  is is sometimes referred to as Heaps’ law. Typical values for k and β are o en stated to be 10 ≤ k ≤ 100 and β ≈ 0.5. Heaps’ law predicts that the number of new words will increase very rapidly when the corpus is small and will continue to increase inde nitely, but at a slower rate for larger corpora. Figure 4.3 shows a plot of vocabulary growth for the AP89 collection compared to a graph of Heaps’ law with k = 62.95 and β = 0.455. Clearly, Heaps’ law is a good  t.  e parameter values are similar for many of the other TREC news collections. As an example of the accuracy of this predic- tion, if the  rst 10,879,522 words of the AP89 collection are scanned, Heaps’ law predicts that the number of unique words will be 100,151, whereas the actual value is 100,024. Predictions are much less accurate for small numbers of words (< 1,000).
200000 180000 160000 140000 120000 100000
80000 60000 40000 20000
0
AP89
Heaps 62.95, 0.455
4.2 TextStatistics 81
                      Words in Vocabulary
                       0 5e+006
1e+007 1.5e+007
2e+007 2.5e+007
3e+007 3.5e+007
4e+007
Words in Collection
Fig. 4.3. Vocabulary growth for the TREC AP89 collection compared to Heaps’ law
82 4 ProcessingText
Web-scale collections are considerably larger than the AP89 collection.  e AP89 collection contains about 40 million words, but the (relatively small) TREC Web collection GOV26 contains more than 20 billion words. With that many words, it seems likely that the number of new words would eventually drop to near zero and Heaps’ law would not be applicable. It turns out this is not the case. Figure 4.4 shows a plot of vocabulary growth for GOV2 together with a graph of Heaps’ law with k = 7.34 and β = 0.648.  is data indicates that the number of unique words continues to grow steadily even a er reaching 30 million.  is has signi cant implications for the design of search engines, which will be discussed in Chapter 5. Heaps’ law provides a good  t for this data, although the parameter values are very different than those for other TREC collections and outside the boundaries established as typical with these and other smaller collections.
4.5e+007 4e+007 3.5e+007 3e+007 2.5e+007 2e+007 1.5e+007 1e+007 5e+006 0
GOV2
Heaps 7.34, 0.648
                   Words in Vocabulary
                  0 5e+009
1e+010 1.5e+010
Words in Collection
2e+010 2.5e+010
Fig. 4.4. Vocabulary growth for the TREC GOV2 collection compared to Heaps’ law
6 Webpagescrawledfromwebsitesinthe.govdomainduringearly2004.Seesection8.2 for more details.
 
4.2.2 Estimating Collection and Result Set Sizes
Wordoccurrencestatisticscanalsobeusedtoestimatethesizeoftheresultsfroma web search. All web search engines have some version of the query interface shown in Figure 4.5, where immediately a er the query (“tropical  sh aquarium” in this case) and before the ranked list of results, an estimate of the total number of results is given.  is is typically a very large number, and descriptions of these systems always point out that it is just an estimate. Nevertheless, it is always included.
4.2 TextStatistics 83
  tropical fish aquarium
Web results Page 1 of 3,880,000 results
 
Fig. 4.5. Result size estimate for web search
To estimate the size of a result set, we  rst need to de ne “results.” For the purposes of this estimation, a result is any document (or web page) that contains all of the query words. Some search applications will rank documents that do not contain all the query words, but given the huge size of the Web, this is usually not necessary. If we assume that words occur independently of each other, then the probability of a document containing all the words in the query is simply the product of the probabilities of the individual words occurring in a document. For example, if there are three query words a, b, and c, then:
P(a∩b∩c) = P(a)·P(b)·P(c)
where P (a ∩ b ∩ c) is the joint probability, or the probability that all three words occur in a document, and P (a), P (b), and P (c) are the probabilities of each word occurring in a document. A search engine will always have access to the number of documents that a word occurs in (fa, fb, and fc),7 and the number of documents in the collection (N ), so these probabilities can easily be estimated as P (a) = fa/N, P(b) = fb/N, and P(c) = fc/N.  is gives us
fabc =N·fa/N·fb/N·fc/N=(fa·fb·fc)/N2 where fabc is the estimated size of the result set.
7 Note that these are document occurrence  equencies, not the total number of word oc- currences (there may be many occurrences of a word in a document).
Search
 
84 4 ProcessingText
Document Estimated Word(s) Frequency Frequency
 tropical
 sh
aquarium
breeding
tropical  sh
tropical aquarium tropical breeding
 sh aquarium
 sh breeding aquarium breeding tropical  sh aquarium tropical  sh breeding
120,990 1,131,855 26,480 81,885
18,472 5,433 1,921 127 5,510 393 9,722 1,189
36,427 3,677 1,848 86 1,529 6 3,629 18
 Table 4.6. Document frequencies and estimated frequencies for word combinations (as- suming independence) in the GOV2 Web collection. Collection size (N ) is 25,205,179.
Table 4.6 gives document occurrence frequencies for the words “tropical”, “ sh”, “aquarium”, and “breeding”, and for combinations of those words in the TREC GOV2 Web collection. It also gives the estimated size of the frequencies of the combinations based on the independence assumption. Clearly, this assump- tion does not lead to good estimates for result size, especially for combinations of three words.  e problem is that the words in these combinations do not occur independently of each other. If we see the word “ sh” in a document, for example, then the word “aquarium” is more likely to occur in this document than in one that does not contain “ sh”.
Better estimates are possible if word co-occurrence information is also avail- able from the search engine. Obviously, this would give exact answers for two- word queries. For longer queries, we can improve the estimate by not assuming independence. In general, for three words
P(a∩b∩c) = P(a∩b)·P(c|(a∩b))
where P (a ∩ b) is the probability that the words a and b co-occur in a document, and P (c|(a∩b)) is the probability that the word c occurs in a document given that the words a and b occur in the document.8 If we have co-occurrence information,
8  is is called a conditional probability.
 
4.2 TextStatistics 85
we can approximate this probability using either P (c|a) or P (c|b), whichever is the largest. For the example query “tropical  sh aquarium” in Table 4.6, this means we estimate the result set size by multiplying the number of documents containing both “tropical” and “aquarium” by the probability that a document contains “ sh” given that it contains “aquarium”, or:
ftropical∩fish∩aquarium = ftropical∩aquarium · ffish∩aquarium/faquarium = 1921 · 9722/26480 = 705
Similarly, for the query “tropical  sh breeding”:
ftropical∩fish∩breeding =ftropical∩breeding ·ffish∩breeeding/fbreeding = 5510 · 36427/81885 = 2451
 ese estimates are much better than the ones produced assuming indepen- dence, but they are still too low. Rather than storing even more information, such as the number of occurrences of word triples, it turns out that reasonable esti- mates of result size can be made using just word frequency and the size of the cur- rent result set. Search engines estimate the result size because they do not rank all the documents that contain the query words. Instead, they rank a much smaller subset of the documents that are likely to be the most relevant. If we know the proportion of the total documents that have been ranked (s) and the number of documents found that contain all the query words (C), we can simply estimate the result size as C /s, which assumes that the documents containing all the words are distributed uniformly.9  e proportion of documents processed is measured by the proportion of the documents containing the least frequent word that have been processed, since all results must contain that word.
For example, if the query “tropical  sh aquarium” is used to rank GOV2 doc- uments in the Galago search engine, a er processing 3,000 out of the 26,480 doc- uments that contain “aquarium”, the number of documents containing all three words is 258.  is gives a result size estimate of 258/(3,000 ÷ 26,480) = 2,277. A er processing just over 20% of the documents, the estimate is 1,778 (compared to the actual  gure of 1,529). For the query “tropical  sh breeding”, the estimates a er processing 10% and 20% of the documents that contain “breeding” are 4,076
9 We are also assuming document-at-a-time processing, where the inverted lists for all query words are processed at the same time, giving complete document scores (see Chapter 5).
 
86 4 ProcessingText
and 3,762 (compared to 3,629).  ese estimates, as well as being quite accurate, do not require knowledge of the total number of documents in the collection.
Estimating the total number of documents stored in a search engine is, in fact, ofsigni cantinteresttobothacademia(howbigistheWeb?)andbusiness(which search engine has better coverage of the Web?). A number of papers have been written about techniques to do this, and one of these is based on the concept of word independence that we used before. If a and b are two words that occur independently, then
and
fab/N = fa/N · fb/N N = (fa · fb)/fab
To get a reasonable estimate of N , the two words should be independent and, as we have seen from the examples in Table 4.6, this is o en not the case. We can be more careful about the choice of query words, however. For example, if we use the word “lincoln” (document frequency 771,326 in GOV2), we would expect the words in the query “tropical lincoln” to be more independent than the word pairs in Table 4.6 (since the former are less semantically related).  e document frequency of “tropical lincoln” in GOV2 is 3,018, which means we can estimate the size of the collection as N = (120,990 · 771,326)/3,018 = 30,922,045.  is is quite close to the actual number of 25,205,179.
4.3 Document Parsing
4.3.1 Overview
Document parsing involves the recognition of the content and structure of text documents.  e primary content of most documents is the words that we were counting and modeling using the Zipf distribution in the previous section. Rec- ognizing each word occurrence in the sequence of characters in a document is called tokenizing or lexical analysis. Apart from these words, there can be many other types of content in a document, such as metadata, images, graphics, code, and tables. As mentioned in Chapter 2, metadata is information about a doc- ument that is not part of the text content. Metadata content includes docu- ment attributes such as date and author, and, most importantly, the tags that are used by markup languages to identify document components.  e most popular
4.3 DocumentParsing 87
markup languages are HTML (Hypertext Markup Language) and XML (Exten- sible Markup Language).
 e parser uses the tags and other metadata recognized in the document to interpret the document’s structure based on the syntax of the markup language (syntactic analysis) and to produce a representation of the document that includes both the structure and content. For example, an HTML parser interprets the structure of a web page as speci ed using HTML tags, and creates a Document Object Model (DOM) representation of the page that is used by a web browser. In a search engine, the output of a document parser is a representation of the con- tent and structure that will be used for creating indexes. Since it is important for a search index to represent every document in a collection, a document parser for a search engine is o en more tolerant of syntax errors than parsers used in other applications.
In the  rst part of our discussion of document parsing, we focus on the recog- nition of the tokens, words, and phrases that make up the content of the docu- ments. In later sections, we discuss separately the important topics related to doc- ument structure, namely markup, links, and extraction of structure from the text content.
4.3.2 Tokenizing
Tokenizing is the process of forming words from the sequence of characters in a document. In English text, this appears to be simple. In many early systems, a “word” was de ned as any sequence of alphanumeric characters of length 3 or more, terminated by a space or other special character. All uppercase letters were also converted to lowercase.10  is means, for example, that the text
Bigcorp’s 2007 bi-annual report showed profits rose 10%.
would produce the following sequence of tokens:
bigcorp 2007 annual report showed profits rose
Although this simple tokenizing process was adequate for experiments with small test collections, it does not seem appropriate for most search applications or even experiments with TREC collections, because too much information is discarded. Some examples of issues involving tokenizing that can have signi cant impact on the effectiveness of search are:
10  is is sometimes referred to as case folding, case normalization, or downcasing.
 
88 4 ProcessingText
• Small words (one or two characters) can be important in some queries, usually in combinations with other words. For example, xp, ma, pm, ben e king, el paso, master p, gm, j lo, world war II.11
• Both hyphenated and non-hyphenated forms of many words are common. In some cases the hyphen is not needed. For example, e-bay, wal-mart, active-x, cd-rom, t-shirts. At other times, hyphens should be considered either as part of the word or a word separator. For example, winston-salem, mazda rx-7, e-cards, pre-diabetes, t-mobile, spanish-speaking.
• Special characters are an important part of the tags, URLs, code, and other important parts of documents that must be correctly tokenized.
• Capitalized words can have different meaning from lowercase words. For ex- ample, “Bush” and “Apple”.
• Apostrophes can be a part of a word, a part of a possessive, or just a mistake. For example, rosie o’donnell, can’t, don’t, 80’s, 1890’s, men’s straw hats, master’s degree, england’s ten largest cities, shriner’s.
• Numbers can be important, including decimals. For example, nokia 3250, top 10 courses, united 93, quicktime 6.5 pro, 92.3 the beat, 288358 (yes, this was a real query; it’s a patent number).
• Periods can occur in numbers, abbreviations (e.g., “I.B.M.”, “Ph.D.”), URLs, ends of sentences, and other situations.
From these examples, tokenizing seems more complicated than it  rst appears.  e fact that these examples come from queries also emphasizes that the text pro- cessing for queries must be the same as that used for documents. If different to- kenizing processes are used for queries and documents, many of the index terms used for documents will simply not match the corresponding terms from queries. Mistakes in tokenizing become obvious very quickly through retrieval failures.
To be able to incorporate the range of language processing required to make matching effective, the tokenizing process should be both simple and  exible. One approach to doing this is for the  rst pass of tokenizing to focus entirely on identifying markup or tags in the document.  is could be done using a tok- enizer and parser designed for the speci c markup language used (e.g., HTML), but it should accommodate syntax errors in the structure, as mentioned previ- ously. A second pass of tokenizing can then be done on the appropriate parts of the document structure. Some parts that are not used for searching, such as those containing HTML code, will be ignored in this pass.
11  ese and other examples were taken from a small sample of web queries.
 
4.3 DocumentParsing 89
Given that nearly everything in the text of a document can be important for some query, the tokenizing rules have to convert most of the content to search- able tokens. Instead of trying to do everything in the tokenizer, some of the more difficult issues, such as identifying word variants or recognizing that a string is a name or a date, can be handled by separate processes, including stemming, in- formation extraction, and query transformation. Information extraction usually requires the full form of the text as input, including capitalization and punctua- tion, so this information must be retained until extraction has been done. Apart from this restriction, capitalization is rarely important for searching, and text can be reduced to lowercase for indexing.  is does not mean that capitalized words are not used in queries.  ey are, in fact, used quite o en, but in queries where the capitalization does not reduce ambiguity and so does not impact effective- ness. Words such as “Apple” that are o en used in examples (but not so o en in real queries) can be handled by query reformulation techniques (Chapter 6) or simply by relying on the most popular pages (section 4.5).
If we take the view that complicated issues are handled by other processes, the most general strategy for hyphens, apostrophes, and periods would be to treat them as word terminators (like spaces). It is important that all the tokens pro- duced are indexed, including single characters such as “s” and “o”.  is will mean, for example, that the query12 ”o’connor” is equivalent to ”o connor”, ”bob’s” is equivalent to ”bob s”, and ”rx-7” is equivalent to ”rx 7”. Note that this will also mean that a word such as “rx7” will be a different token than “rx-7” and therefore will be indexed separately.  e task of relating the queries rx 7, rx7, and rx-7 will then be handled by the query transformation component of the search engine.
On the other hand, if we rely entirely on the query transformation component to make the appropriate connections or inferences between words, there is the risk that effectiveness could be lowered, particularly in applications where there is not enough data for reliable query expansion. In these cases, more rules can be incor- porated into the tokenizer to ensure that the tokens produced by the query text will match the tokens produced from document text. For example, in the case of TREC collections, a rule that tokenizes all words containing apostrophes by the string without the apostrophe is very effective. With this rule, “O’Connor” would be tokenized as “oconnor” and “Bob’s” would produce the token “bobs”. Another effective rule for TREC collections is to tokenize all abbreviations con-
12 We assume the common syntax for web queries where ”<words>” means match exactly the phrase contained in the quotes.
 
90 4 ProcessingText
taining periods as the string without periods. An abbreviation in this case is any string of alphabetic single characters separated by periods.  is rule would tok- enize “I.B.M.” as “ibm”, but “Ph.D.” would still be tokenized as “ph d”.
In summary, the most general tokenizing process will involve  rst identify- ing the document structure and then identifying words in text as any sequence of alphanumeric characters, terminated by a space or special character, with ev- erything converted to lowercase.  is is not much more complicated than the simple process we described at the start of the section, but it relies on informa- tion extraction and query transformation to handle the difficult issues. In many cases, additional rules are added to the tokenizer to handle some of the special characters, to ensure that query and document tokens will match.
4.3.3 Stopping
Human language is  lled with function words: words that have little meaning apart from other words.  e most popular—“the,” “a,” “an,” “that,” and “those”—are determiners.  ese words are part of how we describe nouns in text, and express concepts like location or quantity. Prepositions, such as “over,” “under,” “above,” and “below,” represent relative position between two nouns.
Two properties of these function words cause us to want to treat them in a special way in text processing. First, these function words are extremely common. Table 4.2 shows that nearly all of the most frequent words in the AP89 collection fall into this category. Keeping track of the quantity of these words in each docu- ment requires a lot of disk space. Second, both because of their commonness and their function, these words rarely indicate anything about document relevance on their own. If we are considering individual words in the retrieval process and not phrases, these function words will help us very little.
In information retrieval, these function words have a second name: stopwords. We call them stopwords because text processing stops when one is seen, and they are thrown out.  rowing out these words decreases index size, increases retrieval efficiency, and generally improves retrieval effectiveness.
Constructing a stopword list must be done with caution. Removing too many words will hurt retrieval effectiveness in particularly frustrating ways for the user. For instance, the query ”to be or not to be” consists entirely of words that are usu- ally considered stopwords. Although not removing stopwords may cause some problems in ranking, removing stopwords can cause perfectly valid queries to re- turn no results.
4.3 DocumentParsing 91
A stopword list can be constructed by simply using the top n (e.g., 50) most frequent words in a collection.  is can, however, lead to words being included that are important for some queries. More typically, either a standard stopword list is used,13 or a list of frequent words and standard stopwords is manually edited to remove any words that may be signi cant for a particular application. It is also possible to create stopword lists that are customized for speci c parts of the doc- ument structure (also called  elds). For example, the words “click”, “here”, and “privacy” may be reasonable stopwords to use when processing anchor text.
If storage space requirements allow, it is best to at least index all words in the documents. If stopping is required, the stopwords can always be removed from queries. By keeping the stopwords in the index, there will be a number of possi- ble ways to execute a query with stopwords in it. For instance, many systems will remove stopwords from a query unless the word is preceded by a plus sign (+). If keeping stopwords in an index is not possible because of space requirements, as few as possible should be removed in order to maintain maximum  exibility.
4.3.4 Stemming
Part of the expressiveness of natural language comes from the huge number of ways to convey a single idea.  is can be a problem for search engines, which rely on matching words to  nd relevant documents. Instead of restricting matches to words that are identical, a number of techniques have been developed to allow a search engine to match words that are semantically related. Stemming, also called con ation, is a component of text processing that captures the relationships be- tween different variations of a word. More precisely, stemming reduces the dif- ferent forms of a word that occur because of in ection (e.g., plurals, tenses) or derivation (e.g., making a verb into a noun by adding the suffix -ation) to a com- mon stem.
Suppose you want to search for news articles about Mark Spitz’s Olympic swimming career. You might type “mark spitz swimming” into a search engine. However, many news articles are usually summaries of events that have already happened, so they are likely to contain the word “swam” instead of “swimming.” It is the job of the stemmer to reduce “swimming” and “swam” to the same stem (probably “swim”) and thereby allow the search engine to determine that there is a match between these two words.
13 Such as the one distributed with the Lemur toolkit and included with Galago.
 
92 4 ProcessingText
In general, using a stemmer for search applications with English text produces a small but noticeable improvement in the quality of results. In applications in- volving highly in ected languages, such as Arabic or Russian, stemming is a cru- cial part of effective search.
 ere are two basic types of stemmers: algorithmic and dictionary-based. An algorithmic stemmer uses a small program to decide whether two words are re- lated, usually based on knowledge of word suffixes for a particular language. By contrast, a dictionary-based stemmer has no logic of its own, but instead relies on pre-created dictionaries of related terms to store term relationships.
 e simplest kind of English algorithmic stemmer is the suffix-s stemmer.  is kind of stemmer assumes that any word ending in the letter “s” is plural, so cakes → cake, dogs → dog. Of course, this rule is not perfect. It cannot detect many plural relationships, like “century” and “centuries”. In very rare cases, it detects a relationshipwhereitdoesnotexist,suchaswith“I”and“is”. e rstkindoferror is called a false negative, and the second kind of error is called a false positive.14
More complicated algorithmic stemmers reduce the number of false negatives by considering more kinds of suffixes, such as -ing or -ed. By handling more suffix types, the stemmer can  nd more term relationships; in other words, the false negative rate is reduced. However, the false positive rate ( nding a relationship where none exists) generally increases.
 e most popular algorithmic stemmer is the Porter stemmer.15  is has been used in many information retrieval experiments and systems since the 1970s, and a number of implementations are available.  e stemmer consists of a number of steps, each containing a set of rules for removing suffixes. At each step, the rule for the longest applicable suffix is executed. Some of the rules are obvious, whereas others require some thought to work out what they are doing. As an example, here are the  rst two parts of step 1 (of 5 steps):
Step 1a:
- Replace sses by ss (e.g., stresses → stress).
- Delete s if the preceding word part contains a vowel not immediately be-
fore the s (e.g., gaps → gap but gas → gas).
- Replace ied or ies by i if preceded by more than one letter, otherwise by ie
(e.g., ties → tie, cries → cri).
14  ese terms are used in any binary decision process to describe the two types of errors.  is includes evaluation (Chapter 8) and classi cation (Chapter 9).
15 http://tartarus.org/martin/PorterStemmer/
 
4.3 DocumentParsing 93
- If suffix is us or ss do nothing (e.g., stress → stress). Step 1b:
- Replace eed, eedly by ee if it is in the part of the word a er the  rst non-
vowel following a vowel (e.g., agreed → agree, feed → feed).
- Delete ed, edly, ing, ingly if the preceding word part contains a vowel, and then if the word ends in at, bl, or iz add e (e.g., fished → fish, pirating → pirate), or if the word ends with a double letter that is not ll, ss, or zz, remove the last letter (e.g., falling→ fall, dripping → drip), or if the word is short, add
e (e.g., hoping → hope). - Whew!
 e Porter stemmer has been shown to be effective in a number of TREC eval- uations and search applications. It is difficult, however, to capture all the subtleties of a language in a relatively simple algorithm.  e original version of the Porter stemmer made a number of errors, both false positives and false negatives. Table 4.7 shows some of these errors. It is easy to imagine how confusing “execute” with “executive” or “organization” with “organ” could cause signi cant problems in the ranking. A more recent form of the stemmer (called Porter2)16  xes some of these problems and provides a mechanism to specify exceptions.
False positives
organization/organ generalization/generic numerical/numerous policy/police university/universe addition/additive negligible/negligent execute/executive past/paste ignore/ignorant special/specialized head/heading
False negatives
european/europe cylinder/cylindrical matrices/matrix urgency/urgent create/creation analysis/analyses useful/usefully
noise/noisy decompose/decomposition sparse/sparsity resolve/resolution triangle/triangular
 Table 4.7. Examples of errors made by the original Porter stemmer. False positives are pairs of words that have the same stem. False negatives are pairs that have different stems.
16 http://snowball.tartarus.org
 
94 4 ProcessingText
A dictionary-based stemmer provides a different approach to the problem of stemming errors. Instead of trying to detect word relationships from letter pat- terns, we can store lists of related words in a large dictionary. Since these word lists can be created by humans, we can expect that the false positive rate will be very low for these words. Related words do not even need to look similar; a dic- tionary stemmer can recognize that “is,” “be,” and “was” are all forms of the same verb. Unfortunately, the dictionary cannot be in nitely long, so it cannot react automatically to new words.  is is an important problem since language is con- stantly evolving. It is possible to build stem dictionaries automatically by statistical analysis of a text corpus. Since this is particularly useful when stemming is used for query expansion, we discuss this technique in section 6.2.1.
Another strategy is to combine an algorithmic stemmer with a dictionary- based stemmer. Typically, irregular words such as the verb “to be” are the oldest in the language, while new words follow more regular grammatical conventions.  is means that newly invented words are likely to work well with an algorith- mic stemmer. A dictionary can be used to detect relationships between common words, and the algorithmic stemmer can be used for unrecognized words.
A well-known example of this hybrid approach is the Krovetz stemmer (Kro- vetz, 1993).  is stemmer makes constant use of a dictionary to check whether the word is valid.  e dictionary in the Krovetz stemmer is based on a general English dictionary but also uses exceptions that are generated manually. Before being stemmed, the dictionary is checked to see whether a word is present; if it is, it is either le  alone (if it is in the general dictionary) or stemmed based on the exception entry. If the word is not in the dictionary, it is checked for a list of common in ectional and derivational suffixes. If one is found, it is removed and the dictionary is again checked to see whether the word is present. If it is not found, the ending of the word may be modi ed based on the ending that was removed. For example, if the ending -ies is found, it is replaced by -ie and checked in the dictionary. If it is found in the dictionary, the stem is accepted; otherwise the ending is replaced by y.  is will result in calories → calorie, for example.  e suffixes are checked in a sequence (for example, plurals before -ion endings), so multiple suffixes may be removed.
 e Krovetz stemmer has a lower false positive rate than the Porter stemmer, but also tends to have a higher false negative rate, depending on the size of the ex- ception dictionaries. Overall, the effectiveness of the two stemmers is comparable when used in search evaluations.  e Krovetz stemmer has the additional advan- tage of producing stems that, in most cases, are full words, whereas the Porter
Original text: 
Document will describe marketing strategies carried out by U.S. companies for their agricultural  chemicals, report predictions for market share of such chemicals, or report market statistics for  agrochemicals, pesticide, herbicide, fungicide, insecticide, fertilizer, predicted sales, market share,  stimulate demand, price cut, volume of sales. 
 
Porter stemmer: 
 document describ market strategi carri compani agricultur chemic report predict market share chemic  report market statist agrochem pesticid herbicid fungicid insecticid fertil predict sale market share  stimul demand price cut volum sale 
 
Krovetz stemmer: 
 document describe marketing strategy carry company agriculture chemical report prediction market  share chemical report market statistic agrochemic pesticide herbicide fungicide insecticide fertilizer  predict sale stimulate demand price cut volume sale 
Fig. 4.6. Comparison of stemmer output for a TREC query. Stopwords have also been removed.
stemmer o en produces stems that are word fragments.  is is a concern if the stems are used in the search interface.
Figure 4.6 compares the output of the Porter and Krovetz stemmers on the text of a TREC query.  e output of the Krovetz stemmer is similar in terms of which words are reduced to the same stems, although “marketing” is not reduced to “market” because it was in the dictionary.  e stems produced by the Krovetz stemmer are mostly words.  e exception is the stem “agrochemic”, which oc- curred because “agrochemical” was not in the dictionary. Note that text process- ing in this example has removed stopwords, including single characters.  is re- sulted in the removal of “U.S.” from the text, which could have signi cant conse- quences for some queries.  is can be handled by better tokenization or informa- tion extraction, as we discuss in section 4.6.
As in the case of stopwords, the search engine will have more  exibility to an- swer a broad range of queries if the document words are not stemmed but instead are indexed in their original form. Stemming can then be done as a type of query expansion, as explained in section 6.2.1. In some applications, both the full words and their stems are indexed, in order to provide both  exibility and efficient query processing times.
We mentioned earlier that stemming can be particularly important for some languages, and have virtually no impact in others. Incorporating language-speci c
4.3 DocumentParsing 95
96 4 ProcessingText
stemming algorithms is one of the most important aspects of customizing, or in- ternationalizing, a search engine for multiple languages. We discuss other aspects of internationalization in section 4.7, but focus on the stemming issues here.
As an example, Table 4.8 shows some of the Arabic words derived from the same root. A stemming algorithm that reduced Arabic words to their roots would clearly not work (there are less than 2,000 roots in Arabic), but a broad range of pre xes and suffixes must be considered. Highly in ectional languages like Ara- bic have many word variants, and stemming can make a large difference in the accuracy of the ranking. An Arabic search engine with high-quality stemming can be more than 50% more effective, on average, at  nding relevant documents than a system without stemming. In contrast, improvements for an English search engine vary from less than 5% on average for large collections to about 10% for small, domain-speci c collections.
 kitab kitabi alkitab kitabuki kitabuka kitabuhu kataba maktaba maktab
a book
my book
the book
your book (f ) your book (m) his book
to write
library, bookstore office
 Table 4.8. Examples of words with the Arabic root ktb
Fortunately, stemmers for a number of languages have already been developed and are available as open source so ware. For example, the Porter stemmer is avail- able in French, Spanish, Portuguese, Italian, Romanian, German, Dutch, Swedish, Norwegian, Danish, Russian, Finnish, Hungarian, and Turkish.17 In addition, the statistical approach to building a stemmer that is described in section 6.2.1 can be used when only a text corpus is available.
17 http://snowball.tartarus.org/
 
4.3.5 Phrases and N-grams
Phrases are clearly important in information retrieval. Many of the two- and three-word queries submitted to search engines are phrases, and  nding docu- ments that contain those phrases will be part of any effective ranking algorithm. For example, given the query “black sea”, documents that contain that phrase are much more likely to be relevant than documents containing text such as “the sea turned black”. Phrases are more precise than single words as topic descriptions (e.g., “tropical  sh” versus “ sh”) and usually less ambiguous (e.g., “rotten ap- ple” versus “apple”).  e impact of phrases on retrieval can be complex, however. Given a query such as “ shing supplies”, should the retrieved documents contain exactly that phrase, or should they get credit for containing the words “ sh“, “ sh- ing”, and “supplies” in the same paragraph, or even the same document?  e de- tails of how phrases affect ranking will depend on the speci c retrieval model that is incorporated into the search engine, so we will defer this discussion until Chap- ter 7. From the perspective of text processing, the issue is whether phrases should be identi ed at the same time as tokenizing and stemming, so that they can be indexed for faster query processing.
 ere are a number of possible de nitions of a phrase, and most of them have been studied in retrieval experiments over the years. Since a phrase has a grammat- ical de nition, it seems reasonable to identify phrases using the syntactic structure of sentences.  e de nition that has been used most frequently in information re- trieval research is that a phrase is equivalent to a simple noun phrase.  is is o en restricted even further to include just sequences of nouns, or adjectives followed by nouns. Phrases de ned by these criteria can be identi ed using a part-of-speech (POS) tagger. A POS tagger marks the words in a text with labels corresponding to the part-of-speech of the word in that context. Taggers are based on statistical or rule-based approaches and are trained using large corpora that have been man- ually labeled. Typical tags that are used to label the words include NN (singular noun), NNS (plural noun), VB (verb), VBD (verb, past tense), VBN (verb, past participle), IN (preposition), JJ (adjective), CC (conjunction, e.g., “and”, “or”), PRP (pronoun), and MD (modal auxiliary, e.g., “can”, “will”).
Figure 4.7 shows the output of a POS tagger for the TREC query text used in Figure 4.6.  is example shows that the tagger can identify phrases that are sequences of nouns, such as “marketing/NN strategies/NNS”, or adjectives fol- lowed by nouns, such as “agricultural/JJ chemicals/NNS”. Taggers do, however, make mistakes.  e words “predicted/VBN sales/NNS” would not be identi ed as a noun phrase, because “predicted” is tagged as a verb.
4.3 DocumentParsing 97
98 4 ProcessingText
Original text: 
Document will describe marketing strategies carried out by U.S. companies for their agricultural  chemicals, report predictions for market share of such chemicals, or report market statistics for  agrochemicals, pesticide, herbicide, fungicide, insecticide, fertilizer, predicted sales, market share,  stimulate demand, price cut, volume of sales. 
 
Brill tagger: 
Document/NN will/MD describe/VB marketing/NN strategies/NNS carried/VBD out/IN by/IN U.S./NNP  companies/NNS for/IN their/PRP agricultural/JJ chemicals/NNS ,/, report/NN predictions/NNS for/IN  market/NN share/NN of/IN such/JJ chemicals/NNS ,/, or/CC report/NN market/NN statistics/NNS for/IN  agrochemicals/NNS ,/, pesticide/NN ,/, herbicide/NN ,/, fungicide/NN ,/, insecticide/NN ,/, fertilizer/NN  ,/, predicted/VBN sales/NNS ,/, market/NN share/NN ,/, stimulate/VB demand/NN ,/, price/NN cut/NN  ,/, volume/NN of/IN sales/NNS ./. 
Fig. 4.7. Output of a POS tagger for a TREC query
Table 4.9 shows the high-frequency simple noun phrases from a TREC cor- pus consisting mainly of news stories and a corpus of comparable size consisting of all the 1996 patents issued by the United States Patent and Trademark Of-  ce (PTO).  e phrases were identi ed by POS tagging.  e frequencies of the example phrases indicate that phrases are used more frequently in the PTO col- lection, because patents are written in a formal, legal style with considerable repe- tition.  ere were 1,100,000 phrases in the TREC collection that occurred more than  ve times, and 3,700,000 phrases in the PTO collection. Many of the TREC phrases are proper nouns, such as “los angeles” or “european union”, or are topics that will be important for retrieval, such as “peace process” and “human rights”. Two phrases are associated with the format of the documents (“article type”, “end recording”). On the other hand, most of the high-frequency phrases in the PTO collection are standard terms used to describe all patents, such as“present inven- tion” and “preferred embodiment”, and relatively few are related to the content of the patents, such as “carbon atoms” and “ethyl acetate”. One of the phrases, “group consisting”, was the result of a frequent tagging error.
Although POS tagging produces reasonable phrases and is used in a number of applications, in general it is too slow to be used as the basis for phrase index- ing of large collections.  ere are simpler and faster alternatives that are just as effective. One approach is to store word position information in the indexes and use this information to identify phrases only when a query is processed.  is pro- vides considerable  exibility in that phrases can be identi ed by the user or by using POS tagging on the query, and they are not restricted to adjacent groups of
TREC data
Frequency
65824
61327
33864
18062
17788
17308
15513
15009
12869
12799
12067
10811
9912
8127
7640
7620
7524
7436
7362
7086
6792
6348
6157
5955
5837
Phrase
united states article type
los angeles
hong kong
north korea
new york
san diego
orange county prime minister  rst time
soviet union russian federation united nations southern california south korea
end recording european union south africa
san francisco news conference city council middle east peace process human rights white house
Patent data
Frequency
975362
191625
147352
95097
87903
81809
78458
75850
66407
59828
58724
56715
54619
54117
52195
52003
46299
41694
40554
37911
35827
34881
33947
32338
30193
4.3 DocumentParsing 99
Phrase
present invention
u.s. pat
preferred embodiment carbon atoms
group consisting room temperature seq id
brief description prior art perspective view  rst embodiment reaction mixture detailed description ethyl acetate example 1
block diagram
second embodiment accompanying drawings output signal
 rst end
second end
appended claims
distal end cross-sectional view outer surface
  Table 4.9. High-frequency noun phrases from a TREC collection and U.S. patents from 1996
words.  e identi cation of syntactic phrases is replaced by testing word proxim- ity constraints, such as whether two words occur within a speci ed text window. WedescribepositionindexinginChapter5andretrievalmodelsthatexploitword proximity in Chapter 7.
In applications with large collections and tight constraints on response time, such as web search, testing word proximities at query time is also likely to be too slow. In that case, we can go back to identifying phrases in the documents dur-
100 4 ProcessingText
ing text processing, but use a much simpler de nition of a phrase: any sequence of n words.  is is also known as an n-gram. Sequences of two words are called bigrams, and sequences of three words are called trigrams. Single words are called unigrams. N-grams have been used in many text applications and we will mention them again frequently in this book, particularly in association with language mod- els (section 7.3). In this discussion, we are focusing on word n-grams, but character n-grams are also used in applications such as OCR, where the text is “noisy” and word matching can be difficult (section 11.6). Character n-grams are also used for indexing languages such as Chinese that have no word breaks (section 4.7). N-grams, both character and word, are generated by choosing a particular value for n and then moving that “window” forward one unit (character or word) at a time. In other words, n-grams overlap. For example, the word “tropical” con- tains the following character bigrams: tr, ro, op, pi, ic, ca, and al. Indexes based on n-grams are obviously larger than word indexes.
 e more frequently a word n-gram occurs, the more likely it is to correspond to a meaningful phrase in the language. N-grams of all lengths form a Zipf distri- bution, with a few common phrases occurring very frequently and a large number occurring with frequency 1. In fact, the rank-frequency data for n-grams (which includes single words)  ts the Zipf distribution better than words alone. Some of the most common n-grams will be made up of stopwords (e.g., “and the”, “there is”) and could be ignored, although as with words, we should be cautious about discarding information. Our previous example query “to be or not to be” could certainly make use of n-grams. We could potentially index all n-grams in a doc- ument text up to a speci c length and make them available to the ranking algo- rithm.  is would seem to be an extravagant use of indexing time and disk space because of the large number of possible n-grams. A document containing 1,000 words, for example, would contain 3,990 instances of word n-grams of length 2 ≤ n ≤ 5. Many web search engines, however, use n-gram indexing because it provides a fast method of incorporating phrase features in the ranking.
Google recently made available a  le of n-grams derived from web pages.18  e statistics for this sample are shown in Table 4.10. An analysis of n-grams on the Web (Yang et al., 2007) found that “all rights reserved” was the most frequent trigram in English, whereas “limited liability corporation” was the most frequent in Chinese. In both cases, this was due to the large number of corporate sites, but
18 http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to- you.html
 
4.4 DocumentStructureandMarkup 101
it also indicates that n-grams are not dominated by common patterns of speech such as “and will be”.
Number of tokens: Number of sentences: Number of unigrams: Number of bigrams: Number of trigrams: Number of fourgrams: Number of  vegrams:
1,024,908,267,229 95,119,665,584 13,588,391 314,843,401 977,069,902 1,313,818,354 1,176,470,663
Table 4.10. Statistics for the Google n-gram sample
4.4 Document Structure and Markup
In database applications, the  elds or attributes of database records are a critical part of searching. Queries are speci ed in terms of the required values of these  elds. In some text applications, such as email or literature search,  elds such as author and date will have similar importance and will be part of the query speci - cation. In the case of web search, queries usually do not refer to document struc- ture or  elds, but that does not mean that this structure is unimportant. Some parts of the structure of web pages, indicated by HTML markup, are very signi - cant features used by the ranking algorithm.  e document parser must recognize this structure and make it available for indexing.
As an example, Figure 4.8 shows part of a web page for a Wikipedia19 entry.  e page has some obvious structure that could be used in a ranking algorithm.  e main heading for the page, “tropical  sh”, indicates that this phrase is particu- larly important.  e same phrase is also in bold and italics in the body of the text, which is further evidence of its importance. Other words and phrases are used as the anchor text for links and are likely to be good terms to represent the content of the page.
 e HTML source for this web page (Figure 4.9) shows that there is even more structure that should be represented for search. Each  eld or element in HTML is indicated by a start tag (such as <h1>) and an optional end tag (e.g.,
19  e Web encyclopedia, http://en.wikipedia.org/.
 
102 4 ProcessingText
Tropical fish
From Wikipedia, the free encyclopedia
Tropical fish include fish found in tropical environments around the world, including both freshwater and salt water species. Fishkeepers often use the term tropical fish to refer only those requiring fresh water, with saltwater tropical fish referred to as marine fish.
Tropical fish are popular aquarium fish , due to their often bright coloration. In freshwater fish, this coloration typically derives from iridescence, while salt water fish are generally pigmented.
Fig. 4.8. Part of a web page from Wikipedia
</h1>).20 Elements can also have attributes (with values), given by attribute_name = ”value” pairs.  e <head> element of an HTML document contains metadata that is not displayed by a browser.  e metadata element for keywords (<meta name=”keywords”) gives a list of words and phrases that can be used as additional content terms for the page. In this case, these are the titles of other Wikipedia pages.  e <title> metadata element gives the title for the page (which is different from the main heading).
 e <body> element of the document contains the content that is displayed.  e main heading is indicated by the <h1> tag. Other headings, of different sizes and potentially different importance, would be indicated by <h2> through <h6> tags. Terms that should be displayed in bold or italic are indicated by <b> and <i> tags. Unlike typical database  elds, these tags are primarily used for format- ting and can occur many times in a document.  ey can also, as we have said, be interpreted as a tag indicating a word or phrase of some importance.
Links, such as <a href=”/wiki/Fish” title=”Fish”>fish</a>, are very common.  ey are the basis of link analysis algorithms such as PageRank (Brin & Page, 1998), but also de ne the anchor text. Links and anchor text are of particular importance to web search and will be described in the next section.  e title attribute for a link is used to provide extra information about that link, although in our example it is the words in the last part of the URL for the associated Wikipedia page. Web search engines also make use of the URL of a page as a source of additional meta- data.  e URL for this page is:
http://en.wikipedia.org/wiki/Tropical_fish
20 In XML the end tag is not optional.
           
4.4 DocumentStructureandMarkup 103
 e fact that the words “tropical” and “ sh” occur in the URL will increase the importance of those words for this page.  e depth of a URL (i.e., the number of directories deep the page is) can also be important. For example, the URL www.ibm.com is more likely to be the home page for IBM than a page with the URL:
www.pcworld.com/businesscenter/article/698/ibm_buys_apt!
<html>
<head>
<meta name="keywords" content="Tropical fish, Airstone, Albinism, Algae eater, Aquarium, Aquarium fish feeder, Aquarium furniture, Aquascaping, Bath treatment (fishkeeping),Berlin Method, Biotope" />
...
<title>Tropical fish - Wikipedia, the free encyclopedia</title>
</head>
<body>
...
<h1 class="firstHeading">Tropical fish</h1>
...
<p><b>Tropical fish</b> include <a href="/wiki/Fish" title="Fish">fish</a> found in <a href="/wiki/Tropics" title="Tropics">tropical</a> environments around the world, including both <a href="/wiki/Fresh_water" title="Fresh water">freshwater</a> and <a href="/wiki/Sea_water" title="Sea water">salt water</a> species. <a href="/wiki/Fishkeeping" title="Fishkeeping">Fishkeepers</a> often use the term <i>tropical fish</i> to refer only those requiring fresh water, with saltwater tropical fish referred to as <i><a href="/wiki/List_of_marine_aquarium_fish_species" title="List of marine aquarium fish species">marine fish</a></i>.</p>
<p>Tropical fish are popular <a href="/wiki/Aquarium" title="Aquarium">aquarium</a> fish , due to their often bright coloration. In freshwater fish, this coloration typically derives from <a href="/wiki/Iridescence" title="Iridescence">iridescence</a>, while salt water fish are generally <a href="/wiki/Pigment" title="Pigment">pigmented</a>.</p> ...
</body></html>
Fig. 4.9. HTML source for example Wikipedia page
In HTML, the element types are prede ned and are the same for all docu- ments. XML, in contrast, allows each application to de ne what the element types are and what tags are used to represent them. XML documents can be described by a schema, similar to a database schema. XML elements, consequently, are more closely tied to the semantics of the data than HTML elements. Search applica-
104 4 ProcessingText
tions o en use XML to record semantic annotations in the documents that are produced by information extraction techniques, as described in section 4.6. A document parser for these applications would record the annotations, along with the other document structure, and make them available for indexing.
 e query language XQuery21 has been de ned by the database community for searching structured data described using XML. XQuery supports queries that specify both structural and content constraints, which raises the issue of whether a database or information retrieval approach is better for building a search engine for XML data. We discuss this topic in more detail in section 11.4, but the gen- eral answer is that it will depend on the data, the application, and the user needs. For XML data that contains a substantial proportion of text, the information re- trieval approach is superior. In Chapter 7, we will describe retrieval models that are designed for text documents that contain both structure and metadata.
4.5 Link Analysis
Links connecting pages are a key component of the Web. Links are a power- ful navigational aid for people browsing the Web, but they also help search en- gines understand the relationships between the pages.  ese detected relation- ships help search engines rank web pages more effectively. It should be remem- bered, however, that many document collections used in search applications such as desktop or enterprise search either do not have links or have very little link structure. For these collections, link analysis will have no impact on search per- formance.
As we saw in the last section, a link in a web page is encoded in HTML with a statement such as:
For more information on this topic, please go to <a href=”http://www.somewhere.com”>the somewhere page</a>.
When this page appears in your web browser, the words “the somewhere page” will be displayed differently than regular text, usually underlined or in a different color (or both). When you click on that link, your browser will then load the web page http://www.somewhere.com. In this link, “the somewhere page” is called the anchor text, and http://www.somewhere.com is the destination. Both components are useful in the ranking process.
21 http://www.w3.org/XML/Query/
 
4.5.1 Anchor Text
Anchor text has two properties that make it particularly useful for ranking web pages. First, it tends to be very short, perhaps two or three words, and those words o en succinctly describe the topic of the linked page. For instance, links to www.ebay.com are highly likely to contain the word “eBay” in the anchor text. Many queries are very similar to anchor text in that they are also short topical de- scriptions of web pages.  is suggests a very simple algorithm for ranking pages: search through all links in the collection, looking for anchor text that is an exact match for the user’s query. Each time there is a match, add 1 to the score of the destination page. Pages would then be ranked in decreasing order of this score.  is algorithm has some glaring faults, not the least of which is how to handle the query “click here”. More generally, the collection of all the anchor text in links pointing to a page can be used as an additional text  eld for that page, and incor- porated into the ranking algorithm.
Anchor text is usually written by people who are not the authors of the des- tination page.  is means that the anchor text can describe a destination page from a different perspective, or emphasize the most important aspect of the page from a community viewpoint.  e fact that the link exists at all is a vote of impor- tance for the destination page. Although anchor text is not mentioned as o en as link analysis algorithms (for example, PageRank) in discussions of web search engines, TREC evaluations have shown that it is the most important part of the representation of a page for some types of web search. In particular, it is essential for searches where the user is trying to  nd a home page for a particular topic, person, or organization.
4.5.2 PageRank
 ere are tens of billions of web pages, but most of them are not very interesting. Many of those pages are spam and contain no useful content at all. Other pages are personal blogs, wedding announcements, or family picture albums.  ese pages are interesting to a small audience, but probably not broadly. On the other hand, there are a few pages that are popular and useful to many people, including news sites and the websites of popular companies.
 e huge size of the Web makes this a difficult problem for search engines. Suppose a friend had told you to visit the site for eBay, and you didn’t know that www.ebay.com was the URL to use. You could type “eBay” into a search engine, but there are millions of web pages that contain the word “eBay”. How can the
4.5 LinkAnalysis 105
106 4 ProcessingText
search engine choose the most popular (and probably the correct) one? One very effective approach is to use the links between web pages as a way to measure pop- ularity.  e most obvious measure is to count the number of inlinks (links point- ing to a page) for each page and use this as a feature or piece of evidence in the ranking algorithm. Although this has been shown to be quite effective, it is very susceptible to spam. Measures based on link analysis algorithms are designed to provide more reliable ratings of web pages. Of these measures, PageRank, which is associated with the Google search engine, is most o en mentioned.
PageRank is based on the idea of a random surfer (as in web surfer). Imagine a person named Alice who is using her web browser. Alice is extremely bored, so she wanders aimlessly between web pages. Her browser has a special “surprise me” button at the top that will jump to a random web page when she clicks it. Each time a web page loads, she chooses whether to click the “surprise me” button or whether to click one of the links on the web page. If she clicks a link on the page, she has no preference for any particular link; instead, she just picks one randomly. Alice is sufficiently bored that she intends to keep browsing the Web like this for- ever.22
To put this in a more structured form, Alice browses the Web using this algo- rithm:
1. Choose a random number r between 0 and 1.
2. If r < λ:
• Click the “surprise me” button. 3. If r ≥ λ:
• Click a link at random on the current page. 4. Start again.
Typically we assume that λ is fairly small, so Alice is much more likely to click a link than to pick the “surprise me” button. Even though Alice’s path through the web pages is random, Alice will still see popular pages more o en than unpopular ones.  at’s because Alice o en follows links, and links tend to point to popular pages. So, we expect that Alice will end up at a university website, for example, more o en than a personal website, but less o en than the CNN website.
22  ePageRankcalculationcorrespondsto ndingwhatisknownasthestationaryprob- ability distribution of a random walk on the graph of the Web. A random walk is a spe- cial case of a Markov chain in which the next state (the next page visited) depends solely on the current state (current page).  e transitions that are allowed between states are all equally probable and are given by the links.
 
4.5 LinkAnalysis 107
Suppose that CNN has posted a story that contains a link to a professor’s web page. Alice now becomes much more likely to visit that professor’s page, because Alice visits the CNN website frequently. A single link at CNN might in uence Alice’s activity more than hundreds of links at less popular sites, because Alice visits CNN far more o en than those less popular sites.
Because of Alice’s special “surprise me” button, we can be guaranteed that eventually she will reach every page on the Internet.23 Since she plans to browse the Web for a very long time, and since the number of web pages is  nite, she will visit every page a very large number of times. It is likely, however, that she will visit a popular site thousands of times more o en than an unpopular one. Note that if she did not have the “surprise me” button, she would get stuck on pages that did not have links, pages whose links no longer pointed to any page, or pages that formed a loop. Links that point to the  rst two types of pages, or pages that have not yet been crawled, are called dangling links.
Now suppose that while Alice is browsing, you happened to walk into her room and glance at the web page on her screen. What is the probability that she will be looking at the CNN web page when you walk in?  at probability is CNN’s PageRank. Every web page on the Internet has a PageRank, and it is uniquely determined by the link structure of web pages. As this example shows, PageRank has the ability to distinguish between popular pages (those with many incoming links, or those that have links from popular pages) and unpopular ones.  e PageRank value can help search engines si  through the millions of pages that contain the word “eBay” to  nd the one that is most popular (www.ebay.com).
Alice would have to click on many billions of links in order for us to get a reasonable estimate of PageRank, so we can’t expect to compute it by using actual people. Fortunately, we can compute PageRank in a much more efficient way.
Suppose for the moment that the Web consists of just three pages, A, B, and C. We will suppose that page A links to pages B and C, page B links to page C, and page C links to page A, as shown in Figure 4.10.
 e PageRank of page C, which is the probability that Alice will be looking at this page, will depend on the PageRank of pages A and B. Since Alice chooses randomly between links on a given page, if she starts in page A, there is a 50% chance that she will go to page C (because there are two outgoing links). Another way of saying this is that the PageRank for a page is divided evenly between all the
23  e “surprise button” makes the random surfer model an ergodic Markov chain, which guarantees that the iterative calculation of PageRank will converge.
 
108 4 ProcessingText
Fig. 4.10. A sample “Internet” consisting of just three web pages.  e arrows denote links between the pages.
outgoing links. If we ignore the “surprise me” button, this means that the Page- Rank of page C, represented as P R(C), can be calculated as:
PR(C) = PR(A) + PR(B) 21
More generally, we could calculate the PageRank for any page u as:
∑ PR(v)
Lv
where Bu is the set of pages that point to u, and Lv is the number of outgoing links from page v (not counting duplicate links).
 ere is an obvious problem here: we don’t know the PageRank values for the pages, because that is what we are trying to calculate. If we start by assuming that the PageRank values for all pages are the same (1/3 in this case), then it is easy to see that we could perform multiple iterations of the calculation. For example, in the  rst iteration, P R(C) = 0.33/2 + 0.33 = 0.5, P R(A) = 0.33, and P R(B) = 0.17. In the next iteration, PR(C) = 0.33/2 + 0.17 = 0.33, PR(A) = 0.5, and P R(B) = 0.17. In the third iteration, P R(C) = 0.42, P R(A) = 0.33, and P R(B) = 0.25. A er a few more iterations, the PageRank values converge to the  nal values of PR(C) = 0.4, PR(A) = 0.4, and PR(B) = 0.2.
If we take the “surprise me” button into account, part of the PageRank for page C will be due to the probability of coming to that page by pushing the button. Given that there is a 1/3 chance of going to any page when the button is pushed,
     A
     PR(u) =
B
 C
 v∈Bu
4.5 LinkAnalysis 109 the contribution to the PageRank for C for the button will be λ/3.  is means
that the total PageRank for C is now:
PR(C) = λ + (1 − λ) · (PR(A) + PR(B))
Similarly, the general formula for PageRank is:
   321
PR(u)= λ +(1−λ)· ∑ PR(v) N Lv
  v∈Bu
where N is the number of pages being considered.  e typical value for λ is 0.15.
 is can also be expressed as a matrix equation:
R = TR
where R is the vector of PageRank values and T is the matrix representing the transition probabilities for the random surfer model.  e element Tij represents the probability of going from page i to page j, and:
Tij = λ + (1 − λ) 1 N Li
 ose of you familiar with linear algebra may recognize that the solution R is an eigenvector of the matrix T.
Figure 4.11 shows some pseudocode for computing PageRank.  e algorithm takes a graph G as input. Graphs are composed of vertices and edges, so G = (V, E). In this case, the vertices are web pages and the edges are links, so the pseu- docode uses the letters P and L instead. A link is represented as a pair (p, q), where p and q are the source and destination pages. Dangling links, which are links where the page q does not exist, are assumed to be removed. Pages with no outbound links are rank sinks, in that they accumulate PageRank but do not dis- tribute it. In this algorithm, we assume that these pages link to all other pages in the collection.
 e  rst step is to make a guess at the PageRank value for each page. Without any better information, we assume that the PageRank is the same for each page. Since PageRank values need to sum to 1 for all pages, we assign a PageRank of 1/|P | to each page in the input vector I . An alternative that may produce faster convergence would be to use a value based on the number of inlinks.
  
110
1: 2: 3: 4: 5: 6: 7: 8: 9:
10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28:
4 ProcessingText
procedurePR(G)
◃ G is the web graph, consisting of vertices (pages) and edges (links).
(P, L) ← G
I ← a vector of length |P | R ← a vector of length |P | for all entries Ii ∈ I do
Ii ←1/|P| end for
while R has not converged do for all entries Ri ∈ R do
◃ Split graph into pages and links ◃  e current PageRank estimate ◃  e resulting better PageRank estimate
◃Startwitheachpagebeingequallylikely
Ri ←λ/|P| ◃Eachpagehasaλ/|P|chanceofrandomselection end for
forallpagesp ∈ P do
Q ←thesetofpagessuchthat(p,q) ∈ Landq ∈ P if |Q| > 0 then
forallpagesq ∈ Qdo
Rq ← Rq + (1 − λ)Ip/|Q| ◃ Probability Ip of being at page p
end for else
forallpagesq ∈ P do
Rq ←Rq +(1−λ)Ip/|P|
end for end if
I ← R end for
end while
return R endprocedure
◃ Update our current PageRank estimate
Fig. 4.11. Pseudocode for the iterative PageRank algorithm
In each iteration, we start by creating a result vector, R, and storing λ/|P | in each entry.  is is the probability of landing at any particular page because of a random jump.  e next step is to compute the probability of landing on a page because of a clicked link. We do that by iterating over each web page in P. At each page, we retrieve the estimated probability of being at that page, Ip. From that page, the user has a λ chance of jumping randomly, or 1 − λ of clicking on a link.  ere are |Q| links to choose from, so the probability of jumping to a page q ∈ Q is (1 − λ)Ip/|Q|. We add this quantity to each entry Rq. In the event that
4.5 LinkAnalysis 111
there are no usable outgoing links, we assume that the user jumps randomly, and therefore the probability (1 − λ)Ip is spread evenly among all |P | pages.
To summarize, PageRank is an important example of query-independent meta- data that can improve ranking for web search. Web pages have the same PageRank value regardless of what query is being processed. Search engines that use Page- Rank will prefer pages with high PageRank values instead of assuming that all web pages are equally likely to satisfy a query. PageRank is not, however, as im- portant in web search as the conventional wisdom holds. It is just one of many features used in ranking. It does, however, tend to have the most impact on pop- ular queries, which is a useful property.
 e HITS24 algorithm (Kleinberg, 1999) for link analysis was developed at about the same time as PageRank and has also been very in uential.  is algo- rithm estimates the value of the content of a page (the authority value) and the value of the links to other pages (the hub value). Both values are computed us- ing an iterative algorithm based solely on the link structure, similar to PageRank.  e HITS algorithm, unlike PageRank, calculates authority and hub values for a subset of pages retrieved by a given query.25  is can be an advantage in terms of the impact of the HITS metadata on ranking, but may be computationally in- feasible for search engines with high query traffic. In Chapter 10, we discuss the application of the HITS algorithm to  nding web communities.
4.5.3 Link Quality
It is well known that techniques such as PageRank and anchor text extraction are used in commercial search engines, so unscrupulous web page designers may try to create useless links just to improve the search engine placement of one of their web pages.  is is called link spam. Even typical users, however, can unwittingly fool simple search engine techniques. A good example of this is with blogs.
Many blog posts are comments about other blog posts. Suppose author A reads a post called b in author B’s blog. Author A might write a new blog post, called a, which contains a link to post b. In the process of posting, author A may post a trackback to post b in author B’s blog. A trackback is a special kind of comment that alerts author B that a reply has been posted in author A’s blog.
24 Hypertext Induced Topic Search
25 Query-independentversionsofHITSandtopic-dependentversionsofPageRankhave
also been de ned.
 
112 4 ProcessingText
Blog A
Blog B
          Post a
Link
Trackback links
Post b
 Fig. 4.12. Trackback links in blog postings
As Figure 4.12 shows, a cycle has developed between post a and post b. Post a links to post b, and post b contains a trackback link to post a. Intuitively we would say that post b is in uential, because author A has decided to write about it. However, from the PageRank perspective, a and b have links to each other, and therefore neither is more in uential than the other.  e trouble here is that a trackback is a fundamentally different kind of link than one that appears in a post.
 e comments section of a blog can also be a source of link spam. Page authors may try to promote their own websites by posting links to them in the comments section of popular blogs. Based on our discussion of PageRank, we know that a link from a popular website can make another website seem much more impor- tant.  erefore, this comments section is an attractive target for spammers.
In this case, one solution is for search engine companies to automatically de- tect these comment sections and effectively ignore the links during indexing. An even easier way to do this is to ask website owners to alter the unimportant links so that search engines can detect them.  is is the purpose behind the rel=nofollow link attribute.
Most blog so ware is now designed to modify any link in a blog comment to contain the rel=nofollow attribute.  erefore, a post like this:
4.6 InformationExtraction 113
Come visit my <a href=”http://www.page.com”>web page</a>.
becomes something like this:
Come visit my <a rel=nofollow href=”http://www.page.com”>web page</a>.
 e link still appears on the blog, but search engines are designed to ignore all links marked rel=nofollow.  is helps preserve the integrity of PageRank calcula- tion and anchor text harvesting.
4.6 Information Extraction
Information extraction is a language technology that focuses on extracting struc- ture from text. Information extraction is used in a number of applications, and particularly for text data mining. For search applications, the primary use of in- formation extraction is to identify features that can be used by the search engine to improve ranking. Some people have speculated that information extraction tech- niques could eventually transform text search into a database problem by extract- ing all of the important information from text and storing it in structured form, but current applications of these techniques are a very long way from achieving that goal.
Some of the text processing steps we have already discussed could be consid- ered information extraction. Identifying noun phrases, titles, or even bolded text are examples. In each of these cases, a part of the text has been recognized as hav- ing some special property, and that property can be described using a markup lan- guage, such as XML. If a document is already described using HTML or XML, the recognition of some of the structural features (such as titles) is straightfor- ward, but others, such as phrases, require additional processing before the feature can be annotated using the markup language. In some applications, such as when the documents in the collection are input through OCR, the document has no markup and even simple structures such as titles must be recognized and anno- tated.
 ese types of features are very general, but most of the recent research in in- formation extraction has been concerned with features that have speci c seman- tic content, such as named entities, relationships, and events. Although all of these features contain important information, named entity recognition has been used most o en in search applications. A named entity is a word or sequence of words that is used to refer to something of interest in a particular application.  e most
114 4 ProcessingText
common examples are people’s names, company or organization names, locations, time and date expressions, quantities, and monetary values. It is easy to come up with other “entities” that would be important for speci c applications. For an e- commerce application, for example, the recognition of product names and model numbers in web pages and reviews would be essential. In a pharmaceutical appli- cation, the recognition of drug names, dosages, and medical conditions may be important. Given the more speci c nature of these features, the process of rec- ognizing them and tagging them in text is sometimes called semantic annotation. Some of these recognized entities would be incorporated directly into the search using, for example, facets (see Chapter 6), whereas others may be used as part of browsing the search results. An example of the latter is the search engine feature that recognizes addresses in pages and provides links to the appropriate map.
Fred Smith, who lives at 10 Water Street, Springfield, MA, is a long time  collector of tropical fish. 
<p ><PersonName><GivenName>Fred</GivenName> <Sn>Smith</Sn>  </PersonName>, who lives at <address><Street >10 Water Street</Street>,  <City>Springfield</City>, <State>MA</State></address>, is a long time  collector of <b>tropical fish.</b></p> 
Fig. 4.13. Text tagged by information extraction
Figure 4.13 shows a sentence and the corresponding XML markup a er us- ing information extraction. In this case, the extraction was done by a well-known word processing program.26 In addition to the usual structure markup (<p> and <b>), a number of tags have been added that indicate which words are part of named entities. It shows, for example, that an address consisting of a street (“10 Water Street”), a city (“Spring eld”), and a state (“MA”) was recognized in the text.
Two main approaches have been used to build named entity recognizers: rule- based and statistical. A rule-based recognizer uses one or more lexicons (lists of words and phrases) that categorize names. Some example categories would be locations (e.g., towns, cities, states, countries, places of interest), people’s names (given names, family names), and organizations (e.g., companies, government
26 Microso  Word
 
4.6 InformationExtraction 115
agencies, international groups). If these lists are sufficiently comprehensive, much of the extraction can be done simply by lookup. In many cases, however, rules or patterns are used to verify an entity name or to  nd new entities that are not in the lists. For example, a pattern such as “<number> <word> street” could be used to identify street addresses. Patterns such as “<street address>, <city>” or “in <city>” could be used to verify that the name found in the location lexicon as a city was indeed a city. Similarly, a pattern such as “<street address>, <city>, <state>” could also be used to identify new cities or towns that were not in the lexicon. New per- son names could be recognized by rules such as “<title> <name>”, where <title> would include words such as “President”, “Mr.”, and “CEO”. Names are generally easier to extract in mixed-case text, because capitalization o en indicates a name, but many patterns will apply to all lower- or uppercase text as well. Rules incor- porating patterns are developed manually, o en by trial and error, although an initial set of rules can also be used as seeds in an automated learning process that can discover new rules.27
A statistical entity recognizer uses a probabilistic model of the words in and around an entity. A number of different approaches have been used to build these models, but because of its importance, we will brie y describe the Hidden Markov Model (HMM) approach. HMMs are used for many applications in speech and language. For example, POS taggers can be implemented using this approach.
4.6.1 Hidden Markov Models for Extraction
One of the most difficult parts of entity extraction is that words can have many differentmeanings. eword“Bush”,forexample,candescribeaplantoraperson. Similarly, “Marathon” could be the name of a race or a location in Greece. People tell the difference between these different meanings based on the context of the word, meaning the words that surround it. For instance, if “Marathon” is preceded by “Boston”, the text is almost certainly describing a race. We can describe the context of a word mathematically by modeling the generation28 of the sequence of words in a text as a process with the Markov property, meaning that the next word in the sequence depends on only a small number of the previous words.
27 GATE (http://gate.ac.uk) is an example of an open source toolkit that provides both an information extraction component and an environment for customizing extraction for a speci c application.
28 We discuss generative models in more detail in Chapter 7.
 
116 4 ProcessingText
More formally, a Markov Model describes a process as a collection of states with transitions between them. Each of the transitions has an associated probability.  e next state in the process depends solely on the current state and the transition probabilities. In a Hidden Markov Model, each state has a set of possible outputs that can be generated. As with the transitions, each output also has a probability associated with it.
Figure 4.14 shows a state diagram representing a very simple model for sen- tence generation that could be used by a named entity recognizer. In this model, the words in a sentence are assumed to be either part of an entity name (in this case, either a person, organization, or location) or not part of one. Each of these entity categories is represented by a state, and a er every word the system may stay in that state (represented by the arrow loops) or transition to another state.  ere are two special states representing the start and end of the sentence. Associated with each state representing an entity category, there is a probability distribution of the likely sequences of words for that category.
 start
           not-an- <every entity location entity category>
organ- ization
person
         Fig. 4.14. Sentence model for statistical entity extractor
One possible use of this model is to construct new sentences. Suppose that we begin in the start state, and then the next state is randomly chosen according
end
4.6 InformationExtraction 117
to the start state’s transition probability table. For example, we may transition to the person state. Once we have entered the person state, we complete the tran- sition by choosing an output according to the person state’s output probability distribution. An example output may be the word “ omas”.  is process would continue, with a new state being transitioned to and an output being generated during each step of the process.  e  nal result is a set of states and their associated outputs.
Although such models can be used to generate new sentences, they are more commonlyusedtorecognizeentitiesinasentence.Todothisforagivensentence, a sequence of entity categories is found that gives the highest probability for the words in that sentence. Only the outputs generated by state transitions are visible (i.e., can be observed); the underlying states are “hidden.” For the sentence in Fig- ure 4.13, for example, the recognizer would  nd the sequence of states
<start><name><not-an-entity><location><not-an-entity><end>
to have the highest probability for that model.  e words that were associated with the entity categories in this sequence would then be tagged.  e problem of  nding the most likely sequence of states in an HMM is solved by the Viterbi algorithm,29 which is a dynamic programming algorithm.
 e key aspect of this approach to entity recognition is that the probabili- ties in the sentence model must be estimated from training data. To estimate the transition and output probabilities, we generate training data that consists of text manually annotated with the correct entity tags. From this training data, we can directly estimate the probability of words associated with a given category (i.e., output probabilities), and the probability of transitions between categories. To build a more accurate recognizer, features that are highly associated with named entities, such as capitalized words and words that are all digits, would be included in the model. In addition, the transition probabilities could depend on the pre- vious word as well as the previous category.30 For example, the occurrence of the word “Mr.” increases the probability that the next category is Person.
Although such training data can be useful for constructing accurate HMMs, collecting it requires a great deal of human effort. To generate approximately one million words of annotated text, which is the approximate size of training data required for accurate estimates, people would have to annotate the equivalent of
29 Named a er the electrical engineer Andrew Viterbi.
30 Bikel et al. (1997) describe one of the  rst named entity recognizers based on the
HMM approach.
 
118 4 ProcessingText
more than 1,500 news stories.  is may require considerably more effort than de- veloping rules for a simple set of features. Both the rule-based and statistical ap- proaches have recognition effectiveness of about 90%31 for entities such as name, organization, and location, although the statistical recognizers are generally the best. Other entity categories, such as product names, are considerably more diffi- cult.  e choice of which entity recognition approach to use will depend on the application, the availability of so ware, and the availability of annotators.
Interestingly, there is little evidence that named entities are useful features for general search applications. Named entity recognition is a critical part of question-answering systems (section 11.5), and can be important in domain- speci c or vertical search engines for accurately recognizing and indexing domain terms. Named entity recognition can also be useful for query analysis in applica- tions such as local search, and as a tool for understanding and browsing search results.
4.7 Internationalization
 e Web is used all over the world, and not just by English speakers. Although 65–70% of the Web is written in English, that percentage is continuing to de- crease. More than half of the people who use the Web, and therefore search the Web, do not use English as their primary language. Other search applications, such as desktop search and corporate search, are being used in many different languages every day. Even an application designed for an environment that has mostly English-speaking users can have many non-English documents in the col- lection. Try using “poissons tropicaux” (tropical  sh) as a query for your favorite web search engine and see how many French web pages are retrieved.32
A monolingual search engine is, as the name suggests, a search engine designed for a particular language.33 Many of the indexing techniques and retrieval models we discuss in this book work well for any language.  e differences between lan- guages that have the most impact on search engine design are related to the text processing steps that produce the index terms for searching.
31 By this we mean that about 9 out of 10 of the entities found are accurately identi ed, and 9 out of 10 of the existing entities are found. See Chapter 8 for details on evaluation measures.
32 You would  nd many more French web pages, of course, if you used a French version of the search engine, such as http://fr.yahoo.com.
33 We discuss cross-language search engines in section 6.4.
 
4.7 Internationalization 119
As we mentioned in the previous chapter, character encoding is a crucial issue for search engines dealing with non-English languages, and Unicode has become the predominant character encoding standard for the internationalization of so - ware.
Other text processing steps also need to be customized for different languages.  e importance of stemming for highly in ected languages has already been men- tioned, but each language requires a customized stemmer. Tokenizing is also im- portant for many languages, especially for the CJK family of languages. For these languages, the key problem is word segmentation, where the breaks correspond- ing to words or index terms must be identi ed in the continuous sequence of characters (spaces are generally not used). One alternative to segmenting is to in- dex overlapping character bigrams (pairs of characters, see section 4.3.5). Figure 4.15 shows word segmentation and bigrams for the text “impact of droughts in China”. Although the ranking effectiveness of search based on bigrams is quite good, word segmentation is preferred in many applications because many of the bigrams do not correspond to actual words. A segmentation technique can be im- plemented based on statistical approaches, such as a Hidden Markov Model, with sufficient training data. Segmentation can also be an issue in other languages. Ger- man, for example, has many compound words (such as “ schzuchttechniken” for “ sh farming techniques”) that should be segmented for indexing.
1. Original text
          
(the impact of droughts in China) 2. Word segmentation
           
drought   at     china    make           impact 
3. Bigrams
                      
Fig. 4.15. Chinese segmentation and bigrams
In general, given the tools that are available, it is not difficult to build a search engine for the major languages.  e same statement holds for any language that
120 4 ProcessingText
has a signi cant amount of online text available on the Web, since this can be used as a resource to build and test the search engine components.  ere are, however, a large number of other so-called “low-density” languages that may have many speakers but few online resources. Building effective search engines for these lan- guages is more of a challenge.
References and Further Reading
 e properties and statistics of text and document collections has been studied for some time under the heading of bibliometrics, which is part of the  eld of library and information science. Information science journals such as the Journal of the American Society of Information Science and Technology (JASIST) or Information Processing and Management (IPM) contain many papers in this general area. In- formation retrieval has, from the beginning, emphasized a statistical view of text, and researchers from IR and information science have always worked closely to- gether. Belew (2000) contains a good discussion of the cognitive aspects of Zipf ’s law and other properties of text in relationship to IR. With the shi  to statistical methods in the 1990s, natural language processing researchers also became inter- ested in studying the statistical properties of text. Manning and Schütze (1999) is a good summary of text statistics from this perspective. Ha et al. (2002) give an interesting result showing that phrases (or n-grams) also generally follow Zipf ’s law, and that combining the phrases and words results in better predictions for frequencies at low ranks.
 e paper by Anagnostopoulos et al. (2005) describes a technique for estimat- ing query result size and also points to much of the relevant literature in this area. Similarly, Broder et al. (2006) show how to estimate corpus size and compare their estimation with previous techniques.
Not much is written about tokenizing or stopping. Both are considered suf-  ciently “well known” that they are hardly mentioned in papers. As we have pointed out, however, getting these basic steps right is crucial for the overall sys- tem’s effectiveness. For many years, researchers used the stopword list published in van Rijsbergen (1979). When it became clear that this was not sufficient for the larger TREC collections, a stopword list developed at the University of Mas- sachusetts and distributed with the Lemur toolkit has frequently been used. As mentioned previously, this list contains over 400 words, which will be too long for many applications.
4.7 Internationalization 121
 e original paper describing the Porter stemmer was written in 1979, but was reprinted in Porter (1997).  e paper by Krovetz (1993) describes his stemming algorithm but also takes a more detailed approach to studying the role of mor- phology in a stemmer.34  e Krovetz stemmer is available on the Lemur website. Stemmers for other languages are available from various websites (including the Lemur website and the Porter stemmer website). A description of Arabic stem- ming techniques can be found in Larkey et al. (2002).
Research on the use of phrases in searching has a long history. Cro  et al. (1991) describe retrieval experiments with phrases derived by both syntactic and statistical processing of the query, and showed that effectiveness was similar to phrases selected manually. Many groups that have participated in the TREC eval- uations have used phrases as part of their search algorithms (Voorhees & Harman, 2005).
Church (1988) described an approach to building a statistical (or stochastic) part-of-speech tagger that is the basis for many current taggers.  is approach uses manually tagged training data to train a probabilistic model of sequences of parts of speech, as well as the probability of a part of speech for a speci c word. For a given sentence, the part-of-speech tagging that gives the highest probability for the whole sentence is used.  is method is essentially the same as that used by a statistical entity extractor, with the states being parts of speech instead of en- tity categories.  e Brill tagger (Brill, 1994) is a popular alternative approach that uses rules that are learned automatically from tagged data. Manning and Schütze (1999) provide a good overview of part-of-speech tagging methods.
Many variations of PageRank can be found in the literature. Many of these variations are designed to be more efficient to compute or are used in different ap- plications.  e topic-dependent version of PageRank is described in Haveliwala (2002). Both PageRank and HITS have their roots in the citation analysis algo- rithms developed in the  eld of bibliometrics.
 e idea of enhancing the representation of a hypertext document (i.e., a web page) using the content of the documents that point to it has been around for some time. For example, Cro  and Turtle (1989) describe a retrieval model based on incorporating text from related hypertext documents, and Dunlop and van Rijsbergen (1993) describe how documents with little text content (such as those containing images) could be retrieved using the text in linked documents. Re-
34 Morphology is the study of the internal structure of words, and stemming is a form of morphological processing.
 
122 4 ProcessingText
stricting the text that is incorporated to the anchor text associated with inlinks was  rst mentioned by McBryan (1994). Anchor text has been shown to be es- sential for some categories of web search in TREC evaluations, such as in Ogilvie and Callan (2003).
Techniqueshavebeendevelopedforapplyinglinkanalysisincollectionswith- out explicit link structure (Kurland & Lee, 2005). In this case, the links are based on similarities between the content of the documents, calculated by a similarity measure such as the cosine correlation (see Chapter 7).
Information extraction techniques were developed primarily in research pro- grams such as TIPSTER and MUC (Cowie & Lehnert, 1996). Using named en- tity extraction to provide additional features for search was also studied early in the TREC evaluations (Callan et al., 1992, 1995). One of the best-known rule- based information extraction systems is FASTUS (Hobbs et al., 1997).  e BBN system Identi nder (Bikel et al., 1999), which is based on an HMM, has been used in many projects.
A detailed description of HMMs and the Viterbi algorithm can be found in Manning and Schütze (1999). McCallum (2005) provides an overview of infor- mation extraction, with references to more recent advances in the  eld. Statistical models that incorporate more complex features than HMMs, such as conditional random  elds, have become increasingly popular for extraction (Sutton & McCal- lum, 2007).
Detailed descriptions of all the major encoding schemes can be found in Wikipedia. Fujii and Cro  (1993) was one of the early papers that discussed the problems of text processing for search with CJK languages. An entire journal, ACM Transactions on Asian Language Information Processing,35 has now been de- voted to this issue. Peng et al. (2004) describe a statistical model for Chinese word segmentation and give references to other approaches.
Exercises
4.1. Plot rank-frequency curves (using a log-log graph) for words and bigrams in the Wikipedia collection available through the book website (http://www.search- engines-book.com). Plot a curve for the combination of the two. What are the best values for the parameter c for each curve?
35 http://talip.acm.org/
 
4.7 Internationalization 123
4.2. Plot vocabulary growth for the Wikipedia collection and estimate the pa- rameters for Heaps’ law. Should the order in which the documents are processed make any difference?
4.3. Try to estimate the number of web pages indexed by two different search en- gines using the technique described in this chapter. Compare the size estimates from a range of queries and discuss the consistency (or lack of it) of these esti- mates.
4.4. Modify the Galago tokenizer to handle apostrophes or periods in a different way. Describe the new rules your tokenizer implements. Give examples of where the new tokenizer does a better job (in your opinion) and examples where it does not.
4.5. Examine the Lemur stopword list and list 10 words that you think would cause problems for some queries. Give examples of these problems.
4.6. Process  ve Wikipedia documents using the Porter stemmer and the Krovetz stemmer. Compare the number of stems produced and  nd 10 examples of dif- ferences in the stemming that could have an impact on ranking.
4.7. Use the GATE POS tagger to tag a Wikipedia document. De ne a rule or rules to identify phrases and show the top 10 most frequent phrases. Now use the POS tagger on the Wikipedia queries. Are there any problems with the phrases identi ed?
4.8. Find the 10 Wikipedia documents with the most inlinks. Show the collec- tion of anchor text for those pages.
4.9. Compute PageRank for the Wikipedia documents. List the 20 documents with the highest PageRank values together with the values.
4.10. Figure 4.11 shows an algorithm for computing PageRank. Prove that the entries of the vector I sum to 1 every time the algorithm enters the loop on line 9.
4.11. Implement a rule-based recognizer for cities (you can choose a subset of cities to make this easier). Create a test collection that you can scan manually to  nd cities mentioned in the text and evaluate your recognizer. Summarize the performance of the recognizer and discuss examples of failures.
124 4 ProcessingText
4.12. Create a small test collection in some non-English language using web pages. Do the basic text processing steps of tokenizing, stemming, and stopping using tools from the book website and from other websites. Show examples of the index term representation of the documents.
5
Ranking with Indexes
“Must go faster.”
David Levinson, Independence Day
5.1 Overview
As this is a fairly technical book, if you have read this far, you probably understand something about data structures and how they are used in programs. If you want to store a list of items, linked lists and arrays are good choices. If you want to quickly  nd an item based on an attribute, a hash table is a better choice. More complicated tasks require more complicated structures, such as B-trees or priority queues.
Why are all these data structures necessary? Strictly speaking, they aren’t. Most things you want to do with a computer can be done with arrays alone. However, arrays have drawbacks: unsorted arrays are slow to search, and sorted arrays are slow at insertion. By contrast, hash tables and trees are fast for both search and insertion.  ese structures are more complicated than arrays, but the speed dif- ference is compelling.
Text search is very different from traditional computing tasks, so it calls for its own kind of data structure, the inverted index.  e name “inverted index” is really an umbrella term for many different kinds of structures that share the same general philosophy. As you will see shortly, the speci c kind of data structure used depends on the ranking function. However, since the ranking functions that rank documents well have a similar form, the most useful kinds of inverted indexes are found in nearly every search engine.
 is chapter is about how search engine queries are actually processed by a computer, so this whole chapter could arguably be called query processing.  e last section of this chapter is called that, and the query processing algorithms pre- sented there are based on the data structures presented earlier in the chapter.
 
126 5 RankingwithIndexes
Efficient query processing is a particularly important problem in web search, as it has reached a scale that would have been hard to imagine just 10 years ago. People all over the world type in over half a billion queries every day, searching indexes containing billions of web pages. Inverted indexes are at the core of all modern web search engines.
 ere are strong dependencies between the separate components of a search engine.  e query processing algorithm depends on the retrieval model, and dic- tates the contents of the index.  is works in reverse, too, since we are unlikely to choose a retrieval model that has no efficient query processing algorithm. Since we will not be discussing retrieval models in detail until Chapter 7, we start this chapter by describing an abstract model of ranking that motivates our choice of indexes. A er that, there are four main parts to the chapter. In the  rst part, we discuss the different types of inverted index and what information about docu- ments is captured in each index.  e second part gives an overview of compres- sion techniques, which are a critical part of the efficient implementation of in- verted indexes for text retrieval.  e third part of the chapter describes how in- dexes are constructed, including a discussion of the MapReduce framework that can be used for very large document collections.  e  nal part of the chapter fo- cuses on how the indexes are used to generate document rankings in response to queries.
5.2 Abstract Model of Ranking
Before we begin to look at how to build indexes for a search system, we will start by considering an abstract model of ranking. All of the techniques we will consider in this chapter can be seen as implementations of this model.
Figure 5.1 shows the basic components of our model. On the le  side of the  gure is a sample document. Documents are written in natural human languages, which are difficult for computers to analyze directly. So, as we saw in Chapter 4, the text is transformed into index terms or document features. For the purposes of this chapter, a document feature is some attribute of the document we can express numerically. In the  gure, we show two kinds of features. On top, we have topical features, which estimate the degree to which the document is about a particular subject. On the bottom of the  gure, we see two possible document quality fea- tures. One feature is the number of web pages that link to this document, and an- other is the number of days since this page was last updated.  ese features don’t address whether the document is a good topical match for a query, but they do
 Fred's Tropical Fish Shop is the best place to  nd tropical  sh at low, low prices. Whether you're looking for a little  sh or a big  sh, we've got what you need. We even have fake seaweed for your  shtank (and little sur oards too).
9.7  sh
4.2 tropical 22.1 tropical  sh
8.2 seaweed 4.2 sur oards
Topical Features
14 incoming links
3 days since last update
Quality Features
5.2 AbstractModelofRanking 127
tropical  sh
Query
Ranking Function 24.5 Document Score
       Document
Fig. 5.1.  e components of the abstract model of ranking : documents, features, queries, the retrieval function, and document scores
address its quality: a page with no incoming links that hasn’t been edited in years is probably a poor match for any query. Each of these feature values is generated using a feature function, which is just a mathematical expression that generates numbers from document text. In Chapter 4 we discussed some of the important topical and quality features, and in Chapter 7 you will learn about the techniques for creating good feature functions. In this chapter, we assume that reasonable feature values have already been created.
On the right side of the  gure, we see a cloud representing the ranking func- tion.  e ranking function takes data from document features combined with the query and produces a score. For now, the contents of that cloud are unimportant, except for the fact that most reasonable ranking functions ignore many of the doc- ument features, and focus only on the small subset that relate to the query.  is fact makes the inverted index an appealing data structure for search.
 e  nal output of the ranking function is a score, which we assume is some real number. If a document gets a high score, this means that the system thinks that document is a good match for the query, whereas lower numbers mean that the system thinks the document is a poor match for the query. To build a ranked list of results, the documents are sorted by score so that the highest-scoring doc- uments come  rst.
Suppose that you are a human search engine, trying to sort documents in an appropriate order in response to a user query. Perhaps you would place the docu- ments in piles, like “good,” “not so good,” and “bad.”  e computer is doing essen- tially the same thing with scoring. However, you might also break ties by looking carefully at each document to decide which one is more relevant. Unfortunately,
128 5 RankingwithIndexes
 nding deep meaning in documents is difficult for computers to do, so search en- gines focus on identifying good features and scoring based on those features.
A more concrete ranking model
Later in the chapter we will look at query evaluation techniques that assume some- thing stronger about what happens in the ranking function. Speci cally, we as- sume that the ranking function R takes the following form:
∑
i
Here, fi is some feature function that extracts a number from the document text. gi is a similar feature function that extracts a value from the query.  ese two functions form a pair of feature functions. Each pair of functions is multiplied together, and the results from all pairs are added to create a  nal document score.
R(Q, D) =
gi(Q)fi(D)
 sh 5.2 tropical 3.4 tropical sh 9.9 chichlids 1.2 barbs 0.7
Topical Features
incoming links 1.2 update count 0.9
Quality Features
Document Score
Fig. 5.2. A more concrete model of ranking. Notice how both the query and the docu- ment have feature functions in this model.
Figure 5.2 shows an example of this model. Just as in the abstract model of ranking, various features are extracted from the document.  is picture shows only a few features, but in reality there will be many more.  ese correspond to the fi(D) functions in the equation just shown. We could easily name these ftropical(D) or ffish(D); these values will be larger for documents that contain the words “tropical” or “ sh” more o en or more prominently.
 Fred's Tropical Fish Shop is the best place to  nd tropical  sh at low, low prices. Whether you're looking for a little  sh or a big  sh, we've got what you need. We even have fake seaweed for your  shtank (and little sur oards too).
4.2 tropical fi22.1tropical sh
8.2 seaweed 4.2 sur oards
Topical Features
14 incoming links 3 update count
Quality Features
9.7  sh
gi
tropical  sh
Query
          Document
303.01
5.3 InvertedIndexes 129
 e document has some features that are not topical. For this example docu- ment, we see that the search engine notices that this document has been updated three times, and that it has 14 incoming links. Although these features don’t tell us anything about whether this document would match the subject of a query, they do give us some hints about the quality of the document. We know that it wasn’t just posted to the Web and then abandoned, since it gets updated occa- sionally. We also know that there are 14 other pages that have links pointing to it, which might mean that it has some useful information on it.
Notice that there are also feature functions that act on the query.  e feature function gtropical(Q) evaluates to a large value because “tropical” is in the query. However, gbarbs(Q) also has a small non-zero value because it is related to other terms in the query.  ese values from the query feature functions are multiplied by the document feature functions, then summed to create a document score.
 e query also has some feature values that aren’t topical, such as the update count feature. Of course, this doesn’t mean that the query has been updated.  e value of this feature indicates how important document updates are to relevance for this query. For instance, if the query was “today’s weather in london”, we would prefer documents that are updated frequently, since a document that isn’t updated at least daily is unlikely to say anything interesting about today’s weather.  is query should have a high value for the update count feature. By contrast, a docu- ment that never changed could be very relevant for the query “full text of moby dick”.  is query could have a low feature value for update count.
If a retrieval system had to perform a sum over millions of features for every document, text search systems would not be practical. In practice, the query fea- tures (gi(Q)) are mostly zero.  is means that the sum for each document is only over the non-zero gi(Q) values.
5.3 Inverted Indexes
All modern search engine indexes are based on inverted indexes. Other index structures have been used in the past, most notably signature  les,1 but inverted indexes are considered the most efficient and  exible index structure.
1 A signature is a concise representation of a block of text (or document) as a sequence of bits, similar to the  ngerprints discussed in Chapter 3. A hash function is used for each word in the text block to set bits in speci c positions in the signature to one.
 
130 5 RankingwithIndexes
An inverted index is the computational equivalent of the index found in the back of this textbook. You might want to look over the index in this book as an example.  e book index is arranged in alphabetical order by index term. Each index term is followed by a list of pages about that word. If you want to know more about stemming, for example, you would look through the index until you found words starting with “s”.  en, you would scan the entries until you came to the word “stemming.”  e list of page numbers there would lead you to Chapter 4.
Similarly, an inverted index is organized by index term.  e index is inverted because usually we think of words being a part of documents, but if we invert this idea, documents are associated with words. Index terms are o en alphabetized like a traditional book index, but they need not be, since they are o en found di- rectly using a hash table. Each index term has its own inverted list that holds the relevant data for that term. In an index for a book, the relevant data is a list of page numbers. In a search engine, the data might be a list of documents or a list of word occurrences. Each list entry is called a posting, and the part of the posting that refers to a speci c document or location is o en called a pointer. Each doc- ument in the collection is given a unique number to make it efficient for storing document pointers.
Indexes in books store more than just location information. For important words, o en one of the page numbers is marked in boldface, indicating that this page contains a de nition or extended discussion about the term. Inverted  les can also have extended information, where postings can contain a range of in- formation other than just locations. By storing the right information along with each posting, the feature functions we saw in the last section can be computed efficiently.
Finally, by convention, the page numbers in a book index are printed in as- cending order, so that the smallest page numbers come  rst. Traditionally, in- verted lists are stored the same way.  ese document-ordered lists are ordered by document number, which makes certain kinds of query processing more efficient and also improves list compression. However, some inverted  les we will consider have other kinds of orderings.
Alternatives to inverted  les generally have one or more disadvantages.  e sig- nature  le, for example, represents each document in the collection as a small set of bits. To search a signature  le, the query is converted to a signature and the bit patterns are compared. In general, all signatures must be scanned for every search. Even if the index is encoded compactly, this is a lot of processing.  e inverted  le’s advantage is that only a small fraction of the index needs to be considered
5.3 InvertedIndexes 131
to process most queries. Also, matches in signature  les are noisy, so a signature match is not guaranteed to be a match in the document text. Most importantly, it is difficult to generalize signature  le techniques for ranked search (Zobel et al., 1998).
Another approach is to use spatial data structures, such as k-d trees. In this ap- proach, each document is encoded as a point in some high-dimensional space, and the query is as well.  e spatial data structure can then be used to  nd the closest documents to the query. Although many ranking approaches are fundamentally spatial, most spatial data structures are not designed for the number of dimen- sions associated with text applications.2 As a result, it tends to be much faster to use an inverted list to rank documents than to use a typical spatial data structure.
In the next few sections, we will look at some different kinds of inverted  les. In each case, the inverted  le organization is dictated by the ranking function. More complex ranking functions require more information in the index.  ese more complicated indexes take additional space and computational power to process, but can be used to generate more effective document rankings. Index organization is by no means a solved problem, and research is ongoing into the best way to create indexes that can more efficiently produce effective document rankings.
5.3.1 Documents
 e simplest form of an inverted list stores just the documents that contain each word, and no additional information.  is kind of list is similar to the kind of index you would  nd at the back of this textbook.
Figure5.3showsanindexofthistypebuiltfromthefoursentencesinTable5.1 (so in this case, the “documents” are sentences).  e index contains every word found in all four sentences. Next to each word, there are a list of boxes, and each one contains the number of a sentence. Each one of these boxes is a posting. For example, look at the word “ sh”. You can quickly see that this word appears in all four sentences, because the numbers 1, 2, 3, and 4 appear by it. You can also quickly determine that “ sh” is the only word that appears in all the sentences. Two words come close: “tropical” appears in every sentence but S4, and “water” is not in S3.
2 Every term in a document corresponds to a dimension, so there are tens of thousands of dimensions in effect.  is is in comparison to a typical database application with tens of dimensions at most.
 
132 5 RankingwithIndexes
S1 Tropical fish include fish found in tropical environments around the world, including both freshwater and salt water species.
S2 Fishkeepers often use the term tropical fish to refer only those requiring fresh water, with saltwater tropical fish referred to as marine fish.
S3 Tropical fish are popular aquarium fish, due to their often bright coloration.
S4 In freshwater fish, this coloration typically derives from irides- cence, while salt water fish are generally pigmented.
Table 5.1. Four sentences from the Wikipedia entry for tropical  sh
  and
aquarium are
around as
both bright coloration derives due environments fish fishkeepers found fresh freshwater from generally in include including iridescence marine often
only pigmented popular refer referred requiring salt saltwater species term the their this those to tropical typically use water while with world
1
2
  3
4
   3
4
3
  1
2
  2
2
  1
2
   3
4
1
4
   3
2
  4
 2
 2
 3
 4
 3
   4
2
3
 2
 4
 4
Fig. 5.3. An inverted index for the documents (sentences) in Table 5.1
1
  3
2
  1
1
  1
3
  2
4
  1
2
  2
2
  1
1
  4
4
  4
2
  1
1
  1
4
  1
2
  4
1
 2
  2
3
5.3 InvertedIndexes 133
Notice that this index does not record the number of times each word appears; it only records the documents in which each word appears. For instance, S2 con- tains the word “ sh” twice, whereas S1 contains “ sh” only once.  e inverted list for “ sh” shows no distinction between sentences 1 and 2; both are listed in the same way. In the next few sections, we will look at indexes that include informa- tion about word frequencies.
Inverted lists become more interesting when we consider their intersection. Suppose we want to  nd the sentence that contains the words “coloration” and “freshwater”.  e inverted index tells us that “coloration” appears in S3 and S4, while “freshwater” appears in S1 and S4. We can quickly tell that only S4 contains both “coloration” and “freshwater”. Since each list is sorted by sentence number,  nding the intersection of these lists takes O(max(m, n)) time, where m and n are the lengths of the two lists.  e algorithm is the same as in merge sort. With list skipping, which we will see later in the chapter, this cost drops to O(min(m, n)).
5.3.2 Counts
Remember that our abstract model of ranking considers each document to be composed of features. With an inverted index, each word in the index corre- sponds to a document feature.  is feature data can be processed by a ranking function into a document score. In an inverted index that contains only docu- ment information, the features are binary, meaning they are 1 if the document contains a term, 0 otherwise.  is is important information, but it is too coarse to  nd the best few documents when there are a lot of possible matches.
For instance, consider the query “tropical  sh”.  ree sentences match this query: S1, S2, and S3.  e data in the document-based index (Figure 5.3) gives us no reason to prefer any of these sentences over any other.
Now look at the index in Figure 5.4.  is index looks similar to the previous one. We still have the same words and the same number of postings, and the  rst number in each posting is the same as in the previous index. However, each post- ing now has a second number.  is second number is the number of times the word appears in the document.  is small amount of additional data allows us to prefer S2 over S1 and S3 for the query “tropical  sh”, since S2 contains “tropical” twice and “ sh” three times.
In this example, it may not be obvious that S2 is much better than S1 or S3, but in general, word counts can be a powerful predictor of document relevance. In particular, word counts can help distinguish documents that are about a particular
134
5 RankingwithIndexes
and
aquarium are around
as
both bright coloration derives due environments fish fishkeepers found fresh freshwater from generally in include including iridescence marine often
  1:1
only pigmented popular refer referred requiring salt saltwater species term the their this those to tropical typically use water while with world
2:1
  3:1
4:1
 4:1
  3:1
3:1
  1:1
2:1
  2:1
2:1
  1:1
2:1
 4:1
  3:1
1:1
 4:1
  3:1
2:1
  4:1
1:1
  3:1
2:1
 2:1
  1:1
1:1
 2:3
 3:2
 4:2
  1:2
3:1
  2:1
4:1
  1:1
2:1
   2:1
2:2
 4:1
    1:1
1:2
2:2
3:1
  4:1
4:1
  4:1
2:1
 4:1
 2:1
 4:1
  1:1
Fig. 5.4. An inverted index, with word counts, for the documents in Table 5.1
subject from those that discuss that subject in passing. Imagine two documents, one about tropical  sh and another about tropical islands.  e document about tropical islands would probably contain the word “ sh”, but only a few times. On the other hand, the document about tropical  sh would contain the word “ sh” many times. Using word occurrence counts helps us rank the most relevant doc- ument highest in this example.
5.3.3 Positions
When looking for matches for a query like “tropical  sh”, the location of the words in the document is an important predictor of relevance. Imagine a doc- ument about food that included a section on tropical fruits followed by a section on saltwater  sh. So far, none of the indexes we have considered contain enough information to tell us that this document is not relevant. Although a document
1:1
3:1
  1:1
4:1
  1:1
4:1
2:1
  1:1
 2:1
  2:1
3:1
and
aquarium are around
as
fish
 both bright coloration derives due
4,5
5.3 InvertedIndexes 135
 1,15
 3,5
 3,3
 1,9
 2,21
 1,13
 3,11
 3,12
 4,7
 3,7
1,8
 1,2
 2,22
 2,2
 2,10
 4,16
 3,4
 2,9
 2,19
 2,12
 1,16
 2,16
 1,18
 2,5
 1,10
 3,9
 4,4
 2,11
 2,8
 1,1
 4,6
 2,3
 1,17
 4,10
 2,15
 1,11
marine often only pigmented popular refer referred requiring salt saltwater environments   species term the their this those to tropical typically use water while with world
3,10
 4,14
   2,7
 3,2
 4,13
 2,18
 3,6
 2,23
 4,3
1,4
4,11
  2,4
 2,1
 1,5
 2,13
 1,14
 4,8
 4,15
 1,6
 1,3
 1,12
 4,9
fishkeepers found fresh freshwater from generally in include including iridescence
 2,20
 1,7
 3,8
 2,6
   4,2
Fig. 5.5. An inverted index, with word positions, for the documents in Table 5.1
that contains the words “tropical” and “ sh” is likely to be relevant, we really want to know if the document contains the exact phrase “tropical  sh”.
To determine this, we can add position information to our index, as in Fig- ure 5.5.  is index shares some structural characteristics with the previous in- dexes, in that it has the same index terms and each list contains some postings.  ese postings, however, are different. Each posting contains two numbers: a doc- ument number  rst, followed by a word position. In the previous indexes, there was just one posting per document. Now there is one posting per word occur- rence.
Look at the long list for the word “ sh”. In the other indexes, this list contained just four postings. Now the list contains nine postings.  e  rst two postings tell us that the word “ sh” is the second word and fourth word in S1.  e next three postings tell us that “ sh” is the seventh, eighteenth, and twenty-third word in S2.
2,17
3,1
   4,1
2,14
4,12
136 5 RankingwithIndexes
tropical
fish
Fig. 5.6. Aligning posting lists for “tropical” and “ sh” to  nd the phrase “tropical  sh”
 is information is most interesting when we look at intersections with other posting lists. Using an intersection with the list for “tropical”, we  nd where the phrase “tropical  sh” occurs. In Figure 5.6, the two inverted lists are lined up next to each other. We see that the word “tropical” is the  rst word in S1, and “ sh” is the second word in S1, which means that S1 must start with the phrase “tropical  sh”.  e word “tropical” appears again as the seventh word in S1, but “ sh” does not appear as the eighth word, so this is not a phrase match. In all, there are four occurrences of the phrase “tropical  sh” in the four sentences.  e phrase matches are easy to see in the  gure; they happen at the points where the postings are lined up in columns.
 is same technique can be extended to  nd longer phrases or more general proximity expressions, such as “ nd tropical within 5 words of fish.” Suppose that the word “tropical” appears at position p. We can then look in the inverted list for “ sh” for any occurrences between position p − 5 and p + 5. Any of those occurrences would constitute a match.
5.3.4 Fields and Extents
Real documents are not just lists of words.  ey have sentences and paragraphs that separate concepts into logical units. Some documents have titles and head- ings that provide short summaries of the rest of the content. Special types of doc- uments have their own sections; for example, every email contains sender infor- mation and a subject line. All of these are instances of what we will call document  elds, which are sections of documents that carry some kind of semantic meaning.
It makes sense to include information about  elds in the index. For example, suppose you have a professor named Dr. Brown. Dr. Brown sent you an email about when course projects are due, but you can’t  nd it. You can type “brown” into your email program’s search box, but the result you want will be mixed in with other uses of the word “brown”, such as Brown University or brown socks. A search for “brown” in the From: line of the email will focus your search on exactly what you want.
 1,1
 1,2
  2,6
 2,7
 2,17
 2,18
 3,1
 3,2
1,7
     1,4
2,23
3,6
4,3
4,13
5.3 InvertedIndexes 137
Field information is useful even when it is not used explicitly in the query. Ti- tles and headings tend to be good summaries of the rest of a document.  erefore, if a user searches for “tropical  sh”, it makes sense to prefer documents with the title “Tropical Fish,” even if a document entitled “Mauritius” mentions the words “tropical” and “ sh” more o en.  is kind of preference for certain document  elds can be integrated into the ranking function.
In order to handle these kinds of searches, the search engine needs to be able to determine whether a word is in a particular  eld. One option is to make separate inverted lists for each kind of document  eld. Essentially, you could build one index for document titles, one for document headings, and one for body text. Searching for words in the title is as simple as searching the title index. However,  nding a word in any section of the document is trickier, since you need to fetch inverted lists from many different indexes to make that determination.
Another option is to store information in each word posting about where the word occurred. For instance, we could specify that the number 0 indicates a title and 1 indicates body text. Each inverted list posting would then contain a 0 or a 1 at the end.  is data could be used to quickly determine whether a posting was in a title, and it would require only one bit per posting. However, if you have more  elds than just a title, the representation will grow.
Both of these suggestions have problems when faced with more complicated kinds of document structure. For instance, suppose we want to index books. Some books, like this one, have more than one author. Somewhere in the XML descrip- tion of this book, you might  nd:
   <author>W. Bruce Croft</author>,
   <author>Donald Metzler</author>, and
   <author>Trevor Strohman</author>
Suppose you would like to  nd books by an author named Cro  Donald. If you type the phrase query ”croft donald” into a search engine, should this book match?  e words “cro ” and “donald” appear in it, and in fact, they appear next to each other. However, they are in two distinct author  elds.  is probably is not a good match for the query ”croft donald”, but the previous two methods for dealing with  elds (bits in the posting list, separate indexes) cannot make this kind of distinction.
 is is where extent lists come in. An extent is a contiguous region of a doc- ument. We can represent these extents using word positions. For example, if the title of a book started on the   h word and ended just before the ninth word,
138 5 RankingwithIndexes
we could encode that as (5,9). For the author text shown earlier, we could write author: (1,4), (4,6), (7,9).  e (1,4) means that the  rst three words (“W. Bruce Cro ”) constitute the  rst author, followed by the second author (“Donald Metz- ler”), which is two words.  e word “and” is not in an author  eld, but the next two words are, so the last posting is (7,9).
fish
title
Fig. 5.7. Aligning posting lists for “ sh” and title to  nd matches of the word “ sh” in the title  eld of a document.
Figure 5.7 shows how this works in practice. Here we have the same positions posting list for “ sh” that we used in the previous example. We also have an ex- tent list for the title  eld. For clarity, there are gaps in the posting lists so that the appropriate postings line up next to each other. At the very beginning of both lists, we see that document 1 has a title that contains the  rst two words (1 and 2, ending just before the third word). We know that this title includes the word “ sh”, because the inverted list for “ sh” tells us that “ sh” is the second word in document 1. If the user wants to  nd documents with the word “ sh” in the title, document 1 is a match. Document 2 does not match, because its title ends just be- fore the   h word, but “ sh” doesn’t appear until the seventh word. Document 3 apparently has no title at all, so no matches are possible. Document 4 has a title that starts at the ninth word (perhaps the document begins with a date or an au- thor declaration), and it does contain the word “ sh”. In all, this example shows two matching documents: 1 and 4.
 is concept can be extended to all kinds of  elds, such as headings, para- graphs, or sentences. It can also be used to identify smaller pieces of text with speci c meaning, such as addresses or names, or even just to record which words are verbs.
5.3.5 Scores
If the inverted lists are going to be used to generate feature function values, why not just store the value of the feature function?  is is certainly possible, and some very efficient search engines do just this.  is approach makes it possible to store feature function values that would be too computationally intensive to compute
         1,2
1,4
2,7
2,18
2,23
3,2
3,6
4,3
4,13
   1:(1,3)
2:(1,5)
4:(9,15)
5.3 InvertedIndexes 139
during the query processing phase. It also moves complexity out of the query pro- cessing engine and into the indexing code, where it may be more tolerable.
Let’s make this more concrete. In the last section, there was an example about how a document with the title “Tropical Fish” should be preferred over a docu- ment “Mauritius” for the query “tropical  sh”, even if the Mauritius document contains the words “tropical” and “ sh” many times. Computing the scores that re ect this preference requires some complexity at query evaluation time.  e postings for “tropical  sh” have to be segregated into groups, so we know which ones are in the title and which ones aren’t.  en, we have to de ne some score for the title postings and the non-title postings and mix those numbers together, and this needs to be done for every document.
An alternate approach is to store the  nal value right in the inverted list. We could make a list for “ sh” that has postings like [(1:3.6), (3:2.2)], meaning that the total feature value for “ sh” in document 1 is 3.6, and in document 3 it is 2.2. Presumably the number 3.6 came from taking into account how many times “ sh” appeared in the title, in the headings, in large fonts, in bold, and in links to the document. Maybe the document doesn’t contain the word “ sh” at all, but instead many names of  sh, such as “carp” or “trout”.  e value 3.6 is then some indicator of how much this document is about  sh.
Storing scores like this both increases and decreases the system’s  exibility. It increases  exibility because computationally expensive scoring becomes possible, since much of the hard work of scoring documents is moved into the index. How- ever,  exibility is lost, since we can no longer change the scoring mechanism once the index is built. More importantly, information about word proximity is gone in this model, meaning that we can’t include phrase information in scoring unless we build inverted lists for phrases, too.  ese precomputed phrase lists require considerable additional space.
5.3.6 Ordering
So far, we have assumed that the postings of each inverted list would be ordered by document number. Although this is the most popular option, this is not the only way to order an inverted list. An inverted list can also be ordered by score, so that the highest-scoring documents come  rst.  is makes sense only when the lists already store the score, or when only one kind of score is likely to be com- puted from the inverted list. By storing scores instead of documents, the query processing engine can focus only on the top part of each inverted list, where the
140 5 RankingwithIndexes
highest-scoring documents are recorded.  is is especially useful for queries con- sisting of a single word. In a traditional document-ordered inverted list, the query processing engine would need to scan the whole list to  nd the top k scoring doc- uments, whereas it would only need to read the  rst k postings in a score-sorted list.
5.4 Compression
 ere are many different ways to store digital information. Usually we make a sim- ple distinction between persistent and transient storage. We use persistent stor- age to store things in  les and directories that we want to keep until we choose to delete them. Disks, CDs, DVDs,  ash memory, and magnetic tape are commonly used for this purpose. Dynamic RAM (Random Access Memory), on the other hand, is used to store transient information, which is information we need only while the computer is running. We expect that when we turn off the computer, all of that information will vanish.
We can make  ner distinctions between types of storage based on speed and capacity. Magnetic tape is slow, disks are faster, but dynamic RAM is much faster. Modern computers are so fast that even dynamic RAM isn’t fast enough to keep up, so microprocessors contain at least two levels of cache memory.  e very fastest kind of memory makes up the processor registers. In a perfect world, we could use registers or cache memory for all transient storage, but it is too expen- sive to be practical.
 e reality, then, is that modern computers contain a memory hierarchy. At the top of the hierarchy we have memory that is tiny, but fast.  e base consists of memory that is huge, but slow.  e performance of a search engine strongly depends on how it makes use of the properties of each type of memory.
Compression techniques are the most powerful tool for managing the mem- ory hierarchy.  e inverted lists for a large collection are themselves very large. In fact, when it includes information about word position and document extents, the index can be comparable in size3 to the document collection. Compression allows the same inverted list data to be stored in less space.  e obvious ben- e t is that this could reduce disk or memory requirements, which would save
3 As an example, indexes for TREC collections built using the Indri open source search engine range from 25–50% of the size of the collection.  e lower  gure is for a col- lection of web pages.
 
5.4 Compression 141
money. More importantly, compression allows data to move up the memory hi- erarchy. If index data is compressed by a factor of four, we can store four times more useful data in the processor cache, and we can feed data to the processor four times faster. On disk, compression also squeezes data closer together, which reduces seek times. In multicore and multiprocessor systems, where many proces- sors share one memory system, compressing data allows the processors to share memory bandwidth more efficiently.
Unfortunately, nothing is free.  e space savings of compression comes at a cost: the processor must decompress the data in order to use it.  erefore, it isn’t enough to pick the compression technique that can store the most data in the smallest amount of space. In order to increase overall performance, we need to choose a compression technique that reduces space and is easy to decompress.
To see this mathematically, suppose some processor can process p inverted list postings per second.  is processor is attached to a memory system that can sup- ply the processor with m postings each second.  e number of postings processed each second is then min(m, p). If p > m, then the processor will spend some of its time waiting for postings to arrive from memory. If m > p, the memory sys- tem will sometimes be idle.
Suppose we introduce compression into the system. Our compression system has a compression ratio of r, meaning that we can now store r postings in the same amount of space as one uncompressed posting.  is lets the processor read mr postings each second. However, the processor  rst needs to decompress each posting before processing it.  is slows processing by a decompression factor, d, and lets the processor process dp postings each second. Now we can process min(mr, dp) postings each second.
When we use no compression at all, r = 1 and d = 1. Any reasonable com- pression technique gives us r > 1, but d < 1. We can see that compression is a useful performance technique only when the p > m, that is, when the processor can process inverted list data faster than the memory system can supply it. A very simple compression scheme will raise r a little bit and reduce d a little bit. A com- plicated compression scheme will raise r a lot, while reducing d a lot. Ideally we would like to pick a compression scheme such that min(mr, dp) is maximized, which should happen when mr = dp.
In this section, we consider only lossless compression techniques. Lossless tech- niques store data in less space, but without losing information.  ere are also lossy data compression techniques, which are o en used for video, images, and audio.  ese techniques achieve very high compression ratios (r in our previous discus-
142 5 RankingwithIndexes
sion), but do this by throwing away the least important data. Inverted list prun- ing techniques, which we discuss later, could be considered a lossy compression technique, but typically when we talk about compression we mean only lossless methods.
In particular, our goal with these compression techniques is to reduce the size of the inverted lists we discussed previously.  e compression techniques in this section are particularly well suited for document numbers, word counts, and doc- ument position information.
5.4.1 Entropy and Ambiguity
By this point in the book, you have already seen many examples of probability distributions. Compression techniques are based on probabilities, too.  e fun- damental idea behind compression is to represent common data elements with short codes while representing uncommon data elements with longer codes.  e inverted lists that we have discussed are essentially lists of numbers, and with- out compression, each number takes up the same amount of space. Since some of those numbers are more frequent than others, if we encode the frequent numbers with short codes and the infrequent numbers with longer codes, we can end up with space savings.
For example, consider the numbers 0, 1, 2, and 3. We can encode these num- bers using two binary bits. A sequence of numbers, like:
0,1,0,3,0,2,0 can be encoded in a sequence of binary digits:
00010010001100
Note that the spaces in the binary sequence are there to make it clear where each number starts and stops, and are not actually part of the encoding.
In our example sequence, the number 0 occurs four times, whereas each of the other numbers occurs just once. We may decide to save space by encoding 0 using just a single 0 bit. Our  rst attempt at an encoding might be:
0010100110
 is looks very successful because this encoding uses just 10 bits instead of the 14 bits used previously.  is encoding is, however, ambiguous, meaning that it is not
5.4 Compression 143
clear how to decode it. Remember that the spaces in the code are only there for our convenience and are not actually stored. If we add some different spaces, we arrive at a perfectly valid interpretation of this encoding:
0010100110
which, when decoded, becomes:
0,1,1,0,0,3,0
Unfortunately, this isn’t the data we encoded.  e trouble is that when we see 010 in the encoded data, we can’t be sure whether (0, 2) or (1, 0) was encoded.
 e uncompressed encoding was not ambiguous. We knew exactly where to put the spaces because we knew that each number took exactly 2 bits. In our com- pressed code, encoded numbers consume either 1 or 2 bits, so it is not clear where to put the spaces. To solve this problem, we need to restrict ourselves to unam- biguous codes, which are confusingly called both pre x codes and pre x- ee codes. An unambiguous code is one where there is only one valid way to place spaces in encoded data.
Let’s  x our code so that it is unambiguous:
Number Code 00
1 101
2 110
3 111
 is results in the following encoding:
0101011101100
 is encoding requires 13 bits instead of the 14 bits required by the uncom- pressed version, so we are still saving some space. However, unlike the last code we considered, this one is unambiguous. Notice that if a code starts with 0, it con- sumes 1 bit; if a code starts with 1, it is 3 bits long.  is gives us a deterministic algorithm for placing spaces in the encoded stream.
In the “Exercises” section, you will prove that there is no such thing as an un- ambiguous code that can compress every possible input; some inputs will get big- ger.  is is why it is so important to know something about what kind of data we
      
144 5 RankingwithIndexes
want to encode. In our example, we notice that the number 0 appears frequently, and we can use that fact to reduce the amount of space that the encoded version requires. Entropy measures the predictability of the input. In our case, the input seems somewhat predictable, because the number 0 is more likely to appear than other numbers. We leverage this entropy to produce a usable code for our pur- poses.
5.4.2 Delta Encoding
All of the coding techniques we will consider in this chapter assume that small numbers are more likely to occur than large ones.  is is an excellent assump- tion for word count data; many words appear just once in a document, and some appear two or three times. Only a small number of words appear more than 10 times.  erefore, it makes sense to encode small numbers with small codes and large numbers with large codes.
However, document numbers do not share this property. We expect that a typical inverted list will contain some small document numbers and some very large document numbers. It is true that some documents contain more words, and therefore will appear more times in the inverted lists, but otherwise there is not a lot of entropy in the distribution of document numbers in inverted lists.
 e situation is different if we consider the differences between document numbers instead of the document numbers themselves. Remember that inverted list postings are typically ordered by document number. An inverted list without counts, for example, is just a list of document numbers, like these:
1,5,9,18,23,24,30,44,45,48
Since these document numbers are ordered, we know that each document number in the sequence is more than the one before it and less than the one a er it.  is fact allows us to encode the list of numbers by the differences between adjacent document numbers:
1,4,4,9,5,1,6,14,1,3
 is encoded list starts with 1, indicating that 1 is the  rst document number.  e next entry is 4, indicating that the second document number is 4 more than the  rst: 1 + 4 = 5.  e third number, 4, indicates that the third document number is4morethanthesecond:5+4 = 9.
 is process is called delta encoding, and the differences are o en called d-gaps. Notice that delta encoding does not de ne the bit patterns that are used to store
5.4 Compression 145
the data, and so it does not save any space on its own. However, delta encoding is particularly successful at changing an ordered list of numbers into a list of small numbers. Since we are about to discuss methods for compressing lists of small numbers, this is a useful property.
Before we move on, consider the inverted lists for the words “entropy” and “who.”  e word “who” is very common, so we expect that most documents will contain it. When we use delta encoding on the inverted list for “who,” we would expect to see many small d-gaps, such as:
1,1,2,1,5,1,4,1,1,3,...
By contrast, the word “entropy” rarely appears in text, so only a few documents
will contain it.  erefore, we would expect to see larger d-gaps, such as:
109, 3766, 453, 1867, 992, ...
However, since “entropy” is a rare word, this list of large numbers will not be very long. In general, we will  nd that inverted lists for frequent terms compress very well, whereas infrequent terms compress less well.
5.4.3 Bit-Aligned Codes
 e code we invented in section 5.4.1 is a bit-aligned code, meaning that the breaks between the coded regions (the spaces) can happen a er any bit posi- tion. In this section we will describe some popular bit-aligned codes. In the next section, we will discuss methods where code words are restricted to end on byte boundaries. In all of the techniques we’ll discuss, we are looking at ways to store small numbers in inverted lists (such as word counts, word positions, and delta- encoded document numbers) in as little space as possible.
One of the simplest codes is the unary code. You are probably familiar with bi- nary, which encodes numbers with two symbols, typically 0 and 1. A unary num- ber system is a base-1 encoding, which means it uses a single symbol to encode numbers. Here are some examples:
Number Code 00
1   10
2   110
3   1110
4   11110 5   111110
   
146 5 RankingwithIndexes
In general, to encode a number k in unary, we output k 1s, followed by a 0. We need the 0 at the end to make the code unambiguous.
 is code is very efficient for small numbers such as 0 and 1, but quickly be- comes very expensive. For instance, the number 1023 can be represented in 10 binary bits, but requires 1024 bits to represent in unary code.
Now we know about two kinds of numeric encodings. Unary is convenient because it is compact for small numbers and is inherently unambiguous. Binary is a better choice for large numbers, but it is not inherently unambiguous. A rea- sonable compression scheme needs to encode frequent numbers with fewer bits than infrequent numbers, which means binary encoding is not useful on its own for compression.
Elias-γ codes
 e Elias-γ (Elias gamma) code combines the strengths of unary and binary
codes. To encode a number k using this code, we compute two quantities: • kd=⌊log2k⌋
• kr =k−2⌊log2k⌋
Suppose you wrote k in binary form.  e  rst value, kd, is the number of binary digits you would need to write. Assuming k > 0, the le most binary digit of k is 1. If you erase that digit, the remaining binary digits are kr.
If we encode kd in unary and kr in binary (in kd binary digits), we get the Elias-γ code. Some examples are shown in Table 5.2.
Number (k) kd   kr Code 1000
21   0100 31   1101 62   211010
15 3   71110111
                 0 11110 0000
255   7   127 11111110 1111111
1023   9   511 1111111110 111111111 Table 5.2. Elias-γ code examples
164
  
5.4 Compression 147
 e trick with this code is that the unary part of the code tells us how many bits toexpectinthebinarypart.Weendupwithacodethatusesnomorebitsthanthe unary code for any number, and for numbers larger than 2, it uses fewer bits.  e savings for large numbers is substantial. We can, for example, now encode 1023 in 19 bits, instead of 1024 using just unary code.
For any number k, the Elias-γ code requires ⌊log2k⌋ + 1 bits for kd in unary code and ⌊log2k⌋ bits for kr in binary.  erefore, 2⌊log2k⌋ + 1 bits are required in all.
Elias-δ codes
Although the Elias-γ code is a major improvement on the unary code, it is not ideal for inputs that might contain large numbers. We know that a number k can be expressed in log2 k binary digits, but the Elias-γ code requires twice as many bits in order to make the encoding unambiguous.
 e Elias-δ code attempts to solve this problem by changing the way that kd is encoded. Instead of encoding kd in unary, we can encode kd + 1 in Elias-γ. In particular, we split kd into:
• kdd=⌊log2(kd+1)⌋
• kdr = (kd + 1) − 2⌊log2(kd+1)⌋
Notice that we use kd + 1 here, since kd may be zero, but log2 0 is unde ned. We then encode kdd in unary, kdr in binary, and kr in binary.  e value of kdd is thelengthofkdr,andkdr isthelengthofkr,whichmakesthiscodeunambiguous.
Table 5.3 gives some examples of Elias-δ encodings.
Number (k) kd   kr kdd kdr Code 100000
             2   1 0   1 3   1 1   1 6   2 2   1
15   3 7   2 16   4 0   2
01000
01001
110110
011000111 1110010000
0 1110 000 1111111
2 1110 010 111111111
code examples
                255   7 127 1023   9 511
3 3
    Table 5.3. Elias-δ
148 5 RankingwithIndexes
Elias-δ sacri ces some efficiency for small numbers in order to gain efficiency at encoding larger numbers. Notice that the code for the number 2 has increased to 4 bits instead of the 3 bits required by the Elias-γ code. However, for numbers larger than 16, the Elias-δ code requires no more space than the Elias-γ code, and for numbers larger than 32, the Elias-δ requires less space.
Speci cally, the Elias-γ code requires ⌊log2(⌊log2 k⌋ + 1)⌋ + 1 bits for kdd in unary, followed by ⌊log2(⌊log2 k⌋ + 1)⌋ bits for kdr in binary, and ⌊log2 k⌋ bits for kr in binary.  e total cost is approximately 2 log2 log2 k + log2 k.
5.4.4 Byte-Aligned Codes
Even though a few tricks can help us decode bit-aligned codes quickly, codes of variable bit length are cumbersome on processors that process bytes.  e proces- sor is built to handle bytes efficiently, not bits, so it stands to reason that byte- aligned codes would be faster in practice.
 ere are many examples of byte-aligned compression schemes, but we con- sider only one popular method here.  is is the code commonly known as v-byte, which is an abbreviation for “variable byte length.”  e v-byte method is very similar to UTF-8 encoding, which is a popular way to represent text (see section 3.5.1).
Like the other codes we have studied so far, the v-byte method uses short codes for small numbers and longer codes for longer numbers. However, each code is a series of bytes, not bits. So, the shortest v-byte code for a single integer is one byte. In some circumstances, this could be very space-inefficient; encoding the number 1 takes eight times as much space in v-byte as in Elias-γ. Typically, the difference in space usage is not quite so dramatic.
 e v-byte code is really quite simple.  e low seven bits of each byte con- tain numeric data in binary.  e high bit is a terminator bit.  e last byte of each code has its high bit set to 1; otherwise, it is set to 0. Any number that can be represented in seven binary digits requires one byte to encode. More information about space usage is shown in Table 5.4.
Some example encodings are shown in Table 5.5. Numbers less than 128 are stored in a single byte in traditional binary form, except that the high bit is set. For larger numbers, the least signi cant seven bits are stored in the  rst byte.  e next seven bits are stored in the next byte until all of the non-zero bits have been stored.
Storing compressed data with a byte-aligned code has many advantages over a bit-aligned code. Byte-aligned codes compress and decompress faster, since pro-
5.4 Compression 149
Table 5.4. Space requirements for numbers encoded in v-byte
k Binary Code   Hexadecimal 1   10000001   81 6   10000110   86
 k
k < 27 27≤k<214   2 214 ≤k<221   3 221 ≤k<228   4
Number of bytes 1
      127 1 1111111
128 0 0000001 1 0000000
130 0 0000001 1 0000010
FF 01 80 01 82 011CA0
cessors (and programming languages) are designed to process bytes instead of bits. For these reasons, the Galago search engine associated with this book uses v-byte exclusively for compression.
5.4.5 Compression in Practice
 e compression techniques we have covered are used to encode inverted lists in real retrieval systems. In this section, we’ll look at how Galago uses compression to encode inverted lists in the PositionListWriter class.
Figure 5.5 illustrates how position information can be stored in inverted lists. Consider just the inverted list for tropical:
(1, 1)(1, 7)(2, 6)(2, 17)(3, 1)
In each pair, the  rst number represents the document and the second number represents the word position. For instance, the third entry in this list states that the word tropical is the sixth word in document 2. Because it helps the example, we’ll add (2, 197) to the list:
(1, 1)(1, 7)(2, 6)(2, 17)(2, 197)(3, 1)
      20000 0 0000001 0 0011100 1 0100000 Table 5.5. Sample encodings for v-byte
150 5 RankingwithIndexes
Wecangroupthepositionsforeachdocumenttogethersothateachdocument has its own entry, (document, count, [positions]), where count is the number of occurrences in the document. Our example data now looks like this:
(1, 2, [1, 7])(2, 3, [6, 17, 197])(3, 1, [1])
 e word count is important because it makes this list decipherable even with- out the parentheses and brackets.  e count tells us how many positions lie within the brackets, and we can interpret these numbers unambiguously, even if they were printed as follows:
1,2,1,7,2,3,6,17,197,3,1,1
However, we will leave the brackets in place for now for clarity.
 ese are small numbers, but with delta encoding we can make them smaller. Notice that the document numbers are sorted in ascending order, so we can safely use delta encoding to encode them:
(1, 2, [1, 7])(1, 3, [6, 17, 197])(1, 1, [1])
 e second entry now starts with a 1 instead of a 2, but this 1 means “this document number is one more than the last document number.” Since position information is also sorted in ascending order, we can delta-encode the positions as well:
(1, 2, [1, 6])(1, 3, [6, 11, 180])(1, 1, [1])
We can’t delta-encode the word counts, because they’re not in ascending order. If we did delta-encode them, some of the deltas might be negative, and the com- pression techniques we have discussed do not handle negative numbers without some extra work.
Now we can remove the brackets and consider this inverted list as just a list of numbers:
1,2,1,6,1,3,6,11,180,1,1,1
Since most of these numbers are small, we can compress them with v-byte to save space:
               81 82 81 86 81 83 86 8B 01 B4 81 81 81
5.4 Compression 151
 e 01 B4 is 180, which is encoded in two bytes.  e rest of the numbers were encoded as single bytes, giving a total of 13 bytes for the entire list.
5.4.6 Looking Ahead
 is section described three compression schemes for inverted lists, and there are many others in common use. Even though compression is one of the older areas of computer science, new compression schemes are developed every year.
Why are these new schemes necessary? Remember at the beginning of this sec- tion we talked about how compression allows us to trade processor computation for data throughput.  is means that the best choice for a compression algorithm is tightly coupled with the state of modern CPUs and memory systems. For a long time, CPU speed was increasing much faster than memory throughput, so com- pression schemes with higher compression ratios became more attractive. How- ever, the dominant hardware trend now is toward many CPU cores with lower clock speeds. Depending on the memory throughput of these systems, lower com- pression ratios may be attractive.
More importantly, modern CPUs owe much of their speed to clever tricks such as branch prediction, which helps the processor guess about how code will execute. Code that is more predictable can run much faster than unpredictable code. Many of the newest compression schemes are designed to make the decode phase more predictable, and therefore faster.
5.4.7 Skipping and Skip Pointers
For many queries, we don’t need all of the information stored in a particular in- verted list. Instead, it would be more efficient to read just the small portion of the data that is relevant to the query. Skip pointers help us achieve that goal.
Consider the Boolean query “galago AND animal”.  e word “animal” occurs in about 300 million documents on the Web versus approximately 1 million for “galago.” If we assume that the inverted lists for “galago” and “animal” are in doc- ument order, there is a very simple algorithm for processing this query:
• Let dg be the  rst document number in the inverted list for “galago.”
• Let da be the  rst document number in the inverted list for “animal.”
• While there are still documents in the lists for “galago” and “animal,” loop:
– If dg < da, set dg to the next document number in the “galago” list. – If da < dg , set da to the next document number in the “animal” list.
152 5 RankingwithIndexes
– If da = dg, the document da contains both “galago” and “animal”. Move both dg and da to the next documents in the inverted lists for “galago” and “animal,” respectively.
Unfortunately, this algorithm is very expensive. It processes almost all docu- ments in both inverted lists, so we expect the computer to process this loop about 300 million times. Over 99% of the processing time will be spent processing the 299 million documents that contain “animal” but do not contain “galago.”
We can change this algorithm slightly by skipping forward in the “animal” list. Everytimewe ndthatda <dg,weskipaheadkdocumentsinthe“animal”list toanewdocument,sa.Ifsa <dg,weskipaheadbyanotherkdocuments.We dothisuntilsa ≥dg.Atthispoint,wehavenarrowedoursearchdowntoarange of k documents that might contain dg , which we can search linearly.
How much time does the modi ed algorithm take? Since the word “galago” appears 1 million times, we know that the algorithm will perform 1 million lin- ear searches of length k, giving an expected cost of 500, 000 × k steps. We also expect to skip forward 300, 000, 000/k times.  is algorithm then takes about 500, 000 × k + 300, 000, 000/k steps in total.
k Steps
5 62.5 million 10 35 million 20 25 million 25 24.5 million 40 27.5 million 50 31 million
100 53 million
Table 5.6. Skip lengths (k) and expected processing steps
Table 5.6 shows the number of processing steps required for some example values of k. We get the best expected performance when we skip 25 documents at a time. Notice that at this value of k, we expect to have to skip forward 12 times in the “animal” list for each occurrence of “galago.”  is is because of the cost of linear search: a larger value of k means more elements to check in the linear search. If we choose a binary search instead, the best value of k rises to about 208, with about 9.2 million expected steps.
         
5.4 Compression 153
If binary search combined with skipping is so much more efficient, why even consider linear search at all?  e problem is compression. For binary search to work, we need to be able to jump directly to elements in the list, but a er com- pression, every element could take a different amount of space. In addition, delta encoding may be used on the document numbers, meaning that even if we could jump to a particular location in the compressed sequence, we would need to de- compress everything up to that point in order to decode the document numbers.  is is discouraging because our goal is to reduce the amount of the list we need to process, and it seems that compression forces us to decompress the whole list.
We can solve the compression problem with a list of skip pointers. Skip pointer lists are small additional data structures built into the index to allow us to skip through the inverted lists efficiently.
A skip pointer (d, p) contains two parts, a document number d and a byte (or bit) position p.  is means that there is an inverted list posting that starts at position p, and that the posting immediately before it is for document d. Notice that this de nition of the skip pointer solves both of our compression problems: we can start decoding at position p, and since we know that d is the document immediately preceding p, we can use it for decoding.
As a simple example, consider the following list of document numbers, un- compressed:
5, 11, 17, 21, 26, 34, 36, 37, 45, 48, 51, 52, 57, 80, 89, 91, 94, 101, 104, 119 If we delta-encode this list, we end up with a list of d-gaps like this:
5,6,6,4,5,9,2,1,8,3,3,1,5,23,9,2,3,7,3,15
We can then add some skip pointers for this list, using 0-based positions (that
is, the number 5 is at position 0 in the list):
(17, 3), (34, 6), (45, 9), (52, 12), (89, 15), (101, 18)
Suppose we try decoding using the skip pointer (34, 6). We move to position 6 in the d-gaps list, which is the number 2. We add 34 to 2, to decode document number 36.
More generally, if we want to  nd document number 80 in the list, we scan the list of skip pointers until we  nd (52, 12) and (89, 15). 80 is larger than 52 but less than 89, so we start decoding at position 12. We  nd:
154 5 RankingwithIndexes
• 52+5=57
• 57+23=80
At this point, we have successfully found 80 in the list. If instead we were searching for 85, we would again start at skip pointer (52, 12):
• 52+5=57
• 57+23=80
• 80+9=89
At this point, since 85 < 89, we would know that 85 is not in the list.
In the analysis of skip pointers for the “galago AND animal” example, the effec- tiveness of the skip pointers depended on the fact that “animal” was much more common than “galago.” We found that 25 was a good value for k given this query, but we only get to choose one value for k for all queries.  e best way to choose k is to  nd the best possible k for some realistic sample set of queries. For most collections and query loads, the optimal skip distance is around 100 bytes.
5.5 Auxiliary Structures
 e inverted  le is the primary data structure in a search engine, but usually other structures are necessary for a fully functional system.
Vocabulary and statistics
An inverted  le, as described in this chapter, is just a collection of inverted lists. To search the index, some kind of data structure is necessary to  nd the inverted list for a particular term.  e simplest way to solve this problem is to store each inverted list as a separate  le, where each  le is named a er the corresponding search term. To  nd the inverted list for “dog,” the system can simply open the  le named dog and read the contents. However, as we saw in Chapter 4, document collections can have millions of unique words, and most of these words will occur only once or twice in the collection.  is means that an index, if stored in  les, would consist of millions of  les, most of which are very small.
Unfortunately, modern  le systems are not optimized for this kind of stor- age. A  le system typically will reserve a few kilobytes of space for each  le, even though most  les will contain just a few bytes of data.  e result is a huge amount of wasted space. As an example, in the AP89 collection, over 70,000 words occur
5.5 AuxiliaryStructures 155
just once (see Table 4.1).  ese inverted lists would require about 20 bytes each, for a total of about 2MB of space. However, if the  le system requires 1KB for each  le, the result is 70MB of space used to store 2MB of data. In addition, many  le systems still store directory information in unsorted arrays, meaning that  le lookups can be very slow for large  le directories.
To  x these problems, inverted lists are usually stored together in a single  le, which explains the name inverted  le. An additional directory structure, called the vocabulary or lexicon, contains a lookup table from index terms to the byte offset of the inverted list in the inverted  le.
In many cases, this vocabulary lookup table will be small enough to  t into memory. In this case, the vocabulary data can be stored in any reasonable way on disk and loaded into a hash table at search engine startup. If the search engine needs to handle larger vocabularies, some kind of tree-based data structure, such as a B-tree, should be used to minimize disk accesses during the search process.
Galago uses a hybrid strategy for its vocabulary structure. A small  le in each index, called vocabulary, stores an abbreviated lookup table from vocabulary terms to offsets in the inverted  le.  is  le contains just one vocabulary entry for each 32K of data in the inverted  le.  erefore, a 32TB inverted  le would require less than 1GB of vocabulary space, meaning that it can always be stored in memory for collections of a reasonable size.  e lists in the inverted  le are stored in alphabetical order. To  nd an inverted list, the search engine uses bi- nary search to  nd the nearest entry in the vocabulary table, and reads the offset from that entry.  e engine then reads 32KB of the inverted  le, starting at the offset.  is approach  nds each inverted list with just one disk seek.
To compute some feature functions, the index needs to contain certain vo- cabulary statistics, such as the term frequency or document frequency (discussed in Chapter 4). When these statistics pertain to a speci c term, they can be eas- ily stored at the start of the inverted list. Some of these statistics pertain to the corpus, such as the total number of documents stored. When there are just a few of these kinds of statistics, efficient storage considerations can be safely ignored. Galago stores these collection-wide statistics in an XML  le called manifest.
Documents, snippets, and external systems
 e search engine, as described so far, returns a list of document numbers and scores. However, a real user-focused search engine needs to display textual infor- mation about each document, such as a document title, URL, or text summary
156 5 RankingwithIndexes
(Chapter 6 explains this in more detail). In order to get this kind of information, the text of the document needs to be retrieved.
In Chapter 3, we saw some ways that documents can be stored for fast access.  ere are many ways to approach this problem, but in the end, a separate system is necessary to convert search engine results from numbers into something readable by people.
5.6 Index Construction
Before an index can be used for query processing, it has to be created from the text collection. Building a small index is not particularly difficult, but as input sizes grow, some index construction tricks can be useful. In this section, we will look at simple in-memory index construction  rst, and then consider the case where the input data does not  t in memory. Finally, we will consider how to build indexes using more than one computer.
5.6.1 Simple Construction
Pseudocode for a simple indexer is shown in Figure 5.8.  e process involves only a few steps. A list of documents is passed to the BuildIndex function, and the function parses each document into tokens, as discussed in Chapter 4.  ese to- kens are words, perhaps with some additional processing, such as downcasing or stemming.  e function removes duplicate tokens, using, for example, a hash ta- ble.  en, for each token, the function determines whether a new inverted list needs to be created in I, and creates one if necessary. Finally, the current docu- ment number, n, is added to the inverted list.
 e result is a hash table of tokens and inverted lists.  e inverted lists are just lists of integer document numbers and contain no special information.  is is enough to do very simple kinds of retrieval, as we saw in section 5.3.1.
As described, this indexer can be used for many small tasks—for example, in- dexing less than a few thousand documents. However, it is limited in two ways. First, it requires that all of the inverted lists be stored in memory, which may not be practical for larger collections. Second, this algorithm is sequential, with no obvious way to parallelize it.  e primary barrier to parallelizing this algorithm is the hash table, which is accessed constantly in the inner loop. Adding locks to the hash table would allow parallelism for parsing, but that improvement alone will
procedure BI(D)
I ← HashTable()
n ← 0
for all documents d ∈ D do
n←n+1
T ← Parse(d)
Remove duplicates from T for all tokens t ∈ T do
if It ̸∈ I then
It ← Array()
end if
It .append(n) end for
end for
return I end procedure
5.6 IndexConstruction 157
◃ D is a set of text documents ◃ Inverted list storage ◃ Document numbering
◃ Parse document into tokens
Fig. 5.8. Pseudocode for a simple indexer
not be enough to make use of more than a handful of CPU cores. Handling large
collections will require less reliance on memory and improved parallelism.
5.6.2 Merging
 e classic way to solve the memory problem in the previous example is by merg- ing. We can build the inverted list structure I until memory runs out. When that happens, we write the partial index I to disk, then start making a new one. At the end of this process, the disk is  lled with many partial indexes, I1, I2, I3, ..., In.  e system then merges these  les into a single result.
By de nition, it is not possible to hold even two of the partial index  les in memory at one time, so the input  les need to be carefully designed so that they can be merged in small pieces. One way to do this is to store the partial indexes in alphabetical order. It is then possible for a merge algorithm to merge the partial indexes using very little memory.
Figure 5.9 shows an example of this kind of merging procedure. Even though this  gure shows only two indexes, it is possible to merge many at once.  e algo- rithm is essentially the same as the standard merge sort algorithm. Since both I1 and I2 are sorted, at least one of them points to the next piece of data necessary to write to I.  e data from the two  les is interleaved to produce a sorted result.
158 5 RankingwithIndexes
Index A Index B
Index A
Index B aardvark Combined index
Fig. 5.9. An example of index merging.  e  rst and second indexes are merged together to produce the combined index.
Since I1 and I2 may have used the same document numbers, the merge function renumbers documents in I2.
 is merging process can succeed even if there is only enough memory to store two words (w1 and w2), a single inverted list posting, and a few  le pointers. In practice, a real merge function would read large chunks of I1 and I2, and then write large chunks to I in order to use the disk most efficiently.
 is merging strategy also shows a possible parallel indexing strategy. If many machines build their own partial indexes, a single machine can combine all of those indexes together into a single,  nal index. However, in the next section, we will explore more recent distributed indexing frameworks that are becoming popular.
5.6.3 Parallelism and Distribution
 e traditional model for search engines has been to use a single, fast machine to create the index and process queries.  is is still the appropriate choice for a large number of applications, but it is no longer a good choice for the largest systems. Instead, for these large systems, it is increasingly popular to use many inexpen- sive servers together and use distributed processing so ware to coordinate their activities. MapReduce is a distributed processing tool that makes this possible.
Two factors have forced this shi . First, the amount of data to index in the largest systems is exploding. Modern web search engines already index tens of bil- lions of pages, but even larger indexes are coming. Consider that if each person on earth wrote one blog post each day, the Web would increase in size by over two trillion pages every year. Optimistically, one typical modern computer can handle a few hundred million pages, although not with the kind of response times that
 aardvark
 2
 3
 4
 5
 apple
 2
 4
   aardvark
 6
 9
 actor
 15
 42
 68
    aardvark
 2
 3
 4
 5
  apple
 2
 4
  6
 9
 actor
 15
 42
 68
    aardvark
 2
 3
 4
 5
 6
 9
 actor
 15
 42
 68
 apple
 2
 4
    
5.6 IndexConstruction 159
mostusersexpect. isleavesahugegulfbetweenthesizeoftheWebandwhatwe can handle with current single-computer technology. Note that this problem is not restricted to a few major web search companies; many more companies want to analyze the content of the Web instead of making it available for public search.  ese companies have the same scalability problem.
 e second factor is simple economics.  e incredible popularity of personal computers has made them very powerful and inexpensive. In contrast, large com- puters serve a very small market, and therefore have fewer opportunities to de- velop economies of scale. Over time, this difference in scale has made it difficult to make a computer that is much more powerful than a personal computer that is still sold for a reasonable amount of money. Many large information retrieval systems ran on mainframes in the past, but today’s platform of choice consists of many inexpensive commodity servers.
Inexpensive servers have a few disadvantages when compared to mainframes. First, they are more likely to break, and the likelihood of at least one server fail- ure goes up as you add more servers. Second, they are difficult to program. Most programmers are well trained for single-threaded programming, less well trained for threaded or multi-process programming, and not well trained at all for coop- erative network programming. Many programming toolkits have been developed to help address this kind of problem. RPC, CORBA, Java RMI, and SOAP have been developed to allow function calls across machine boundaries. MPI provides a different abstraction, called message passing, which is popular for many scienti c tasks. None of these techniques are particularly robust against system failures, and the programming models can be complex. In particular, these systems do not help distribute data evenly among machines; that is the programmer’s job.
Data placement
Before diving into the mechanics of distributed processing, consider the problems of handling huge amounts of data on a single computer. Distributed processing and large-scale data processing have one major aspect in common, which is that not all of the input data is available at once. In distributed processing, the data might be scattered among many machines. In large-scale data processing, most of the data is on the disk. In both cases, the key to efficient data processing is placing the data correctly.
Let’s take a simple example. Suppose you have a text  le that contains data about credit card transactions. Each line of the  le contains a credit card number
160 5 RankingwithIndexes
and an amount of money. How might you determine the number of unique credit card numbers in the  le?
If the  le is not very big, you could read each line, parse the credit card num- ber, and store the credit card number in a hash table. Once the entire  le had been read, the hash table would contain one entry for each unique credit card number. Counting the number of entries in the hash table would give you the answer. Un- fortunately, for a big  le, the hash table would be too large to store in memory.
Now suppose you had the very same credit card data, but the transactions in the  le were ordered by credit card number. Counting the number of unique credit card numbers in this case is very simple. Each line in the  le is read and the credit card number on the line is parsed. If the credit card number found is different than the one on the line before it, a counter is incremented. When the end of the  le is reached, the counter contains a count of the unique credit card numbers in the  le. No hash table is necessary for this to work.
Now, back to distributed computation. Suppose you have more than one com- puter to use for this counting task. You can split the big  le of transactions into small batches of transactions. Each computer can count its fraction, and then the results can be merged together to produce a  nal result.
Initially, we start with an unordered  le of transactions. We split that  le into small batches of transactions and count the unique credit card numbers in each batch. How do we combine the results? We could add the number of credit card numbers found in each batch, but this is incorrect, since the same credit card num- ber might appear in more than one batch, and therefore would be counted more than once in the  nal total. Instead, we would need to keep a list of the unique credit card numbers found in each batch, and then merge those lists together to make a  nal result list.  e size of this  nal list is the number of unique credit card numbers in the whole set.
In contrast, suppose the transactions are split into batches with more care, so that all transactions made with the same credit card end up in the same batch. With this extra restriction, each batch can be counted individually, and then the counts from each batch can be added to make a  nal result. No merge is necessary, because there is no possibility of double-counting. Each credit card number will appear in precisely one batch.
 ese examples might be a little bit tedious, but the point is that proper data grouping can radically change the performance characteristics of a task. Using a sorted input  le made the counting task easy, reduced the amount of memory needed to nearly zero, and made it possible to distribute the computation easily.
MapReduce
MapReduce is a distributed programming framework that focuses on data place- ment and distribution. As we saw in the last few examples, proper data placement can make some problems very simple to compute. By focusing on data placement, MapReduce can unlock the parallelism in some common tasks and make it easier to process large amounts of data.
MapReduce gets its name from the two pieces of code that a user needs to write in order to use the framework: the Mapper and the Reducer.  e MapReduce library automatically launches many Mapper and Reducer tasks on a cluster of machines.  e interesting part about MapReduce, though, is the path the data takes between the Mapper and the Reducer.
Before we look at how the Mapper and Reducer work, let’s look at the founda- tions of the MapReduce idea.  e functions map and reduce are commonly found in functional languages. In very simple terms, the map function transforms a list of items into another list of items of the same length.  e reduce function trans- forms a list of items into a single item.  e MapReduce framework isn’t quite so strict with its de nitions: both Mappers and Reducers can return an arbitrary number of items. However, the general idea is the same.
Map
Input
5.6 IndexConstruction 161
   Reduce
  Output
                Fig. 5.10. MapReduce
Shuffle
162 5 RankingwithIndexes
We assume that the data comes in a set of records.  e records are sent to the Mapper, which transforms these records into pairs, each with a key and a value.  e next step is the shuffle, which the library performs by itself.  is operation uses a hash function so that all pairs with the same key end up next to each other and on the same machine.  e  nal step is the reduce stage, where the records are processed again, but this time in batches, meaning all pairs with the same key are processed at once.  e MapReduce steps are summarized in Figure 5.10.
procedure MCC(input) while not input.done() do
record ← input.next() card ← record.card amount ← record.amount Emit(card, amount)
end while end procedure
Fig. 5.11. Mapper for a credit card summing algorithm
procedure RCC(key, values) total ← 0
card ← key
while not values.done() do
amount ← values.next()
total ← total + amount end while
Emit(card, total)
end procedure
Fig. 5.12. Reducer for a credit card summing algorithm
 e credit card data example we saw in the previous section works well as a MapReduce task. In the Mapper (Figure 5.11), each record is split into a key (the credit card number) and a value (the money amount in the transaction).  e shuffle stage sorts the data so that the records with the same credit card number end up next to each other.  e reduce stage emits a record for each unique credit
5.6 IndexConstruction 163
card number, so the total number of unique credit card numbers is the number of records emitted by the reducer (Figure 5.12).
Typically, we assume that both the Mapper and Reducer are idempotent. By idempotent, we mean that if the Mapper or Reducer is called multiple times on the same input, the output will always be the same.  is idempotence allows the MapReduce library to be fault tolerant. If any part of the computation fails, per- haps because of a hardware machine failure, the MapReduce library can just pro- cess that part of the input again on a different machine. Even when machines don’t fail, sometimes machines can be slow because of miscon guration or slowly failing parts. In this case, a machine that appears to be normal could return re- sults much more slowly than other machines in a cluster. To guard against this, as the computation nears completion, the MapReduce library issues backup Map- pers and Reducers that duplicate the processing done on the slowest machines.  is ensures that slow machines don’t become the bottleneck of a computation.  e idempotence of the Mapper and Reducer are what make this possible. If the Mapper or Reducer modi ed  les directly, for example, multiple copies of them could not be run simultaneously.
Let’s look at the problem of indexing a corpus with MapReduce. In our simple indexer, we will store inverted lists with word positions.
procedure MDTP(input) while not input.done() do
document ← input.next() number ← document.number position ← 0
tokens ← Parse(document) for each word w in tokens do
Emit(w, document:position)
position = position + 1
end for end while
end procedure
Fig. 5.13. Mapper for documents
MapDocumentsToPostings (Figure 5.13) parses each document in the input. At each word position, it emits a key/value pair: the key is the word itself, and the value is document:position, which is the document number and the position
164 5 RankingwithIndexes
procedure RPTL(key, values) word ← key
WriteWord(word)
while not input.done() do
EncodePosting(values.next())
end while end procedure
Fig. 5.14. Reducer for word postings
concatenated together. When ReducePostingsToLists (Figure 5.14) is called, the emitted postings have been shuffled so that all postings for the same word are together.  e Reducer calls WriteWord to start writing an inverted list and then uses EncodePosting to write each posting.
5.6.4 Update
So far, we have assumed that indexing is a batch process.  is means that a set of documents is given to the indexer as input, the indexer builds the index, and then the system allows users to run queries. In practice, most interesting document col- lections are constantly changing. At the very least, collections tend to get bigger over time; every day there is more news and more email. In other cases, such as web search or  le system search, the contents of documents can change over time as well. A useful search engine needs to be able to respond to dynamic collections.
We can solve the problem of update with two techniques: index merging and result merging. If the index is stored in memory, there are many options for quick index update. However, even if the search engine is evaluating queries in mem- ory, typically the index is stored on a disk. Inserting data in the middle of a  le is not supported by any common  le system, so direct disk-based update is not straightforward. We do know how to merge indexes together, though, as we saw in section 5.6.2.  is gives us a simple approach for adding data to the index: make a new, smaller index (I2) and merge it with the old index (I1) to make a new in- dex containing all of the data (I). Postings in I1 for any deleted documents can be ignored during the merge phase so they do not appear in I.
Index merging is a reasonable update strategy when index updates come in large batches, perhaps many thousands of documents at a time. For single docu- ment updates, it isn’t a very good strategy, since it is time-consuming to write the entire index to disk. For these small updates, it is better to just build a small index
5.7 QueryProcessing 165
for the new data, but not merge it into the larger index. Queries are evaluated sep- arately against the small index and the big index, and the result lists are merged to  nd the top k results.
Result merging solves the problem of how to handle new documents: just put them in a new index. But how do we delete documents from the index?  e com- mon solution is to use a deleted document list. During query processing, the sys- tem checks the deleted document list to make sure that no deleted documents enter the list of results shown to the user. If the contents of a document change, we can delete the old version from the index by using a deleted document list and then add a new version to the recent documents index.
Results merging allows us to consider a small, in-memory index structure to hold new documents.  is in-memory structure could be a hash table of arrays, as shown in Figure 5.8, and therefore would be simple and quick to update, even with only a single document.
To gain even more performance from the system, instead of using just two indexes (an in-memory index and a disk-based index), we can use many indexes. Using too many indexes is a bad idea, since each new index slows down query processing. However, using too few indexes results in slow index build throughput because of excessive disk traffic. A particularly elegant solution to this problem is geometric partitioning. In geometric partitioning, the smallest index, I0, contains about as much data as would  t in memory.  e next index, I1, contains about r times as much data as I1. If m is the amount of bytes of memory in the machine, index In then contains between mrn and (m + 1)rn bytes of data. If index In ever contains more than (m + 1)rn, it is merged into index In+1. If r = 2, the system can hold 1000m bytes of index data using just 10 indexes.
5.7 Query Processing
Once an index is built, we need to process the data in it to produce query results. Even with simple algorithms, processing queries using an index is much faster than it is without one. However, clever algorithms can boost query processing speed by ten to a hundred times over the simplest versions. We will explore the simplest two query processing techniques  rst, called document-at-a-time and term-at-a-time, and then move on to faster and more  exible variants.
166 5 RankingwithIndexes
5.7.1 Document-at-a-time Evaluation
Document-at-a-time retrieval is the simplest way, at least conceptually, to per- form retrieval with an inverted  le. Figure 5.15 is a picture of document-at-a-time retrieval for the query “salt water tropical”.  e inverted lists are shown horizon- tally, although the postings have been aligned so that each column represents a different document.  e inverted lists in this example hold word counts, and the score, for this example, is just the sum of the word counts in each document.  e vertical gray lines indicate the different steps of retrieval. In the  rst step, all the counts for the  rst document are added to produce the score for that document. Once the scoring for the  rst document has completed, the second document is scored, then the third, and then the fourth.
salt 1:1 4:1 water 1:1 2:1 4:1
tropical 1:2 2:2
score 1:4 2:3 4:2
Fig. 5.15. Document-at-a-time query evaluation.  e numbers (x:y) represent a docu- ment number (x) and a word count (y).
Figure 5.16 shows a pseudocode implementation of this strategy.  e param- eters are Q, the query; I, the index; f and g, the sets of feature functions; and k, the number of documents to retrieve.  is algorithm scores documents using the abstract model of ranking described in section 5.2. However, in this simpli ed example, we assume that the only non-zero feature values for g(Q) correspond to the words in the query.  is gives us a simple correspondence between inverted lists and features: there is one list for each query term, and one feature for each list. Later in this chapter we will explore structured queries, which are a standard way of moving beyond this simple model.
For each word wi in the query, an inverted list is fetched from the index.  ese inverted lists are assumed to be sorted in order by document number.  e Invert- edList object starts by pointing at the  rst posting in each list. All of the fetched inverted lists are stored in an array, L.
           3:1
    3:1
5.7 QueryProcessing 167
procedure DAATR(Q, I, f, g, k) L ← Array()
R ← PriorityQueue(k) for all terms wi in Q do
li ←InvertedList(wi,I)
L.add( li ) end for
for all documents d ∈ I do sd ← 0
for all inverted lists li in L do
if li.getCurrentDocument() = d then
sd ← sd + gi(Q)fi(li) end if
li.movePastDocument( d ) end for
R.add( sd, d ) end for
return the top k results from R end procedure
◃ Update the document score
Fig. 5.16. A simple document-at-a-time retrieval algorithm
In the main loop, the function loops once for each document in the collection. At each document, all of the inverted lists are checked. If the document appears in one of the inverted lists, the feature function fi is evaluated, and the docu- ment score sD is computed by adding up the weighted function values.  en, the inverted list pointer is moved to point at the next posting. At the end of each doc- ument loop, a new document score has been computed and added to the priority queue R.
For clarity, this pseudocode is free of even simple performance-enhancing changes. Realistically, however, the priority queue R only needs to hold the top k results at any one time. If the priority queue ever contains more than k results, the lowest-scoring documents can be removed until only k remain, in order to save memory. Also, looping over all documents in the collection is unnecessary; we can change the algorithm to score only documents that appear in at least one of the inverted lists.
 e primary bene t of this method is its frugal use of memory.  e only major use of memory comes from the priority queue, which only needs to store k entries
168 5 RankingwithIndexes
at a time. However, in a realistic implementation, large portions of the inverted lists would also be buffered in memory during evaluation.
5.7.2 Term-at-a-time Evaluation
Figure 5.17 shows term-at-a-time retrieval, using the same query, scoring func- tion, and inverted list data as in the document-at-a-time example (Figure 5.15). Notice that the computed scores are exactly the same in both  gures, although the structure of each  gure is different.
salt
partial scores
old partial scores 1:1 4:1 water
new partial scores
old partial scores
tropical
final scores
Fig. 5.17. Term-at-a-time query evaluation
As before, the gray lines indicate the boundaries between each step. In the  rst step, the inverted list for “salt” is decoded, and partial scores are stored in accumu- lators.  ese scores are called partial scores because they are only a part of the  nal document score.  e accumulators, which get their name from their job, accumu- late score information for each document. In the second step, partial scores from the accumulators are combined with data from the inverted list for “water” to produce a new set of partial scores. A er the data from the list for “tropical” is added in the third step, the scoring process is complete.
 e  gure implies that a new set of accumulators is created for each list. Al- though this is one possible implementation technique, in practice accumulators are stored in a hash table.  e information for each document is updated as the
 1:1
 4:1
 1:1
 4:1
    1:1
 2:1
 4:1
 1:2
 2:1
 4:2
    1:2
2:1
4:2
   1:2
2:2
3:1
    1:4
2:3
3:1
4:2
5.7 QueryProcessing 169
inverted list data is processed.  e hash table contains the  nal document scores a er all inverted lists have been processed.
procedure TAATR(Q, I, f, g k) A ← HashTable()
L ← Array()
R ← PriorityQueue(k)
for all terms wi in Q do
li ←InvertedList(wi,I) L.add( li )
end for
for all lists li ∈ L do
while li is not  nished do
d ← li.getCurrentDocument() Ad ← Ad + gi(Q)f(li)
li .moveToNextDocument()
end while end for
for all accumulators Ad in A do sd ← Ad
R.add( sd, d ) end for
return the top k results from R end procedure
◃ Accumulator contains the document score
Fig. 5.18. A simple term-at-a-time retrieval algorithm
 e term-at-a-time retrieval algorithm for the abstract ranking model (Fig- ure 5.18) is similar to the document-at-a-time version at the start. It creates a pri- ority queue and fetches one inverted list for each term in the query, just like the document-at-a-time algorithm. However, the next step is different. Instead of a loop over each document in the index, the outer loop is over each list.  e inner loop then reads each posting of the list, computing the feature functions fi and gi and adding its weighted contribution to the accumulator Ad. A er the main loop completes, the accumulators are scanned and added to a priority queue, which de- termines the top k results to be returned.
 e primary disadvantage of the term-at-a-time algorithm is the memory us- age required by the accumulator table A. Remember that the document-at-a-time
170 5 RankingwithIndexes
strategy requires only the small priority queue R, which holds a limited number of results. However, the term-at-a-time algorithm makes up for this because of its more efficient disk access. Since it reads each inverted list from start to  nish, it requires minimal disk seeking, and it needs very little list buffering to achieve high speeds. In contrast, the document-at-a-time algorithm switches between lists and requires large list buffers to help reduce the cost of seeking.
In practice, neither the document-at-a-time nor term-at-a-time algorithms are used without additional optimizations.  ese optimizations dramatically im- prove the running speed of the algorithms, and can have a large effect on the mem- ory footprint.
5.7.3 Optimization Techniques
 ere are two main classes of optimizations for query processing.  e  rst is to read less data from the index, and the second is to process fewer documents.  e two are related, since it would be hard to score the same number of documents while reading less data. When using feature functions that are particularly com- plex, focusing on scoring fewer documents should be the main concern. For sim- ple feature functions, the best speed comes from ignoring as much of the inverted list data as possible.
List skipping
In section 5.4.7, we covered skip pointers in inverted lists.  is kind of forward skipping is by far the most popular way to ignore portions of inverted lists (Figure 5.19). More complex approaches (for example, tree structures) are also possible but not frequently used.
Fig. 5.19. Skip pointers in an inverted list.  e gray boxes show skip pointers, which point into the white boxes, which are inverted list postings.
Skip pointers do not improve the asymptotic running time of reading an in- verted list. Suppose we have an inverted list that is n bytes long, but we add skip pointers a er each c bytes, and the pointers are k bytes long. Reading the entire
                                 
5.7 QueryProcessing 171
list requires reading Θ(n) bytes, but jumping through the list using the skip point- ers requires Θ(kn/c) time, which is equivalent to Θ(n). Even though there is no asymptotic gain in runtime, the factor of c can be huge. For typical values of c = 100 and k = 4, skipping through a list results in reading just 2.5% of the total data.
Notice that as c gets bigger, the amount of data you need to read to skip through the list drops. So, why not make c as big as possible?  e problem is that if c gets too large, the average performance drops. Let’s look at this problem in more detail.
Suppose you want to  nd p particular postings in an inverted list, and the list is n bytes long, with k-byte skip pointers located at c-byte intervals.  erefore, there are n/c total intervals in the list. To  nd those p postings, we need to read kn/c bytes in skip pointers, but we also need to read data in p intervals. On average, we assume that the postings we want are about halfway between two skip pointers, so we read an additional pc/2 bytes to  nd those postings.  e total number of bytes read is then:
kn + pc c2
Notice that this analysis assumes that p is much smaller than n/c; that’s what allows us to assume that each posting lies in its own interval. As p grows closer to n/c, it becomes likely that some of the postings we want will lie in the same intervals. However, notice that once p gets close to n/c, we need to read almost all of the inverted list, so the skip pointers aren’t very helpful.
Coming back to the formula, you can see that while a larger value of c makes the  rst term smaller, it also makes the second term bigger.  erefore, picking the perfect value for c depends on the value of p, and we don’t know what p is until a query is executed. However, it is possible to use previous queries to simulate skipping behavior and to get a good estimate for c. In the exercises, you will be asked to plot some of graphs of this formula and to solve for the equilibrium point.
Although it might seem that list skipping could save on disk accesses, in prac- tice it rarely does. Modern disks are much better at reading sequential data than they are at skipping to random locations. Because of this, most disks require a skip of about 100,000 postings before any speedup is seen. Even so, skipping is still use- ful because it reduces the amount of time spent decoding compressed data that has been read from disk, and it dramatically reduces processing time for lists that are cached in memory.
  
172 5 RankingwithIndexes
Conjunctive processing
 e simplest kind of query optimization is conjunctive processing. By conjunctive processing, we just mean that every document returned to the user needs to con- tain all of the query terms. Conjunctive processing is the default mode for many web search engines, in part because of speed and in part because users have come to expect it. With short queries, conjunctive processing can actually improve effec- tiveness and efficiency simultaneously. In contrast, search engines that use longer queries, such as entire paragraphs, will not be good candidates for conjunctive processing.
Conjunctive processing works best when one of the query terms is rare, as in the query “ sh locomotion”.  e word “ sh” occurs about 100 times as o en as the word “locomotion”. Since we are only interested in documents that contain both words, the system can skip over most of the inverted list for “ sh” in order to  nd only the postings in documents that also contain the word “locomotion”.
Conjunctive processing can be employed with both term-at-a-time and docu- ment-at-a-time systems. Figure 5.20 shows the updated term-at-a-time algorithm for conjunctive processing. When processing the  rst term, (i = 0), processing proceeds normally. However, for the remaining terms, (i > 0), the algorithm processes postings starting at line ??. It checks the accumulator table for the next document that contains all of the previous query terms, and instructs list li to skip forward to that document if there is a posting for it (line ??). If there is a posting, the accumulator is updated. If the posting does not exist, the accumulator is deleted (line ??).
 e document-at-a-time version (Figure 5.21) is similar to the old document- at-a-time version, except in the inner loop. It begins by  nding the largest docu- ment d currently pointed to by an inverted list (line 13).  is document d is not guaranteed to contain all the query terms, but it is a reasonable candidate.  e next loop tries to skip all lists forward to point at d (line 16). If this is not success- ful, the loop terminates and another document d is chosen. If it is successful, the document is scored and added to the priority queue.
In both algorithms, the system runs fastest when the  rst list (l0) is the shortest and the last list (ln) is the longest.  is results in the biggest possible skip distances in the last list, which is where skipping will help most.
1: 2: 3: 4: 5: 6: 7: 8: 9:
10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35:
5.7 QueryProcessing 173
procedureTAATR(Q,I,f,g,k) A ← Map()
L ← Array()
R ← PriorityQueue(k) foralltermswiinQdo
li ←InvertedList(wi,I)
L.add( li ) end for
foralllistsli∈Ldo d0 ← −1
while li is not  nished do if i = 0 then
d ← li.getCurrentDocument() Ad ← Ad + gi(Q)f(li)
li .moveToNextDocument()
else
d ← li.getCurrentDocument()
d′ ← A.getNextAccumulator(d) A.removeAccumulatorsBetween(d0,d′) ifd=d′then
Ad ← Ad + gi(Q)f(li)
li .moveToNextDocument() else
li skipFor wardToDocument(d′ ) end if
d0←d′ end if
end while end for
for all accumulators Ad in A do sd ← Ad
R.add( sd, d ) end for
return the top k results from R endprocedure
◃ Accumulator contains the document score
Fig. 5.20. A term-at-a-time retrieval algorithm with conjunctive processing
174
1: 2: 3: 4: 5: 6: 7: 8: 9:
10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30:
5 RankingwithIndexes
procedureDAATR(Q,I,f,g,k) L ← Array()
R ← PriorityQueue(k) foralltermswiinQdo
li ←InvertedList(wi,I)
L.add( li ) end for
d←−1
while all lists in L are not  nished do
sd ← 0
for all inverted lists li in L do
if li.getCurrentDocument() > d then d ← li.getCurrentDocument()
end if end for
for all inverted lists li in L do
li .skipFor wardToDocument(d)
if li.getCurrentDocument() = d then
sd ← sd + gi(Q)fi(li)
li.movePastDocument( d ) else
d ← −1
break end if
end for
if d > −1thenR.add(sd,d) end if
end while
return the top k results from R endprocedure
◃ Update the document score
Fig. 5.21. A document-at-a-time retrieval algorithm with conjunctive processing Threshold methods
So far, the algorithms we have considered do not do much with the parameter k until the very last statement. Remember that k is the number of results requested by the user, and for many search applications this number is something small, such as 10 or 20. Because of this small value of k, most documents in the inverted lists
5.7 QueryProcessing 175
will never be shown to the user.  reshold methods focus on this k parameter in order to score fewer documents.
In particular, notice that for every query, there is some minimum score that each document needs to reach before it can be shown to the user.  is minimum score is the score of the kth-highest scoring document. Any document that does not score at least this highly will never be shown to the user. In this section, we will use the Greek letter tau (τ ) to represent this value, which we call the threshold.
If we could know the appropriate value for τ before processing the query, many query optimizations would be possible. For instance, since a document needs a score of at least τ in order to be useful to the user, we could avoid adding docu- ments to the priority queue (in the document-at-a-time case) that did not achieve a score of at least τ . In general, we could safely ignore any document with a score less than τ .
Unfortunately, we don’t know how to compute the true value of τ without evaluating the query, but we can approximate it.  ese approximations will be calledτ′.Wewantτ′ ≤τ,sothatwecansafelyignoreanydocumentwithascore lessthanτ′.Ofcourse,thecloserourestimateτ′ getstoτ,thefasterouralgorithm will run, since it can ignore more documents.
Comingupwithanestimateforτ′ iseasywithadocument-at-a-timestrategy. Remember that R maintains a list of the top k highest-scoring documents seen so far in the evaluation process. We can set τ ′ to the score of the lowest-scoring document currently in R, assuming R already has k documents in it. With term- at-a-time evaluation, we don’t have full document scores until the query evalua- tion is almost  nished. However, we can still set τ′ to be the kth-largest score in the accumulator table.
MaxScore
With reasonable estimates for τ′, it is possible to start ignoring some of the data intheinvertedlists. isestimate,τ′,representsalowerboundonthescoreadoc- ument needs in order to enter the  nal ranked list.  erefore, with a little bit of clever math, we can ignore parts of the inverted lists that will not generate docu- ment scores above τ′.
Let’s look more closely at how this might happen with a simple example. Con- sider the query “eucalyptus tree”.  e word “tree” is about 100 times more com- mon than the word “eucalyptus”, so we expect that most of the time we spend evaluating this query will be spent scoring documents that contain the word “tree”
176 5 RankingwithIndexes
and not “eucalyptus”.  is is a poor use of time, since we’re almost certain to  nd
a set of top k documents that contain both words. eucalyptus
tree
Fig. 5.22. MaxScore retrieval with the query “eucalyptus tree”.  e gray boxes indicate postings that can be safely ignored during scoring.
Figure 5.22 shows this effect in action. We see the inverted lists for “eucalyp- tus” and “tree” extending across the page, with the postings lined up by document, as in previous  gures in this chapter.  is  gure shows that there are many doc- uments that contain the word “tree” and don’t contain the word “eucalyptus”. Suppose that the indexer computed the largest partial score in the “tree” list, and that value is called μtree.  is is the maximum score (hence MaxScore) that any document that contains just this word could have.
Suppose that we are interested only in the top three documents in the ranked list (i.e., k is 3).  e  rst scored document contains just the word “tree”.  e next three documents contain both “eucalyptus” and “tree”. We will use τ ′ to represent the lowest score from these three documents. At this point, it is highly likely that τ′ > μtree, because τ′ is the score of a document that contains both query terms, whereas μtree is a query score for a document that contains just one of the query terms. isiswherethegrayboxescomeintothestory.Onceτ′ >μtree,wecan safely skip over all of the gray postings, since we have proven that these documents will not enter the  nal ranked list.
 e postings data in the  gure is fabricated, but for real inverted lists for “euca- lyptus” and “tree”, 99% of the postings for “tree” would be gray boxes, and there- fore would be safe to ignore.  is kind of skipping can dramatically reduce query times without affecting the quality of the query results.
Early termination
 e MaxScore approach guarantees that the result of query processing will be ex- actly the same in the optimized version as it is without optimization. In some cases, however, it may be interesting to take some risks with quality and process queries in a way that might lead to different results than the same queries in an unoptimized system.
                                          
5.7 QueryProcessing 177
Why might we choose to do this? One reason is that some queries are much, much more expensive than others. Consider the phrase query “to be or not to be”.  is query uses very common terms that would have very long inverted lists. Running this query to completion could severely reduce the amount of system resources available to serve other queries. Truncating query processing for this ex- pensive query can help ensure fairness for others using the same system.
Another reason is that MaxScore is necessarily conservative. It will not skip over regions of the inverted list that might have a usable candidate document. Because of this, MaxScore can spend a lot of time looking for a document that might not exist. Taking a calculated risk to ignore these improbable documents can pay off in decreased system resource consumption.
How might early termination work? In term-at-a-time systems, we can termi- nate processing by simply ignoring some of the very frequent query terms.  is is not so different from using a stopword list, except that in this case we would be ignoring words that usually would not be considered stopwords. Alternatively, we might decide that a er some constant number of postings have been read, no other terms will be considered.  e reasoning here is that, a er processing a sub- stantial number of postings, the ranking should be fairly well established. Reading more information will only change the rankings a little.  is is especially true for queries with many (e.g., hundreds) of terms, which can happen when query ex- pansion techniques are used.
In document-at-a-time systems, early termination means ignoring the docu- ments at the very end of the inverted lists.  is is a poor idea if the documents are sorted in random order, but this does not have to be the case. Instead, documents could be sorted in order by some quality metric, such as PageRank. Terminating early in that case would mean ignoring documents that are considered lower qual- ity than the documents that have already been scored.
List ordering
So far, all the examples in this chapter assume that the inverted lists are stored in the same order, by document number. If the document numbers are assigned randomly, this means that the document sort order is random.  e net effect is that the best documents for a query can easily be at the very end of the lists. With good documents scattered throughout the list, any reasonable query processing algorithm must read or skip through the whole list to make sure that no good documents are missed. Since these lists can be long, it makes sense to consider a more intelligent ordering.
178 5 RankingwithIndexes
One way to improve document ordering is to order documents based on doc- ument quality, as we discussed in the last section.  ere are plenty of quality met- rics that could be used, such as PageRank or the total number of user clicks. If the smallest document numbers are assigned to the highest-quality documents, it be- comes reasonable to consider stopping the search early if many good documents have been found.  e threshold techniques from the MaxScore section can be used here. If we know that documents in the lists are decreasing in quality, we can compute an upper bound on the scores of the documents remaining in the lists at every point during retrieval. When τ ′ rises above the highest possible remaining document score, retrieval can be stopped safely without harming effectiveness.
Another option is to order each list by partial score. For instance, for the “food” list, we could store documents that contain many instances of the word “food”  rst. In a web application, this may correspond to putting restaurant pages early in the inverted list. For a “dog” list, we could store pages about dogs (i.e., containing many instances of “dog”)  rst. Evaluating a query about food or dogs then becomes very easy. Other queries, however, can be more difficult. For ex- ample, how do we evaluate the query “dog food”?  e best way to do it is to use an accumulator table, as in term-at-a-time retrieval. However, instead of reading a whole list at once, we read just small pieces of each list. Once the accumulator table shows that many good documents have been found, we can stop looking. As you can imagine, retrieval works fastest with terms that are likely to appear together, such as “tortilla guacamole”. When the terms are not likely to appear together—for example, “dirt cheese”—it is likely to take much longer to  nd the top documents.
5.7.4 Structured Queries
In the query evaluation examples we have seen so far, our assumption is that each inverted list corresponds to a single feature, and that we add those features to- gether to create a  nal document score. Although this works in simple cases, we might want a more interesting kind of scoring function. For instance, in Figure 5.2 the query had plenty of interesting features, including a phrase (“tropical  sh”), a synonym (“chichlids”), and some non-topical features (e.g., incoming links).
One way to do this is to write specialized ranking code in the retrieval system that detects these extra features and uses inverted list data directly to compute scores, but in a way that is more complicated than just a linear combination of features.  is approach greatly increases the kinds of scoring that you can use, and is very efficient. Unfortunately, it isn’t very  exible.
5.7 QueryProcessing 179
Another option is to build a system that supports structured queries. Struc- tured queries are queries written in a query language, which allows you to change the features used in a query and the way those features are combined.  e query language is not used by normal users of the system. Instead, a query translator converts the user’s input into a structured query representation.  is translation process is where the intelligence of the system goes, including how to weight word features and what synonyms to use. Once this structured query has been created, it is passed to the retrieval system for execution.
You may already be familiar with this kind of model, since database systems work this way. Relational databases are controlled using Structured Query Lan- guage (SQL). Many important applications consist of a user interface and a struc- tured query generator, with the rest of the logic controlled by a database.  is sep- aration of the application logic from the database logic allows the database to be both highly optimized and highly general.
Galago contains a structured query processing system that is described in de- tail in Chapter 7.  is query language is also used in the exercises.
      #od:1
#combine
aquarium
#od:1
feature combinations
proximity expressions
     tropical
fish
list data
Fig. 5.23. Evaluation tree for the structured query #combine(#od:1(tropical fish) #od:1(aquarium fish) fish)
Figure 5.23 shows a tree representation of a structured query written in the Galago structured query language: #combine(#od:1(tropical fish) #od:1(aquarium fish) fish).
 is query indicates that the document score should be a combination of the scores from three subqueries.  e  rst query is #od:1(tropical fish). In the Galago query language, the #od:1 operator means that the terms inside it need to appear next to each other, in that order, in a matching document.  e same is true of
180 5 RankingwithIndexes
#od:1(aquarium fish).  e  nal query term is fish. Each of these subqueries acts as a document feature that is combined using the #combine operator.
 is query contains examples of the main types of structured query expres- sions. At the bottom of the tree, we have index terms.  ese are terms that corre- spond to inverted lists in the index. Above that level, we have proximity expres- sions.  ese expressions combine inverted lists to create more complex features, such as a feature for “ sh” occurring in a document title, or “tropical  sh” occur- ring as a phrase. At the top level, the feature data computed from the inverted lists is combined into a document score. At this level, the position information from the inverted lists is ignored.
Galago evaluates structured queries by making a tree of iterator objects that looks just like the tree shown in Figure 5.23. For instance, an iterator is created that returns the matching documents for #od:1(tropical fish).  e iterator  nds these matching documents by using data from inverted list iterators for “tropical” and “ sh”.  e #combine operator is an iterator of document scores, which uses iterators for #od:1(tropical fish), #od:1(aquarium fish), and fish. Once a tree of itera- tors like this is made, scoring documents is just a matter of using the root iterator to step through the documents.
5.7.5 Distributed Evaluation
A single modern machine can handle a surprising load, and is probably enough for most tasks. However, dealing with a large corpus or a large number of users may require using more than one machine.
 e general approach to using more than one machine is to send all queries to a director machine.  e director then sends messages to many index servers, which do some portion of the query processing.  e director then organizes the results of this process and returns them to the user.
 e easiest distribution strategy is called document distribution. In this strat- egy, each index server acts as a search engine for a small fraction of the total doc- ument collection.  e director sends a copy of the query to each of the index servers, each of which returns the top k results, including the document scores for these results.  ese results are merged into a single ranked list by the director, which then returns the results to the user.
Some ranking algorithms rely on collection statistics, such as the number of occurrences of a term in the collection or the number of documents containing a term.  ese statistics need to be shared among the index servers in order to pro- duce comparable scores that can be merged effectively. In very large clusters of
5.7 QueryProcessing 181
machines, the term statistics at the index server level can vary wildly. If each in- dex server uses only its own term statistics, the same document could receive very different kinds of scores, depending on which index server is used.
Another distribution method is called term distribution. In term distribution, a single index is built for the whole cluster of machines. Each inverted list in that index is then assigned to one index server. For instance, the word “dog” might be handled by the third server, while “cat” is handled by the   h server. For a system with n index servers and a k term query, the probability that all of the query terms would be on the same server is 1/nk−1. For a cluster of 10 machines, this probability is just 1% for a three-term query.  erefore, in most cases the data to process a query is not stored all on one machine.
One of the index servers, usually the one holding the longest inverted list, is chosen to process the query. If other index servers have relevant data, that data is sent over the network to the index server processing the query. When query processing is complete, the results are sent to a director machine.
 e term distribution approach is more complex than document distribution because of the need to send inverted list data between machines. Given the size of inverted lists, the messages involved in shipping this data can saturate a network. In addition, each query is processed using just one processor instead of many, which increases overall query latency versus document distribution.  e main ad- vantage of term distribution is seek time. If we have a k-term query and n index servers, the total number of disk seeks necessary to process a query is O(kn) for a document-distributed system, but just O(k) in a term-distributed system. For a system that is disk-bound, and especially one that is seek-bound, term distribu- tion might be attractive. However, recent research shows that term distribution is rarely worth the effort.
5.7.6 Caching
We saw in Chapter 4 how word frequencies in text follow a Zip an distribution: a few words occur very o en, but a huge number of words occur very infrequently. It turns out that query distributions are similar. Some queries, such as those about popular celebrities or current events, tend to be very popular with public search engines. However, about half of the queries that a search engine receives each day are unique.
 is leads us into a discussion of caching. Broadly speaking, caching means storing something you might want to use later. With search engines, we usually
182 5 RankingwithIndexes
want to cache ranked result lists for queries, but systems can also cache inverted lists from disk.
Caching is perfectly suited for search engines. Queries and ranked lists are small, meaning it doesn’t take much space in a cache to store them. By contrast, processing a query against a large corpus can be very computationally intensive.  is means that once a ranked list is computed, it usually makes sense to keep it around.
However, caching does not solve all of our performance problems, because about half of all queries received each day are unique.  erefore, the search en- gine itself must be built to handle query traffic very quickly.  is leads to com- petition for resources between the search engine and the caching system. Recent research suggests that when memory space is tight, caching should focus on the most popular queries, leaving plenty of room to cache the index. Unique queries with multiple terms may still share a term and use the same inverted list.  is explains why inverted list caching can have higher hit rates than query caching. Once the whole index is cached, all remaining resources can be directed toward caching query results.
When using caching systems, it is important to guard against stale data. Cach- ing works because we assume that query results will not change over time, but eventually they do. Cache entries need acceptable timeouts that allow for fresh results.  is is easier when dealing with partitioned indexes like the ones we dis- cussed in section 5.6.4. Each cache can be associated with a particular index par- tition, and when that partition is deleted, the cache can also be deleted. Keep in mind that a system that is built to handle a certain peak throughput with caching enabled will handle a much smaller throughput with caching off.  is means that if your system ever needs to destroy its cache, be prepared to have a slow system until the cache becomes suitably populated. If possible, cache  ushes should hap- pen at off-peak load times.
References and Further Reading
 is chapter contains information about many topics: indexing, query process- ing, compression, index update, caching, and distribution just to name a few. All these topics are in one chapter to highlight how these components work together.
Because of how interconnected these components are, it is useful to look at studies of real, working systems. Brin and Page (1998) wrote a paper about the early Google system that is an instructive overview of what it takes to build a fully
5.7 QueryProcessing 183
working system. Later papers show how the Google architecture has changed over time—for example Barroso et al. (2003).  e MapReduce paper, by Dean and Ghemawat (2008), gives more detail than this chapter does about how MapRe- duce was developed and how it works in practice.
 e inner workings of commercial search engines are o en considered trade secrets, so the exact details of how they work is not o en published. One im- portant exception is the TodoBR engine, a popular Brazilian web search engine. Before TodoBR was acquired by Google, their engineers frequently published papers about its workings. One example is their paper on a two-level caching scheme (Saraiva et al., 2001), but there are many others.
 e book Managing Gigabytes (Witten et al., 1999) is the standard reference for index construction, and is particularly detailed in its discussion of compres- sion techniques. Work on compression for inverted lists continues to be an active area of research. One of the recent highlights of this research is the PFOR se- ries of compressors from Zukowski et al. (2006), which exploit the performance characteristics of modern processors to make a scheme that is particularly fast for decompressing small integers. Büttcher and Clarke (2007) did a recent study on how compression schemes compare on the latest hardware.
Zobel and Moffat (2006) wrote a review article that outlines all of the impor- tant recent research in inverted indexes, both in index construction and in query processing.  is article is the best place to look for an understanding of how this research  ts together.
Turtle and Flood (1995) developed the MaxScore series of algorithms. Fagin et al. (2003) took a similar approach with score-sorted inputs, although they did not initially apply their ideas to information retrieval. Anh and Moffat (2006) re ned these ideas to make a particularly efficient retrieval system.
Anh and Moffat (2005) and Metzler et al. (2008) cover methods for comput- ing scores that can be stored in inverted lists. In particular, these papers describe how to compute scores that are both useful in retrieval and can be stored com- pactly in the list. Strohman (2007) explores the entire process of building scored indexes and processing queries efficiently with them.
Many of the algorithms from this chapter are based on merging two sorted in- puts; index construction relies on this, as does any kind of document-at-a-time retrieval process. Knuth wrote an entire volume on just sorting and searching, which includes large amounts of material on merging, including disk-based merg- ing (Knuth, 1998). If the Knuth book is too daunting, any standard algorithms textbook should be able to give you more detail about how merging works.
184 5 RankingwithIndexes
Lester et al. (2005) developed the geometric partitioning method for index update. Büttcher et al. (2006) added some extensions to this model, focusing on how very common terms should be handled during update. Strohman and Cro  (2006) show how to update the index without halting query processing.
Exercises
5.1. Section 5.2 introduced an abstract model of ranking, where documents and queries are represented by features. What are some advantages of representing documents and queries by features? What are some disadvantages?
5.2. Our model of ranking contains a ranking function R(Q, D), which com- pares each document with the query and computes a score.  ose scores are then used to determine the  nal ranked list.
An alternate ranking model might contain a different kind of ranking func- tion, f(A, B, Q), where A and B are two different documents in the collection and Q is the query. When A should be ranked higher than B, f(A, B, Q) eval- uates to 1. When A should be ranked below B, f (A, B, Q) evaluates to –1.
If you have a ranking function R(Q, D), show how you can use it in a system that requires one of the form f (A, B, Q). Why can you not go the other way (use f (A, B, Q) in a system that requires R(Q, D))?
5.3. Suppose you build a search engine that uses one hundred computers with a million documents stored on each one, so that you can search a collection of 100 million documents. Would you prefer a ranking function like R(Q, D) or one like f (A, B, Q) (from the previous problem). Why?
5.4. Suppose your search engine has just retrieved the top 50 documents from your collection based on scores from a ranking function R(Q, D). Your user in- terface can show only 10 results, but you can pick any of the top 50 documents to show. Why might you choose to show the user something other than the top 10 documents from the retrieved document set?
5.5. Documents can easily contain thousands of non-zero features. Why is it im- portant that queries have only a few non-zero features?
5.6. Indexes are not necessary to search documents. Your web browser, for in- stance, has a Find function in it that searches text without using an index. When should you use an inverted index to search text? What are some advantages to using an inverted index? What are some disadvantages?
5.7 QueryProcessing 185
5.7. Section 5.3 explains many different ways to store document information in inverted lists. What kind of inverted lists might you build if you needed a very small index? What kind would you build if you needed to  nd mentions of cities, such as Kansas City or São Paulo?
5.8. Write a program that can build a simple inverted index of a set of text docu- ments. Each inverted list will contain the  le names of the documents that contain that word.
Suppose the  le A contains the text “the quick brown fox”, and  le B contains “the slow blue fox”.  e output of your program would be:
   % ./your-program A B
   blue B
   brown A
   fox A B
   quick A
   slow B
   the A B
5.9. In section 5.4.1, we created an unambiguous compression scheme for 2-bit binary numbers. Find a sequence of numbers that takes up more space when it is “compressed” using our scheme than when it is “uncompressed.”
5.10. Suppose a company develops a new unambiguous lossless compression scheme for 2-bit numbers called SuperShrink. Its developers claim that it will re- duce the size of any sequence of 2-bit numbers by at least 1 bit. Prove that the developers are lying. More speci cally, prove that either:
• SuperShrink never uses less space than an uncompressed encoding, or
•  ere is an input to SuperShrink such that the compressed version is larger
than the uncompressed input
You can assume that each 2-bit input number is encoded separately.
5.11. Why do we need to know something about the kind of data we will com- press before choosing a compression algorithm? Focus speci cally on the result from Exercise 5.10.
5.12. Develop an encoder for the Elias-γ code. Verify that your program produces the same codes as in Table 5.2.
186 5 RankingwithIndexes
5.13. Identify the optimal skip distance k when performing a two-term Boolean AND query where one term occurs 1 million times and the other term appears 100 million times. Assume that a linear search will be used once an appropriate region is found to search in.
5.14. In section 5.7.3, we saw that the optimal skip distance c can be determined by minimizing the quantity kn/c + pc/2, where k is the skip pointer length, n is the total inverted list size, c is the skip interval, and p is the number of postings to  nd.
Plot this function using k = 4, n = 1,000,000, and p = 1,000, but varying c.  en, plot the same function, but set p = 10,000. Notice how the optimal value for c changes.
Finally, take the derivative of the function kn/c + pc/2 in terms of c to  nd the optimum value for c for a given set of other parameters (k, n, and p).
5.15. In Chapter 4, you learned about Zipf ’s law, and how approximately 50% of words found in a collection of documents will occur only once. Your job is to design a program that will verify Zipf ’s law using MapReduce.
Your program will output a list of number pairs, like this:
   195840,1
   70944,2
   34039,3
   ...
   1,333807
 is sample output indicates that 195,840 words appeared once in the collection, 70,944 appeared twice, and 34,039 appeared three times, but one word appeared 333,807 times. Your program will print this kind of list for a document collection.
Your program will use MapReduce twice (two Map phases and two Reduce phases) to produce this output.
5.16. Write the program described in Exercise 5.15 using the Galago search toolkit. Verify that it works by indexing the Wikipedia collection provided on the book website.
6
Queries and Interfaces
“ is is Information Retrieval, not Information Dispersal.”
Jack Lint, Brazil
6.1 Information Needs and Queries
Although the index structures and ranking algorithms are key components of a search engine, from the user’s point of view the search engine is primarily an in- terface for specifying queries and examining results. People can’t change the way the ranking algorithm works, but they can interact with the system during query formulation and reformulation, and while they are browsing the results.  ese in- teractions are a crucial part of the process of information retrieval, and can deter- mine whether the search engine is viewed as providing an effective service. In this chapter, we discuss techniques for query transformation and re nement, and for assembling and displaying the search results. We also discuss cross-language search engines here because they rely heavily on the transformation of queries and results.
In Chapter 1, we described an information need as the motivation for a person using a search engine.  ere are many types of information needs, and researchers have categorized them using dimensions such as the number of relevant docu- ments being sought, the type of information that is needed, and the tasks that led to the requirement for information. It has also been pointed out that in some cases it can be difficult for people to de ne exactly what their information need is, because that information is a gap in their knowledge.1 From the point of view of the search engine designer, there are two important consequences of these ob- servations about information needs:
• Queries can represent very different information needs and may require dif- ferent search techniques and ranking algorithms to produce the best rankings.
1  is is Belkin’s well-known Anomalous State of Knowledge (ASK) hypothesis (Belkin et al., 1982/1997).
  
188 6 QueriesandInterfaces
• A query can be a poor representation of the information need.  is can happen because the user  nds it difficult to express the information need. More o en, however, it happens because the user is encouraged to enter short queries, both by the search engine interface and by the fact that long queries o en fail.
 e  rst point is discussed further in Chapter 7.  e second point is a major theme in this chapter. We present techniques—such as spelling correction, query expan- sion, and relevance feedback—that are designed to re ne the query, either auto- matically or through user interaction.  e goal of this re nement process is to produce a query that is a better representation of the information need, and con- sequently to retrieve better documents. On the output side, the way that results are displayed is an important part of helping the user understand whether his in- formation need has been met. We discuss techniques such as snippet generation, result clustering, and document highlighting, that are designed to help this process of understanding the results.
Short queries consisting of a small number of keywords (between two and three on average in most studies of web search) are by far the most popular form of query currently used in search engines. Given that such short queries can be am- biguous and imprecise,2 why don’t people use longer queries?  ere are a number of reasons for this. In the past, query languages for search engines were designed to be used by expert users, or search intermediaries.  ey were called intermediaries because they acted as the interface between the person looking for information and the search engine.  ese query languages were quite complex. For example, here is a query made up by an intermediary for a search engine that provides legal information:
User query: Are there any cases that discuss negligent maintenance or fail- ure to maintain aids to navigation such as lights, buoys, or channel mark- ers?
Intermediary query: NEGLECT! FAIL! NEGLIG! /5 MAINT! REPAIR! /P NAV- IGAT! /5 AID EQUIP! LIGHT BUOY ”CHANNEL MARKER”
 is query language uses wildcard operators and various forms of proximity op- erators to specify the information need. A wildcard operator is used to de ne the minimum string match required for a word to match the query. For example, NE- GLECT! will match “neglected”, “neglects”, or just “neglect”. A proximity operator
2 Would you go up to a person and say, “Tropical  sh?”, or even worse, “Fish?”, if you wanted to ask what types of tropical  sh were easiest to care for?
 
6.1 InformationNeedsandQueries 189
is used to de ne constraints on the distance between words that are required for them to match the query. One type of proximity constraint is adjacency. For ex- ample, the quotes around ”CHANNEL MARKER” specify that the two words must occur next to each other.  e more general window operator speci es a width (in words) of a text window that is allowed for the match. For example, /5 speci es that the words must occur within  ve words of each other. Other typical prox- imity operators are sentence and paragraph proximity. For example, /P speci es that the words must occur in the same paragraph. In this query language, if no constraint is speci ed, it is assumed to be a Boolean OR.
Some of these query language operators are still available in search engine in- terfaces, such as using quotes for a phrase or a “+” to indicate a mandatory term, but in general there is an emphasis on simple keyword queries (sometimes called “natural language” queries) in order to make it possible for most people to do their own searches.3 But if we want to make querying as natural as possible, why not encourage people to type in better descriptions of what they are looking for instead of just a couple of keywords? Indeed, in applications where people expect other people to answer their questions, such as the community-based question answering systems described in section 10.3, the average query length goes up to around 30 words.  e problem is that current search technology does not do a good job with long queries. Most web search engines, for example, only rank doc- uments that contain all the query terms. If a person enters a query with 30 words in it, the most likely result is that nothing will be found. Even if documents con- taining all the words can be found, the subtle distinctions of language used in a long, grammatically correct query will o en be lost in the results. Search engines use ranking algorithms based primarily on a statistical view of text as a collection of words, not on syntactic and semantic features.
Given what happens to long queries, people have quickly learned that they will get the most reliable results by thinking of a few keywords that are likely to be as- sociated with the information they are looking for, and using these as the query.  is places quite a burden on the user, and the query re nement techniques de- scribed here are designed to reduce this burden and compensate for poor queries.
3 Note that the search engine may still be using a complex query language (such as that described in section 7.4.2) internally, but not in the interface.
 
190 6 QueriesandInterfaces
6.2 Query Transformation and Refinement
6.2.1 Stopping and Stemming Revisited
As mentioned in the last section, the most common form of query used in cur- rent search engines consists of a small number of keywords. Some of these queries use quotes to indicate a phrase, or a “+” to indicate that a word must be present, but for the remainder of this chapter we will make the simplifying assumption that the query is simply text.4  e initial stages of processing a text query should mirror the processing steps that are used for documents. Words in the query text must be transformed into the same terms that were produced by document texts, or there will be errors in the ranking.  is sounds obvious, but it has been a source of problems in a number of search projects. Despite this restriction, there is scope for some useful differences between query and document transformation, par- ticularly in stopping and stemming. Other steps, such as parsing the structure or tokenizing, will either not be needed (keyword queries have no structure) or will be essentially the same.
We mentioned in section 4.3.3 that stopword removal can be done at query time instead of during document indexing. Retaining the stopwords in the in- dexes increases the  exibility of the system to deal with queries that contain stop- words. Stopwords can be treated as normal words (by leaving them in the query), removed, or removed except under certain conditions (such as being used with quote or “+” operators).
Query-based stemming is another technique for increasing the  exibility of the search engine. If the words in documents are stemmed during indexing, the words in the queries must also be stemmed.  ere are circumstances, however, where stemming the query words will reduce the accuracy of the results.  e query “ sh village” will, for example, produce very different results from the query “ shing village”, but many stemming algorithms would reduce “ shing” to “ sh”. By not stemming during document indexing, we are able to make the decision at query time whether or not to stem “ shing”.  is decision could be based on a number of factors, such as whether the word was part of a quoted phrase.
For query-based stemming to work, we must expand the query using the ap- propriate word variants, rather than reducing the query word to a word stem.  is is because documents have not been stemmed. If the query word “ shing” was re-
4 Based on a recent sample of web queries, about 1.5% of queries used quotes, and less than 0.5% used a “+” operator.
 
6.2 QueryTransformationandRe nement 191
placed with the stem “ sh”, the query would no longer match documents that contained “ shing”. Instead, the query should be expanded to include the word “ sh”.  is expansion is done by the system (not the user) using some form of synonym operator, such as that described in section 7.4.2. Alternatively, we could index the documents using stems as well as words.  is will make query execution more efficient, but increases the size of the indexes.
Every stemming algorithm implicitly generates stem classes. A stem class is the group of words that will be transformed into the same stem by the stemming al- gorithm.  ey are created by simply running the stemming algorithm on a large collection of text and recording which words map to a given stem. Stem classes can be quite large. For example, here are three stem classes created with the Porter Stemmer on TREC news collections (the  rst entry in each list is the stem):
/bank banked banking bankings banks
/ocean oceaneering oceanic oceanics oceanization oceans /polic polical polically police policeable policed -policement policer policers polices policial
-policically policier policiers policies policing
-policization policize policly policy policying policys
 ese classes are not only long (the “polic” class has 22 entries), but they also contain a number of errors.  e words relating to “police” and “policy” should not be in the same class, and this will cause a loss in ranking accuracy. Other words are not errors, but may be used in different contexts. For example, “banked” is more o en used in discussions of  ying and pool, but this stem class will add words that are more common in  nancial discussions.  e length of the lists is an issue if the stem classes are used to expand the query. Adding 22 words to a simple query will certainly negatively impact response time and, if not done properly using a synonym operator, could cause the search to fail.
Both of these issues can be addressed using an analysis of word co-occurrence in the collection of text.  e assumption behind this analysis is that word variants that could be substitutes for each other should co-occur o en in documents. More speci cally, we do the following steps:
1. For all pairs of words in the stem classes, count how o en they co-occur in text windows of W words. W is typically in the range 50–100.
2. Compute a co-occurrence or association metric for each pair.  is measures how strong the association is between the words.
192 6 QueriesandInterfaces
3. Construct a graph where the vertices represent words and the edges are be- tween words whose co-occurrence metric is above a threshold T .
4. Find the connected components of this graph.  ese are the new stem classes.
 e term association measure used in TREC experiments was based on Dice’s coefficient.  is measure has been used since the earliest studies of term similarity and automatic thesaurus construction in the 1960s and 1970s. If na is the num- ber of windows (or documents) containing word a, nb is the number of windows containing word b, nab is the number of windows containing both words a and b, and N is the number of text windows in the collection, then Dice’s coefficient is de ned as 2 · nab/(na + nb).  is is simply the proportion of term occurrences that are co-occurrences.  ere are other possible association measures, which will be discussed later in section 6.2.3.
Two vertices are in the same connected component of a graph if there is a path between them. In the case of the graph representing word associations, the con- nected components will be clusters or groups of words, where each word has an association above the threshold T with at least one other member of the cluster.  e parameter T is set empirically. We will discuss this and other clustering tech- niques in section 9.2.
Applying this technique to the three example stem classes, and using TREC data to do the co-occurrence analysis, results in the following connected compo- nents:
/policies policy
/police policed policing /bank banking banks
 e new stem classes are smaller, and the inappropriate groupings (e.g., pol- icy/police) have been split up. In general, experiments show that this technique produces good ranking effectiveness with a moderate level of query expansion.
What about the “ shing village” query?  e relevant stem class produced by the co-occurrence analysis is
/fish fished fishing
which means that we have not solved that problem. As mentioned before, the query context determines whether stemming is appropriate. It would be reason- able to expand the query “ shing in Alaska” with the words “ sh” and “ shed”, but not the query “ shing village”.  e co-occurrence analysis described earlier
6.2 QueryTransformationandRe nement 193
uses context in a general way, but not at the level of co-occurrence with speci c query words.
With the recent availability of large query logs in applications such as web search, the concept of validating or even generating stem classes through statistical analysis can be extended to these resources. In this case, the analysis would look for word variants that tended to co-occur with the same words in queries.  is could be a solution to the  sh/ shing problem, in that “ sh” is unlikely to co-occur with “village” in queries.
Comparing this stemming technique to those described in section 4.3.4, it could be described as a dictionary-based approach, where the dictionary is gen- erated automatically based on input from an algorithmic stemmer (i.e., the stem classes).  is technique can also be used for stemming with languages that do not have algorithmic stemmers available. In that case, the stem classes are based on very simple criteria, such as grouping all words that have similar n-grams. A simple example would be to generate classes from words that have the same  rst three characters.  ese initial classes are much larger than those generated by an algorithmic stemmer, but the co-occurrence analysis reduces the  nal classes to similar sizes. Retrieval experiments con rm that typically there is little difference in ranking effectiveness between an algorithmic stemmer and a stemmer based on n-gram classes.
6.2.2 Spell Checking and Suggestions
Spell checking is an extremely important part of query processing. Approximately 10–15% of queries submitted to web search engines contain spelling errors, and people have come to rely on the “Did you mean: ...” feature to correct these er- rors. Query logs contain plenty of examples of simple errors such as the following (taken from a recent sample of web queries):
poiner sisters brimingham news catamarn sailing
hair extenssions marshmellow world miniture golf courses psyhics
home doceration
194 6 QueriesandInterfaces
 ese errors are similar to those that may be found in a word processing docu- ment. In addition, however, there will be many queries containing words related to websites, products, companies, and people that are unlikely to be found in any standard spelling dictionary. Some examples from the same query log are:
realstateisting.bc.com akia 1080i manunal ultimatwarcade mainscourcebank dellottitouche
 e wide variety in the type and severity of possible spelling errors in queries presents a signi cant challenge. In order to discuss which spelling correction tech- niques are the most effective for search engine queries, we  rst have to review how spelling correction is done for general text.
 e basic approach used in many spelling checkers is to suggest corrections for words that are not found in the spelling dictionary. Suggestions are found by comparing the word that was not found in the dictionary to words that are in the dictionary using a similarity measure.  e most common measure for compar- ing words (or more generally, strings) is the edit distance, which is the number of operations required to transform one of the words into the other.  e Damerau- Levenshtein distance metric counts the minimum number of insertions, deletions, substitutions, or transpositions of single characters required to do the transfor- mation.5 Studies have shown that 80% or more of spelling errors are caused by an instance of one of these types of single-character errors.
As an example, the following transformations (shown with the type of error involved) all have Damerau-Levenshtein distance 1 since only a single operation or edit is required to produce the correct word:
extenssions → extensions (insertion error)
poiner → pointer (deletion error)
marshmellow → marshmallow (substitution error) brimingham → birmingham (transposition error)
 e transformation doceration → decoration, on the other hand, has edit dis- tance 2 since it requires two edit operations:
5  e Levenshtein distance is similar but does not include transposition as a basic opera- tion.
 
doceration → deceration deceration → decoration
6.2 QueryTransformationandRe nement 195
A variety of techniques and data structures have been used to speed up the calculation of edit distances between the misspelled word and the words in the dictionary.  ese include restricting the comparison to words that start with the same letter (since spelling errors rarely change the  rst letter), words that are of the same or similar length (since spelling errors rarely change the length of the word), and words that sound the same.6 In the latter case, phonetic rules are used to map words to codes. Words with the same codes are considered as possible corrections.  e Soundex code is a simple type of phonetic encoding that was originally used for the problem of matching names in medical records.  e rules for this encoding are:
1. Keep the  rst letter (in uppercase).
2. Replace these letters with hyphens: a, e, i, o, u, y, h, w. 3. Replace the other letters by numbers as follows:
1:b,f,p,v 2:c,g,j,k,q,s,x,z 3:d,t
4:l
5:m,n
6:r
4. Delete adjacent repeats of a number.
5. Delete the hyphens.
6. Keep the  rst three numbers or pad out with zeros.
Some examples of this code are:
extenssions → E235; extensions → E235 marshmellow → M625; marshmallow → M625 brimingham → B655; birmingham → B655 poiner → P560; pointer → P536
 e last example shows that the correct word may not always have the same Soundex code. More elaborate phonetic encodings have been developed speci - cally for spelling correction (e.g., the GNU Aspell checker7 uses a phonetic code).
6 A word that is pronounced the same as another word but differs in meaning is called a homophone.
7 http://aspell.net/
 
196 6 QueriesandInterfaces
 ese encodings can be designed so that the edit distance for the codes can be used to narrow the search for corrections.
A given spelling error may have many possible corrections. For example, the spelling error “lawers” has the following possible corrections (among others) at edit distance 1: lawers → lowers, lawyers, layers, lasers, lagers.  e spelling correc- tor has to decide whether to present all of these to the user, and in what order to present them. A typical policy would be to present them in decreasing order of their frequency in the language. Note that this process ignores the context of the spelling error. For example, if the error occurred in the query “trial lawers”, this would have no impact on the presentation order of the suggested corrections.  e lack of context in the spelling correction process also means that errors involv- ing valid words will be missed. For example, the query “miniature golf curses” is clearly an example of a single-character deletion error, but this error has produced the valid word “curses” and so would not be detected.
 e typical interface for the “Did you mean:...” feature requires the spell check- er to produce the single best suggestion.  is means that ranking the suggestions using context and frequency information is very important for query spell check- ing compared to spell checking in a word processor, where suggestions can be made available on a pull-down list. In addition, queries contain a large number of run-on errors, where word boundaries are skipped or mistyped.  e two queries “ultimatwarcade” and “mainscourcebank” are examples of run-on errors that also contain single-character errors. With the appropriate framework, leaving out a separator such as a blank can be treated as just another class of single-character error.
 e noisy channel model for spelling correction is a general framework that can address the issues of ranking, context, and run-on errors.  e model is called a “noisy channel” because it is based on Shannon’s theory of communication (Shannon & Weaver, 1963).  e intuition is that a person chooses a word w to output (i.e., write), based on a probability distribution P (w).  e person then tries to write the word w, but the noisy channel (presumably the person’s brain) causes the person to write the word e instead, with probability P (e|w).
 e probabilities P (w), called the language model, capture information about the frequency of occurrence of a word in text (e.g., what is the probability of the word “lawyer” occurring in a document or query?), and contextual information such as the probability of observing a word given that another word has just been observed (e.g., what is the probability of “lawyer” following the word “trial”?). We
6.2 QueryTransformationandRe nement 197
will have more to say about language models in Chapter 7, but for now we can just assume it is a description of word occurrences in terms of probabilities.
 e probabilities P (e|w), called the error model, represent information about the frequency of different types of spelling errors.  e probabilities for words (or strings) that are edit distance 1 away from the word w will be quite high, for ex- ample. Words with higher edit distances will generally have lower probabilities, although homophones will have high probabilities. Note that the error model will have probabilities for writing the correct word (P (w|w)) as well as probabilities for spelling errors.  is enables the spelling corrector to suggest a correction for all words, even if the original word was correctly spelled. If the highest-probability correction is the same word, then no correction is suggested to the user. If, how- ever, the context (i.e., the language model) suggests that another word may be more appropriate, then it can be suggested.  is, in broad terms, is how a spelling corrector can suggest “course” instead of “curse” for the query “golf curse”.
So how do we estimate the probability of a correction? What the person writes is the word e, so we need to calculate P (w|e), which is the probability that the correct word is w given that we can see the person wrote e. If we are interested in  nding the correction with the maximum value of this probability, or if we just want to rank the corrections, it turns out we can use P (e|w)P (w), which is the product of the error model probability and the language model probability.8
To handle run-on errors and context, the language model needs to have in- formation about pairs of words in addition to single words.  e language model probability for a word is then calculated as a mixture of the probability that the word occurs in text and the probability that it occurs following the previous word, or
λP(w) + (1 − λ)P(w|wp)
where λ is a parameter that speci es the relative importance of the two probabili- ties, and P (w|wp) is the probability of a word w following the previous word wp. As an example, consider the spelling error “ sh tink”. To rank the possible correc- tions for “tink”, we multiply the error model probabilities for possible corrections by the language model probabilities for those corrections.  e words “tank” and “think” will both have high error-model probabilities since they require only a sin- gle character correction. In addition, both words will have similar probabilities for P (w) since both are quite common.  e probability P (tank|f ish), however,
8 Bayes’ Rule, which is discussed in Chapter 7, is used to express P (w|e) in terms of the component probabilities.
 
198 6 QueriesandInterfaces
will be much higher than P (think|f ish), and this will result in “tank” being a more likely correction than “think”.
Where does the information for the language model probabilities come from? In many applications, the best source for statistics about word occurrence in text will be the collection of documents that is being searched. In the case of web search (and some other applications), there will also be a query log containing millions of queries submitted to the search engine. Since our task is to correct the spelling of queries, the query log is likely to be the best source of information. It also reduces the number of pairs of words that need to be recorded in the language model, compared to analyzing all possible pairs in a large collection of documents. In addition to these sources, if a trusted dictionary is available for the application, it should be used.
 e estimation of the P (e|w) probabilities in the error model can be relatively simple or quite complex.  e simple approach is to assume that all errors with the same edit distance have equal probability. Additionally, only strings within a cer- tain edit distance (usually 1 or 2) are considered. More sophisticated approaches have been suggested that base the probability estimates on the likelihood of mak- ing certain types of errors, such as typing an “a” when the intention was to type an “e” .  ese estimates are derived from large collections of text by  nding many pairs of correctly and incorrectly spelled words.
Cucerzan and Brill (2004) describe an iterative process for spell checking queries using information from a query log and dictionary.  e steps, in simpli ed form, are as follows:
1. Tokenize the query.
2. For each token, a set of alternative words and pairs of words is found using
an edit distance modi ed by weighting certain types of errors, as described earlier.  e data structure that is searched for the alternatives contains words and pairs from both the query log and the trusted dictionary.
3.  e noisy channel model is then used to select the best correction.
4.  e process of looking for alternatives and  nding the best correction is re-
peated until no better correction is found.
By having multiple iterations, the spelling corrector can potentially make sugges- tions that are quite far (in terms of edit distance) from the original query. As an example, given the query “miniture golfcurses”, the spelling corrector would go through the following iterations:
miniture golfcurses miniature golfcourses miniature golf courses
6.2 QueryTransformationandRe nement 199
Experiments with this spelling corrector show that the language model from the query log is the most important component in terms of correction accuracy. In addition, using context in the form of word pairs in the language model is crit- ical. Having at least two iterations in the correction process also makes a signif- icant difference.  e error model was less important, however, and using a sim- ple model where all errors have the same probability was nearly as effective as the more sophisticated model. Other studies have shown that the error model is more important when the language model is based just on the collection of documents, rather than the query log.
 e best approach to building a spelling corrector for queries in a search ap- plication obviously depends on the data available. If a large amount of query log information is available, then this should be incorporated. Otherwise, the sources available will be the collection of documents for the application and, in some cases, a trusted dictionary. One approach would be to use a general-purpose spelling corrector, such as Aspell, and create an application-speci c dictionary. Building a spelling corrector based on the noisy channel model, however, is likely to be more effective and more adaptable, even if query log data is not available.
6.2.3 Query Expansion
In the early development of search engines, starting in the 1960s, an online the- saurus was considered an essential tool for the users of the system.  e thesaurus described the indexing vocabulary that had been used for the document collec- tion, and included information about synonyms and related words or phrases.  is was particularly important because the collection had typically been manu- ally indexed (tagged)9 using the terms in the thesaurus. Because the terms in the thesaurus were carefully chosen and subject to quality control, the thesaurus was also referred to as a controlled vocabulary. Using the thesaurus, users could deter- mine what words and phrases could be used in queries, and could expand an ini- tial query using synonyms and related words. Table 6.1 shows part of an entry in
9 In information retrieval, indexing is o en used to refer to the process of representing a document using an index term, in addition to the process of creating the indexes to search the collection. More recently, the process of manual indexing has been called tagging, particularly in the context of social search applications (see Chapter 10).
 
200 6 QueriesandInterfaces
the Medical Subject (MeSH) Headings thesaurus that is used in the National Li- brary of Medicine search applications.10  e “tree number” entries indicate, using a numbering scheme, where this term is found in the tree of broader and narrow terms. An “entry term” is a synonym or related phrase for the term.
   MeSH Heading
 Neck Pain
  Tree Number
 C10.597.617.576
  Tree Number
 C23.888.592.612.553
  Tree Number
 C23.888.646.501
  EntryTerm
 Cervical Pain
  EntryTerm
 Neckache
  EntryTerm
 Anterior Cervical Pain
  EntryTerm
 Anterior Neck Pain
  EntryTerm
 Cervicalgia
  EntryTerm
 Cervicodynia
  EntryTerm
 Neck Ache
  EntryTerm
 Posterior Cervical Pain
  EntryTerm
 Posterior Neck Pain
             Table 6.1. Partial entry for the Medical Subject (MeSH) Heading “Neck Pain”
Although the use of an explicit thesaurus is less common in current search ap- plications, a number of techniques have been proposed for automatic and semi- automatic query expansion. A semi-automatic technique requires user interac- tion, such as selecting the expansion terms from a list suggested by the expansion technique.Websearchengines,forexample,providequerysuggestionstotheuser in the form of the original query words expanded with one or more additional words, or replaced with alternative words.
Query expansion techniques are usually based on an analysis of word or term co-occurrence, in either the entire document collection, a large collection of queries, or the top-ranked documents in a result list. From this perspective, query- based stemming can also be regarded as a query expansion technique, with the expansion terms limited to word variants. Automatic expansion techniques that use a general thesaurus, such as Wordnet,11 have not been shown to be effective.
10 http://www.nlm.nih.gov/mesh/meshhome.html 11 http://wordnet.princeton.edu/
 
6.2 QueryTransformationandRe nement 201
 e key to effective expansion is to choose words that are appropriate for the context, or topic, of the query. For example, “aquarium” may be a good expan- sion term for “tank” in the query “tropical  sh tanks”, but not appropriate for the query “armor for tanks”. A general thesaurus lists related terms for many differ- ent contexts, which is why it is difficult to use automatically.  e techniques we will describe use a variety of approaches to address this problem, such as using all the words in a query to  nd related words rather than expanding each word sepa- rately. One well-known expansion technique, called pseudo-relevance feedback, is discussed in the next section along with techniques that are based on user feed- back about the relevance of documents in the results list.
Termassociationmeasuresareanimportantpartofmanyapproachestoquery expansion, and consequently a number of alternatives have been suggested. One of these, Dice’s coefficient, was mentioned in section 6.2.1.  e formula for this measure is
2 · nab rank nab n+n =n+n
rank
where = meansthattheformulaisrankequivalent(producesthesameranking of terms).12
Another measure, mutual information, has been used in a number of studies of word collocation. For two words (or terms) a and b, it is de ned as
log P (a, b) P (a)P (b)
and measures the extent to which the words occur independently.13 P (a) is the probability that word a occurs in a text window of a given size, P (b) is the prob- ability that word b occurs in a text window, and P (a, b) is the probability that a and b occur in the same text window. If the occurrences of the words are in- dependent, P (a, b) = P (a)P (b) and the mutual information will be 0. As an example, we might expect that the two words “ shing” and “automobile” would occur relatively independently of one another. If two words tend to co-occur, for
12 More formally, two functions are de ned to be rank equivalent if they produce the same ordering of items when sorted according to function value. Monotonic trans- forms (such as log), scaling (multiplying by a constant), and translation (adding a con- stant) are all examples of rank-preserving operations.
13  is is actually the pointwise mutual information measure, just to be completely accu- rate.
  abab
  
202 6 QueriesandInterfaces
example “ shing” and “boat”, P (a, b) will be greater than P (a)P (b) and the mu- tual information will be higher.
To calculate mutual information, we use the following simple normalized fre- quency estimates for the probabilities: P(a) = na/N, P(b) = nb/N, and P (a, b) = nab /N , where na is the number of windows (or documents) contain- ing word a, nb is the number of windows containing word b, nab is the number of windows containing both words a and b, and N is the number of text windows in the collection.  is gives the formula:
P (a, b) nab rank nab log P(a)P(b) = logN.n .n = n .n
   A problem that has been observed with this measure is that it tends to favor low-frequency terms. For example, consider two words with frequency 10 (i.e., na =nb =10)thatco-occurhalfthetime(nab =5). eassociationmeasure for these two terms is 5×10−2. For two terms with frequency 1,000 that co-occur half the time (nab = 500), the association measure is 5 × 10−4.  e expected mutual information measure addresses this problem by weighting the mutual in- formation value using the probability P (a, b). Although the expected mutual in- formation in general is calculated over all combinations of the events of word oc- currence and non-occurrence, we are primarily interested in the case where both terms occur, giving the formula:
P(a,b) nab nab rank nab P(a,b).log P(a)P(b) = N log(N.n .n ) = nab.log(N.n .n )
ab ab
If we take the same example as before and assume N = 106, this gives an as- sociation measure of 23.5 for the low-frequency terms, and 1,350 for the high- frequency terms, clearly favoring the latter case. In fact, the bias toward high- frequency terms can be a problem for this measure.
Another popular association measure that has been used in a variety of applica- tions is Pearson’s Chi-squared (χ2) measure.  is measure compares the number of co-occurrences of two words with the expected number of co-occurrences if the two words were independent, and normalizes this comparison by the expected number. Using our estimates, this gives the formula:
ab ab
    (nab − N.na .nb )2 rank (nab − 1 .na.nb)2 NN=N
     N.na .nb na.nb NN
  
6.2 QueryTransformationandRe nement 203  e term N.P(a).P(b) = N.na .nb is the expected number of co-occurrences
NN
if the two terms occur independently.  e χ2 test is usually calculated over all combinations of the events of word occurrence and non-occurrence, similar to the expected mutual information measure, but our focus is on the case where both terms co-occur. In fact, when N is large, this restricted form of χ2 produces the same term rankings as the full form. It should also be noted that χ2 is very similar to the mutual information measure and may be expected to favor low-frequency terms.
Table 6.2 summarizes the association measures we have discussed. To see how they work in practice, we have calculated the top-ranked terms in a TREC news collection using each measure for the words in the sample query “tropical  sh”.
    Measure
Mutual information (MIM)
Expected Mutual Information (EMIM)
Chi-square (χ2 ) Dice’s coefficient (Dice)
Formula
nab na .nb
nab. log(N. nab ) na .nb
(nab− 1 .na.nb)2 N
na .nb nab
na +nb
              Table 6.2. Term association measures
Table 6.3 shows the strongly associated words for “tropical” assuming an un- limited window size (in other words, co-occurrences are counted at the document level).  ere are two obvious features to note.  e  rst is that the ranking for χ2 is identical to the one for MIM.  e second is that MIM and χ2 favor low- frequency words, as expected.  ese words are not unreasonable (“itto”, for ex- ample, is the International Tropical Timber Organization, and “xishuangbanna” is a Chinese tropical botanic garden), but they are so specialized that they are un- likely to be much use for many queries.  e top terms for the EMIM and Dice measures are much more general and, in the case of EMIM, sometimes too gen- eral (e.g., “most”).
Table 6.4 shows the top-ranked words for “ sh”. Note that because this is a higher-frequency term, the rankings for MIM and χ2 are no longer identical, al- though both still favor low-frequency terms.  e top-ranked words for EMIM
204 6 QueriesandInterfaces
MIM
trmm
itto ortuno kuroshio ivirgarzama biofunction kapiolani bstilla almagreb jackfruit adeo xishuangbanna frangipani yuca anthurium
Dice
forest
exotic timber rain banana deforestation plantation coconut jungle tree rainforest palm hardwood greenhouse logging
   EMIM
 χ2
  forest tree rain island like  sh most water fruit area world america some live plant
                             trmm
itto ortuno kuroshio ivirgarzama biofunction kapiolani bstilla almagreb jackfruit adeo xishuangbanna frangipani yuca anthurium
                Table 6.3. Most strongly associated words for “tropical” in a collection of TREC news stories. Co-occurrence counts are measured at the document level.
and Dice are quite similar, although in different ordering. To show the effect of changing the window size, Table 6.5 gives the top-ranked words found using a window of  ve words.  e small window size has an effect on the results, although both MIM and χ2 still  nd low-frequency terms.  e words for EMIM are some- what improved, being more speci c. Overall, it appears that the simple Dice’s co- efficient is the most stable and reliable over a range of window sizes.
 e most signi cant feature of these tables, however, is that even the best rank- ings contain virtually nothing that would be useful to expand the query “tropical  sh”! Instead, the words are associated with other contexts, such as tropical forests and fruits, or  shing conservation. One way to address this problem would be to  nd words that are strongly associated with the phrase “tropical  sh”. Using Dice’s coefficient with the same collection of TREC documents as the previous tables, this produces the following 10 words at the top of the ranking:
gold sh, reptile, aquarium, coral, frog, exotic, stripe, regent, pet, wet
Clearly, this is doing much better at  nding words associated with the right con- text. To use this technique, however, we would have to  nd associations for every group of words that could be used in a query.  is is obviously impractical, but
   EMIM
 χ2
  water species wildlife  shery sea  sherman boat area habitat vessel marine land river food endanger
                             arlsq happyman outerlimit sportk lingcod long n bontadelli sport sher bill sh needle sh damaliscu bontebok taucher orangemouth sheepshead
              MIM
zoologico zapanta wrint wpfmc weighout waterdog long n veracruzana ungutt ulocentra needle sh tunaboat tsolwana olivacea motoroller
6.2 QueryTransformationandRe nement 205
Dice
species wildlife  shery water  sherman boat
sea habitat vessel marine endanger conservation river catch island
  Table 6.4. Most strongly associated words for “ sh” in a collection of TREC news stories. Co-occurrence counts are measured at the document level.
   EMIM
 χ2
  wildlife vessel boat  shery species tuna trout  sherman salmon catch nmf trawl halibut meat shell sh
                             ge lte mbmo zapanta plar hapc odfw southpoint anadromous taiffe mollie frampton idfg billingsgate sealord longline
              MIM
zapanta plar mbmo ge lte hapc odfw southpoint anadromous taiffe mollie frampton idfg billingsgate sealord longline
Dice
wildlife vessel boat  shery species catch water sea meat interior  sherman game salmon tuna caught
  Table 6.5. Most strongly associated words for “ sh” in a collection of TREC news stories. Co-occurrence counts are measured in windows of  ve words.
206 6 QueriesandInterfaces
there are other approaches that accomplish the same thing. One alternative would be to analyze the word occurrences in the retrieved documents for a query.  is is the basis of pseudo-relevance feedback, which is discussed in the next section. Another approach that has been suggested is to index every word in the collection by the words that co-occur with it, creating a virtual document14 representing that word. For example, the following list is the top 35 most strongly associated words for “aquarium” (using Dice’s coefficient):
zoology, cranmore, jouett, zoo, gold sh,  sh, cannery, urchin, reptile, coral, animal, mollusk, marine, underwater, plankton, mussel, oceanog- raphy, mammal, species, exhibit, swim, biologist, cabrillo, saltwater, crea- ture, reef, whale, oceanic, scuba, kelp, invertebrate, park, crustacean, wild, tropical
 ese words would form the index terms for the document representing “aquar- ium”. To  nd expansion words for a query, these virtual documents are ranked in the same way as regular documents, giving a ranking for the corresponding words. In our example, the document for “aquarium” contains the words “tropical” and “ sh” with high weights, so it is likely that it would be highly ranked for the query “tropical  sh”.  is means that “aquarium” would be a highly ranked expansion term. edocumentforawordsuchas“jungle”,ontheotherhand,wouldcontain “tropical” with a high weight but is unlikely to contain “ sh”.  is document, and the corresponding word, would be much further down the ranking than “aquar- ium”.
All of the techniques that rely on analyzing the document collection face both computational and accuracy challenges due to the huge size and variability in quality of the collections in search applications. At the start of this section, it was mentioned that instead of analyzing the document collection, either the result list or a large collection of queries could be used. Recent studies and experience indicate that a large query log is probably the best resource for query expansion. Not only do these logs contain many short pieces of text that are are easier to an- alyze than full text documents, they also contain other data, such as information on which documents were clicked on during the search (i.e., clickthrough data).
As an example of how the query log can be used for expansion, the following list shows the 10 most frequent words associated with queries that contain “trop- ical  sh” in a recent query log sample from a popular web search engine:
14 Sometimes called a context vector.
 
6.2 QueryTransformationandRe nement 207
stores, pictures, live, sale, types, clipart, blue, freshwater, aquarium, sup- plies
 ese words indicate the types of queries that people tend to submit about trop- ical  sh (sales, supplies, pictures), and most would be good words to suggest for query expansion. In current systems, suggestions are usually made in the form of whole queries rather than expansion words, and here again the query log will be extremely useful in producing the best suggestions. For example, “tropical  sh supplies” will be a much more common query than “supplies tropical  sh” and would be a better suggestion for this expansion.
From this perspective, query expansion can be reformulated as a problem of  nding similar queries, rather than expansion terms. Similar queries may not al- ways contain the same words. For example, the query “pet  sh sales” may be a reasonable suggestion as an alternative to “tropical  sh”, even though it doesn’t contain the word “tropical”. It has long been recognized that semantically similar queries can be found by grouping them based on the relevant documents they have in common, rather than just the words. Clickthrough data is very similar to rele- vance data, and recent studies have shown that queries can be successfully grouped or clustered based on the similarity of their clickthrough data.  is means that ev- ery query is represented using the set of pages that are clicked on for that query, and the similarity between the queries is calculated using a measure such as Dice’s coefficient, except that in this case nab will be the number of clicked-on pages the two queries have in common, and na, nb are the number of pages clicked on for each query.
In summary, both automatic and semi-automatic query expansion methods have been proposed, although the default in many search applications is to sug- gest alternative queries to the user. Some term association measures are better than others, but term association based on single words does not produce good expan- sion terms, because it does not capture the context of the query.  e best way to capture query context is to use a query log, both to analyze word associations and to  nd similar queries based on clickthrough data. If there is no query log avail- able, the best alternative would be to use pseudo-relevance feedback, as described in the next section. Of the methods described for constructing an automatic the- saurus based on the document collection, the best alternative is to create virtual documents for each word and rank them for each query.
208 6 QueriesandInterfaces
6.2.4 Relevance Feedback
Relevance feedback is a query expansion and re nement technique with a long history. First proposed in the 1960s, it relies on user interaction to identify rel- evant documents in a ranking based on the initial query. Other semi-automatic techniques were discussed in the last section, but instead of choosing from lists of terms or alternative queries, in relevance feedback the user indicates which docu- ments are interesting (i.e., relevant) and possibly which documents are completely off-topic (i.e., non-relevant). Based on this information, the system automatically reformulates the query by adding terms and reweighting the original terms, and a new ranking is generated using this modi ed query.
 is process is a simple example of using machine learning in information retrieval, where training data (the identi ed relevant and non-relevant docu- ments) is used to improve the system’s performance. Modifying the query is in fact equivalent to learning a classi er that distinguishes between relevant and non- relevant documents. We discuss classi cation and classi cation techniques fur- ther in Chapters 7 and 9. Relative to many other applications of machine learning, however, the amount of training data generated in relevance feedback is extremely limited since it is based on the user’s input for this query session only, and not on historical data such as clickthrough.
 e speci c method for modifying the query depends on the underlying re- trieval model. In the next chapter, we describe how relevance feedback works in the vector space model and the probabilistic model. In general, however, words that occur more frequently in the relevant documents than in the non-relevant documents, or in the collection as a whole, are added to the query or increased in weight.  e same general idea is used in the technique of pseudo-relevance feed- back, where instead of asking the user to identify relevant documents, the system simply assumes that the top-ranked documents are relevant. Words that occur fre- quently in these documents may then be used to expand the initial query. Once again, the speci cs of how this is done depend on the underlying retrieval model. Wedescribepseudo-relevancefeedbackbasedonthelanguagemodelapproachto retrieval in the next chapter.  e expansion terms generated by pseudo-relevance feedback will depend on the whole query, since they are extracted from docu- ments ranked highly for that query, but the quality of the expansion will be de- termined by how many of the top-ranked documents in the initial ranking are in fact relevant.
As a simple example of how this process works, consider the ranking shown in Figure 6.1, which was generated using a popular search engine with the query
6.2 QueryTransformationandRe nement 209
1. BadmansTropicalFish
A freshwater aquarium page covering all aspects of the tropical fish hobby. ... to
Badman's Tropical Fish. ... world of aquariology with Badman's Tropical Fish. ... 2. TropicalFish
Notes on a few species and a gallery of photos of African cichlids.
3. TheTropicalTankHomepage-TropicalFishandAquariums
Info on tropical fish and tropical aquariums, large fish species index with ... Here you will find lots of information on Tropical Fish and Aquariums. ...
4. TropicalFishCentre
Offers a range of aquarium products, advice on choosing species, feeding, and health
care, and a discussion board.
5. Tropicalfish-Wikipedia,thefreeencyclopedia
Tropical fish are popular aquarium fish , due to their often bright coloration. ... Practical Fishkeeping • Tropical Fish Hobbyist • Koi. Aquarium related companies: ...
6. TropicalFishFind
Home page for Tropical Fish Internet Directory ... stores, forums, clubs, fish facts,
tropical fish compatibility and aquarium ... 7. Breedingtropicalfish
... intrested in keeping and/or breeding Tropical, Marine, Pond and Coldwater fish. ... Breeding Tropical Fish ... breeding tropical, marine, coldwater & pond fish. ...
8. FishLore
Includes tropical freshwater aquarium how-to guides, FAQs, fish profiles, articles, and forums.
9. Cathy'sTropicalFishKeeping
Information on setting up and maintaining a successful freshwater aquarium.
10.Tropical Fish Place
Tropical Fish information for your freshwater fish tank ... great amount of information
about a great hobby, a freshwater tropical fish tank. ...
Fig. 6.1. Top ten results for the query “tropical  sh”
“tropical  sh”. To expand this query using pseudo-relevance feedback, we might assume that all these top 10 documents were relevant. By analyzing the full text of these documents, the most frequent terms, with their frequencies, can be iden- ti ed as:
a (926), td (535), href (495), http (357), width (345), com (343), nbsp (316), www (260), tr (239), htm (233), class (225), jpg (221)
          
210 6 QueriesandInterfaces
Clearly, these words are not appropriate to use as expansion terms, because they consist of stopwords and HTML expressions that will be common in the whole collection. In other words, they do not represent the topics covered in the top- ranked documents. A simple way to re ne this process is to count words in the snippets of the documents and ignore stopwords.  is analysis produces the fol- lowing list of frequent words:
tropical (26),  sh (28), aquarium (8), freshwater (5), breeding (4), information (3), species (3), tank (2), Badman’s (2), page (2), hobby (2), forums (2)
 ese words are much better candidates for query expansion, and do not have the problem of inadequate context that occurs when we try to expand “tropical” and “ sh” separately. If the user was, however, speci cally interested in breeding tropical  sh, the expansion terms could be improved using true relevance feed- back, where the document ranked seventh would be explicitly tagged as relevant. In this case, the most frequent terms are:
breeding (4),  sh (4), tropical (4), marine (2), pond (2), coldwater (2), keeping (1), interested (1)
 e major effect of using this list would be to increase the weight of the expansion term“breeding”. especi cweighting,aswehavesaid,dependsontheunderly- ing retrieval model.
Both relevance feedback and pseudo-relevance feedback have been extensively investigated in the research literature, and have been shown to be effective tech- niques for improving ranking.  ey are, however, seldom incorporated into oper- ational search applications. In the case of pseudo-relevance feedback, this appears to be primarily because the results of this automatic process can be unpredictable. If the initial ranking does not contain many relevant documents, the expansion terms found by pseudo-relevance feedback are unlikely to be helpful and, for some queries, can make the ranking signi cantly worse. To avoid this, the candidate ex- pansion terms could be shown to the user, but studies have shown that this is not particularly effective. Suggesting alternative queries based on an analysis of query logs is a more reliable alternative for semi-automatic query expansion.
Relevance feedback, on the other hand, has been used in some applications, such as document  ltering. Filtering involves tracking a person’s interests over time, and some applications allow people to modify their pro les using relevance feedback. Another simple use of relevance feedback is the “more like this” fea- ture in some early web search engines.  is feature allowed users to click on a link
6.2 QueryTransformationandRe nement 211
associated with each document in a result list in order to generate a ranked list of other documents similar to the clicked-on document.  e new ranked list of documents was based on a query formed by extracting and weighting important words from the clicked-on document.  is is exactly the relevance feedback pro- cess, but limited to a single relevant document for training data.
Although these applications have had some success, the alternative approach of asking users to choose a different query from a list of suggested queries is cur- rently more popular.  ere is no guarantee, of course, that the suggested queries will contain exactly what the user is looking for, and in that sense relevance feed- back supports more precise query reformulation.  ere is an assumption, how- ever, underlying the use of relevance feedback: that the user is looking for many relevant documents, not just the one or two that may be in the initial ranked list. For some queries, such as looking for background information on a topic, this may be true, but for many queries in the web environment, the user will be satis ed with the initial ranking and will not need relevance feedback. Lists of suggested queries will be helpful when the initial query fails, whereas relevance feedback is unlikely to help in that case.
6.2.5 Context and Personalization
One characteristic of most current search engines is that the results of a query will be the same regardless of who submitted the query, why the query was submit- ted, where the query was submitted, or what other queries were submitted in the same session. All that matters is what words were used to describe the query.  e other factors, known collectively as the query context, will affect the relevance of retrieved documents and could potentially have a signi cant impact on the rank- ing algorithm. Most contextual information, however, has proved to be difficult to capture and represent in a way that provides consistent effectiveness improve- ments.
Much research has been done, in particular, on learning user models or pro-  les to represent a person’s interests so that a search can be personalized. If the system knew that a person was interested in sports, for example, the documents retrieved for the query “vikings” may be different than those retrieved by the same query for a person interested in history. Although this idea is appealing, there are a number of problems with actually making it work.  e  rst is the accuracy of the user models.  e most common proposal is to create the pro les based on the documents that the person looks at, such as web pages visited, email messages,
212 6 QueriesandInterfaces
or word processing documents on the desktop.  is type of pro le represents a person using words weighted by their importance. Words that occur frequently in the documents associated with that person, but are not common words in gen- eral, will have the highest weights. Given that documents contain hundreds or even thousands of words, and the documents visited by the person represent only a snapshot of their interests, these models are not very speci c. Experiments have shown that using such models does not improve the effectiveness of ranking on average.
An alternative approach would be to ask the user to describe herself using pre- de ned categories. In addition to requiring additional (and optional) interactions that most people tend to avoid, there is still the fundamental problem that some- one with a general interest in sports may still want to ask a question about history.  is suggests that a category of interest could be speci ed for each query, such as specifying the “history” category for the query “vikings”, but this is no different than simply entering a less ambiguous query. It is much more effective for a person to enter an extra word or two in her query to clarify it—such as “vikings quarter- backs” or “vikings exploration”, for example—than to try to classify a query into a limited set of categories.
Another issue that is raised by any approach to personalization based on user models is privacy. People have understandable concerns about personal details be- ing recorded in corporate and government databases. In response, techniques for maintaining anonymity while searching and browsing on the Web are becoming an increasingly popular area for research and development. Given this, a search engine that creates pro les based on web activity may be viewed negatively, espe- cially since the bene t of doing this is currently not clear.
Problems with user modeling and privacy do not mean that contextual infor- mation is not useful, but rather that the bene ts of any approach based on context need to be examined carefully.  ere are examples of applications where the use of contextual information is clearly effective. One of these is the use of query logs and clickthrough data to improve web search.  e context in this case is the his- tory of previous searches and search sessions that are the same or very similar. In general, this history is based on the entire user population. A particular person’s search history may be useful for “caching” results for common search queries, but learning from a large number of queries across the population appears to be much more effective.
Another effective application of context is local search, which uses geographic information derived from the query, or from the location of the device that the
6.2 QueryTransformationandRe nement 213
query comes from, to modify the ranking of search results. For example, the query “ shing supplies” will generate a long list of web pages for suppliers from all over the country (or the world).  e query “ shing supplies Cape Cod”, however, should use the context provided by the location “Cape Cod” to rank suppliers in that region higher than any others. Similarly, if the query “ shing supplies” came from a mobile device in a town in Cape Cod, then this information could be used to rank suppliers by their proximity to that town.
Local search based on queries involves the following steps:
1. Identify the geographic region associated with web pages.  is is done either by using location metadata that has been manually added to the document, or by automatically identifying locations, such as place names, city names, or country names, in the document text.
2. Identify the geographic region associated with the query using automatic techniques. Analysis of query logs has shown that 10–15% of queries contain some location reference.
3. Rank web pages using a comparison of the query and document location in- formation in addition to the usual text- and link-based features.
Automatically identifying the location information in text is a speci c exam-
ple of the information extraction techniques mentioned in Chapter 4. Location names are mapped to speci c regions and coordinates using a geographic ontol- ogy15 and algorithms developed for spatial reasoning in geographic information systems. For example, the location “Cape Cod” in a document might be mapped to bounding rectangles based on latitude and longitude, as shown in Figure 6.2, whereas a town location would be mapped to more speci c coordinates (or a smaller bounding rectangle). Although this sounds straightforward, there are many issues involved in identifying location names (for example, there are more than 35 places named Spring eld in the United States), deciding which locations are signi cant (if a web page discusses the “problems with Washington lobbyists”, should“Washington”beusedaslocationmetadata?),andcombiningmultiplelo- cation references in a document.
15 Anontologyisessentiallythesamethingasathesaurus.Itisarepresentationofthecon- cepts in a domain and the relationships between them, whereas a thesaurus describes words, phrases, and relationships between them. Ontologies usually have a richer set of relationships than a thesaurus. A taxonomy is another term used to describe categories of concepts.
 
214 6 QueriesandInterfaces
 e geographic comparison used in the ranking could involve inclusion (for example, the location metadata for a supplier’s web page indicates that the sup- plier is located in the bounding box that represents Cape Cod), distance (for ex- ample, the supplier is within 10 miles of the town that the query mentioned), or other spatial relationships. From both an efficiency and effectiveness perspective, there will be implications for exactly how and when the geographic information is incorporated into the ranking process.
Fig. 6.2. Geographic representation of Cape Cod using bounding rectangles
To summarize, the most useful contextual information for improving search quality is based on past interactions with the search engine (i.e., the query log and session history). Local search based on geographic context can also produce substantial improvements for a subset of queries. In both cases, context is used to provide additional features to enhance the original query (query expansion pro- vides additional words, and local search provides geographic distance). To under- stand the context for a speci c query, however, there is no substitute for the user providing a more speci c query. Indeed, local search in most cases relies on the location being speci ed in the query. Typically, more speci c queries come from users examining the results and then reformulating the query.  e results display, which we discuss next, must convey the context of the query term matches so that the user can decide which documents to look at in detail or how to reformulate the query.
   
6.3 Showing the Results
6.3.1 Result Pages and Snippets
Successful interactions with a search engine depend on the user understanding the results. Many different visualization techniques have been proposed for dis- playing search output (Hearst, 1999), but for most search engines the result pages consist of a ranked list of document summaries that are linked to the actual doc- uments or web pages. A document summary for a web search typically contains the title and URL of the web page, links to live and cached versions of the page, and, most importantly, a short text summary, or snippet, that is used to convey the content of the page. In addition, most result pages contain advertisements con- sisting of short descriptions and links. Query words that occur in the title, URL, snippet, or advertisements are highlighted to make them easier to identify, usually by displaying them in a bold font.
Figure 6.3 gives an example of a document summary from a result page for a web search. In this case, the snippet consists of two partial sentences. Figure 6.1 gives more examples of snippets that are sometimes full sentences, but o en text fragments, extracted from the web page. Some of the snippets do not even contain the query words. In this section, we describe some of the basic features of the algorithms used for snippet generation.
Tropical Fish
One of the U.K.s Leading suppliers of Tropical, Coldwater, Marine Fish and Invertebrates plus.. . next day fish delivery service ...
www.tropicalfish.org.uk/tropical_fish.htm Cached page
Fig. 6.3. Typical document summary for a web search
Snippet generation is an example of text summarization. Summarization tech- niques have been developed for a number of applications, but primarily have been tested using news stories from the TREC collections. A basic distinction is made between techniques that produce query-independent summaries and those that produce query-dependent summaries. Snippets in web search engine result pages are clearly query-dependent summaries, since the snippet that is generated for a page will depend on the query that retrieved it, but some query-independent fea- tures, such as the position of the text in the page and whether the text is in a head- ing, are also used.
6.3 ShowingtheResults 215
  
216 6 QueriesandInterfaces
 e development of text summarization techniques started with H. P. Luhn in the 1950s (Luhn, 1958). Luhn’s approach was to rank each sentence in a doc- ument using a signi cance factor and to select the top sentences for the summary.  e signi cance factor for a sentence is calculated based on the occurrence of sig- ni cant words. Signi cant words are de ned in his work as words of medium fre- quency in the document, where “medium” means that the frequency is between prede ned high-frequency and low-frequency cutoff values. Given the signi cant words, portions of the sentence that are “bracketed” by these words are consid- ered, with a limit set for the number of non-signi cant words that can be between two signi cant words (typically four).  e signi cance factor for these bracketed text spans is computed by dividing the square of the number of signi cant words in the span by the total number of words. Figure 6.4 gives an example of a text span for which the signi cance factor is 42/7 = 2.3.  e signi cance factor for a sentence is the maximum calculated for any text span in the sentence.
                  w  w  w  w  w  w  w  w  w  w  w. 
                        (Initial sentence) 
  w  w  s  w  s  s  w  w  s  w  w.                  (Identify significant words) 
 
                  w  w [s  w  s  s  w  w  s] w  w. 
(Text span bracketed by significant words) 
 
Fig. 6.4. An example of a text span of words (w) bracketed by signi cant words (s) using
Luhn’s algorithm
To be more speci c about the de nition of a signi cant word, the following is a frequency-based criterion that has been used successfully in more recent research. If fd,w is the frequency of word w in document d, then w is a signi cant word if it is not a stopword (which eliminates the high-frequency words), and

7−0.1×(25−sd),ifsd <25
fd,w ≥7, if25≤sd ≤40 7 + 0.1 × (sd − 40), otherwise,
where sd is the number of sentences in document d. As an example, the second page of Chapter 1 of this book contains less than 25 sentences (roughly 20), and so the signi cant words will be non-stopwords with a frequency greater than or equal
6.3 ShowingtheResults 217
to 6.5.  e only words that satisfy this criterion are “information” (frequency 9), “story” (frequency 8), and “text” (frequency 7).
Most work on summarization since Luhn has involved improvements to this basic approach, including better methods of selecting signi cant words and se- lecting sentences or sentence fragments. Snippet generation techniques can also be viewed as variations of Luhn’s approach with query words being used as the signi cant words and different sentence selection criteria.
Typical features that would be used in selecting sentences for snippets to sum- marize a text document such as a news story would include whether the sentence is a heading, whether it is the  rst or second line of the document, the total num- ber of query terms occurring in the sentence, the number of unique query terms in the sentence, the longest contiguous run of query words in the sentence, and a density measure of query words, such as Luhn’s signi cance factor. In this ap- proach, a weighted combination of features would be used to rank sentences. Web pages, however, o en are much less structured than a news story, and can contain a lot of text that would not be appropriate for snippets. To address this, snip- pet sentences are o en selected from the metadata associated with the web page, such as the “description” identi ed by the <meta name=”description” content= ...> HTML tags, or from external sources, such as web directories.16 Certain classes of web pages, such as Wikipedia entries, are more structured and have snippet sentences selected from the text.
Although many variations are possible for snippet generation and document summaries in result pages, some basic guidelines for effective summaries have been derived from an analysis of clickthrough data (Clarke et al., 2007).  e most im- portant is that whenever possible, all of the query terms should appear in the summary, showing their relationship to the retrieved page. When query terms are present in the title, however, they need not be repeated in the snippet.  is al- lows for the possibility of using sentences from metadata or external descriptions that may not have query terms in them. Another guideline is that URLs should be selected and displayed in a manner that emphasizes their relationship to the query by, for example, highlighting the query terms present in the URL. Finally, search engine users appear to prefer readable prose in snippets (such as complete or near-complete sentences) rather than lists of keywords and phrases. A feature that measures readability should be included in the computation of the ranking for snippet selection.
16 For example, the Open Directory Project, http://www.dmoz.org.
 
218 6 QueriesandInterfaces
 e efficient implementation of snippet generation will be an important part of the search engine architecture since the obvious approach of  nding, opening, and scanning document  les would lead to unacceptable overheads in an envi- ronment requiring high query throughput. Instead, documents must be fetched from a local document store or cache at query time and decompressed.  e docu- ments that are processed for snippet generation should have all HTML tags and other “noise” (such as Javascript) removed, although metadata must still be distin- guished from text content. In addition, sentence boundaries should be identi ed and marked at indexing time, to avoid this potentially time-consuming operation when selecting snippets.
6.3.2 Advertising and Search
Advertising is a key component of web search engines since that is how companies generate revenue. In the case of advertising presented with search results (spon- sored search), the goal is to  nd advertisements that are appropriate for the query context. When browsing web pages, advertisements are selected for display based on the contents of pages. Contextual advertising is thought to lead to more user clicks on advertisements (clickthrough), which is how payments for advertising are determined. Search engine companies maintain a database of advertisements, which is searched to  nd the most relevant advertisements for a given query or web page. An advertisement in this database usually consists of a short text de- scription and a link to a web page describing the product or service in more detail. Searching the advertisement database can therefore be considered a special case of general text search.
Nothing is ever that simple, however. Advertisements are not selected solely based on their ranking in a simple text search. Instead, advertisers bid for key- words that describe topics associated with their product.  e amount bid for a keyword that matches a query is an important factor in determining which adver- tisement is selected. In addition, some advertisements generate more clickthrough because they are more appealing to the user population.  e popularity of an ad- vertisement, as measured by the clickthrough over time that is captured in the query log, is another signi cant factor in the selection process.  e popularity of an advertisement can be measured over all queries or on a query-speci c ba- sis. Query-speci c popularity can be used only for queries that occur on a regular basis. For the large number of queries that occur infrequently (so-called long-tail
6.3 ShowingtheResults 219
queries17), the general popularity of advertisements can be used. By taking all of these factors into account, namely relevance, bids, and popularity, the search en- gine company can devise strategies to maximize their expected pro t.
As an example, a pet supplies company that specializes in tropical  sh may place the highest bid for the keywords “aquarium” and “tropical  sh”. Given the query “tropical  sh”, this keyword is certainly relevant.  e content of the ad- vertisement for that company should also contain words that match the query. Given that, this company’s advertisement will receive a high score for relevance and a high score based on the bid. Even though it has made the highest bid, how- ever, there is still some chance that another advertisement will be chosen if it is very popular and has a moderately high bid for the same keywords.
Much ongoing research is directed at developing algorithms to maximize the advertiser’s pro t, drawing on  elds such as economics and game theory. From the information retrieval perspective, the key issues are techniques for matching short pieces of text (the query and the advertisement) and selecting keywords to represent the content of web pages.
When searching the Web, there are usually many pages that contain all of the query terms.  is is not the case, however, when queries are compared to adver- tisements. Advertisements contain a small number of words or keywords relative to a typical page, and the database of advertisements will be several orders of mag- nitude smaller than the Web. It is also important that variations of advertisement keywords that occur in queries are matched. For example, if a pet supply com- pany has placed a high bid for “aquarium”, they would expect to receive some traffic from queries about “ sh tanks”.  is, of course, is the classic vocabulary mismatch problem, and many techniques have been proposed to address this, such as stemming and query expansion. Since advertisements are short, techniques for expanding the documents as well as the queries have been considered.
Two techniques that have performed well in experiments are query reformu- lation based on user sessions in query logs (Jones et al., 2006) and expansion of queries and documents using external sources, such as the Web (Metzler et al., 2007).
Studies have shown that about 50% of the queries in a single session are refor- mulations, where the user modi es the original query through word replacements,
17  e term “long-tail” comes from the long tail of the Zipf distribution described in Chapter 4. Assuming that a query refers to a speci c combination of words, most queries occur infrequently, and a relatively small number account for the majority of the query instances that are processed by search engines.
 
220 6 QueriesandInterfaces
insertions, and deletions. Given a large number of candidate associations between queries and phrases in those queries, statistical tests, such as those described in section 6.2.3, can be used to determine which associations are signi cant. For ex- ample, the association between the phrases “ sh tank” and “aquarium” may occur o en in search sessions as users reformulate their original query to  nd more web pages. If this happens o en enough relative to the frequency of these phrases, it will be considered signi cant.  e signi cant associations can be used as potential substitutions, so that, given an initial query, a ranked list of query reformulations can be generated, with the emphasis on generating queries that contain matches for advertising keywords.
 e expansion technique consists of using the Web to expand either the query, the advertisement text, or both. A form of pseudo-relevance feedback is used where the advertisement text or keywords are used as a query for a web search, and expansion words are selected from the highest-ranking web pages. Experi- ments have shown that the most effective relevance ranking of advertisements is when exact matches of the whole query are ranked  rst, followed by exact matches of the whole query with words replaced by stems, followed by a probabilistic sim- ilarity match of the expanded query with the expanded advertisement.  e type of similarity match used is described in section 7.3.
fish tanks at Target
Find fish tanks Online. Shop & Save at Target.com Today. www.target.com
Aquariums
540+ Aquariums at Great Prices.
fishbowls.pronto.com
Freshwater Fish Species
Everything you need to know to keep your setup clean and beautiful www.FishChannel.com
Pet Supplies at Shop.com
Shop millions of products and buy from our trusted merchants.
shop.com
Custom Fish Tanks
Choose From 6,500+ Pet Supplies. Save On Custom Fish Tanks! shopzilla.com
Fig. 6.5. Advertisements displayed by a search engine for the query “ sh tanks”
     
6.3 ShowingtheResults 221
As an example, Figure 6.5 shows the list of advertisements generated by a search engine for the query “ sh tanks”. Two of the advertisements are obvious matches, in that “ sh tanks” occurs in the titles. Two of the others (the second and fourth) have no words in common with the query, although they are clearly rele- vant. Using the simple pseudo-relevance feedback technique described in section 6.2.4 would produce both “aquarium” (frequency 10) and “acrylic” (frequency 7) as expansion terms based on the top 10 results.  is would give advertisements containing “aquarium”, such as the second one, a higher relevance score in the se- lection process.  e fourth advertisement has presumably been selected because the pet supplier has bid on the keyword “aquarium”, and potentially because many people have clicked on this advertisement.  e third advertisement is similar and matches one of the query words.
In the case of contextual advertising for web pages, keywords typically are ex- tracted from the contents of the page and then used to search the advertising database to select advertisements for display along with the contents of the page. Keyword selection techniques are similar to the summarization techniques de- scribed in the last section, with the focus on keywords rather than sentences. A simple approach would be to select the top words ranked by a signi cance weight based on relative frequencies in the document and the collection of documents.
A more effective approach is to use a classi er based on machine learning tech- niques, as described in Chapter 9. A classi er uses a weighted combination of features to determine which words and phrases are signi cant. Typical features include the frequency in the document, the number of documents in which the word or phrase occurs, functions of those frequencies (such as taking the log or normalizing), frequency of occurrence in the query log, location of the word or phrase in the document (e.g., the title, body, anchor text, metadata, URL), and whether the word or phrase was capitalized or highlighted in some way.  e most useful features are the document and query log frequency information (Yih et al., 2006).
6.3.3 Clustering the Results
 e results returned by a search engine are o en related to different aspects of the query topic. In the case of an ambiguous query, these groups of documents can represent very different interpretations of the query. For example, we have seen how the query “tropical  sh” retrieves documents related to aquariums, pet sup- plies, images, and other subtopics. An even simpler query, such as “ sh”, is likely
222 6 QueriesandInterfaces
to retrieve a heterogeneous mix of documents about the sea, so ware products, a rock singer, and anything else that happens to use the name “ sh”. If a user is in- terested in a particular aspect of a query topic, scanning through many pages on different aspects could be frustrating.  is is the motivation for the use of cluster- ing techniques on search results. Clustering groups documents that are similar in content and labels the clusters so they can be quickly scanned for relevance.
Pictures (38)
Aquarium Fish (28)
Tropical Fish Aquarium (26) Exporter (31)
Supplies (32)
Plants, Aquatic (18)
Fish Tank (15)
Breeding (16)
Marine Fish (16)
Aquaria (9)
Fig. 6.6. Clusters formed by a search engine from top-ranked documents for the query “tropical  sh”. Numbers in brackets are the number of documents in the cluster.
Figure 6.6 shows a list of clusters formed by a web search engine from the top- ranked documents for the query “tropical  sh”.  is list, where each cluster is de- scribed or labeled using a single word or phrase and includes a number indicating the size of the cluster, is displayed to the side of the usual search results. Users that are interested in one of these clusters can click on the cluster label to see a list of those documents, rather than scanning the ranked list to  nd documents related to that aspect of the query. In this example, the clusters are clearly related to the subtopics we mentioned previously, such as supplies and pictures.
Clustering techniques are discussed in detail in Chapter 9. In this section, we focus on the speci c requirements for the task of clustering search results.  e  rst of these requirements is efficiency.  e clusters that are generated must be speci c to each query and are based on the top-ranked documents for that query.  e clus- ters for popular queries could be cached, but clusters will still need to be generated online for most queries, and this process has to be efficient. One consequence of this is that cluster generation is usually based on the text of document snippets,
          
6.3 ShowingtheResults 223
rather than the full text. Snippets typically contain many fewer words than the full text, which will substantially speed up calculations that involve comparing word overlap. Snippet text is also designed to be focused on the query topic, whereas documents can contain many text passages that are only partially relevant.
 e second important requirement for result clusters is that they are easy to understand. In the example in Figure 6.6, each cluster is labeled by a single word or phrase, and the user will assume that every document in that cluster will be described by that concept. In the cluster labeled “Pictures”, for example, it is rea- sonable to expect that every document would contain some pictures of  sh.  is is an example of a monothetic classi cation, where every member of a class has the property that de nes the class.18  is may sound obvious, but in fact it is not the type of classi cation produced by most clustering algorithms. Membership of a class or cluster produced by an algorithm such as K-means19 is based on word overlap. In other words, members of clusters share many properties, but there is no single de ning property.  is is known as a polythetic classi cation. For result clustering, techniques that produce monothetic classi cations (or, at least, those that appear to be monothetic) are preferred because they are easier to understand.
As an example, consider documents in the search results D1, D2, D3, and D4 that contain the terms (i.e., words or phrases) {a, b, c, d, e, f, g}.  e sets of terms representing each document are:
D1 ={a,b,c} D2 ={a,d,e} D3 = {d,e,f,g} D4 ={f,g}
A monothetic algorithm may decide that a and e are signi cant terms and pro- duce the two clusters {D1, D2} (with cluster label a) and {D2, D3} (labeled e). Note that these clusters are overlapping, in that a document may belong to more than one cluster. A polythetic algorithm may decide that, based on term overlap, the only signi cant cluster is {D2, D3, D4}—D2 has two terms in common with D3, and D3 has two terms in common with D4. Note that these three documents have no single term in common, and it is not clear how this cluster would be la- beled.
18  is is also the de nition of a class proposed by Aristotle over 2,400 years ago.
19 K-means clustering is described in Chapter 9, but basically a document is compared
to representatives of the existing clusters and added to the most similar cluster.
 
224 6 QueriesandInterfaces
If we consider the list of snippets shown in Figure 6.1, a simple clustering based on the non-stopwords that occur in more than one document would give us:
aquarium(5) (Documents1,3,4,5,8) freshwater (4) (1, 8, 9, 10)
species (3) (2, 3, 4)
hobby (3) (1, 5, 10)
forums (2) (6, 8)
In an actual implementation of this technique, both words and phrases would be considered and many more of the top-ranking snippets (say, 200) would be used. Additional features of the words and phrases, such as whether they occurred in titles or snippets, the length of the phrase, and the collection frequency of the phrase, as well as the overlap of the resulting clusters, would be considered in choosing the  nal set of clusters.
An alternative approach for organizing results into meaningful groups is to use faceted classi cation or, more simply, facets. A faceted classi cation consists of a set of categories, usually organized into a hierarchy, together with a set of facets that describe the important properties associated with the category. A product de- scribed by a faceted classi cation, for example, could be labeled by more than one category and will have values for every facet. Faceted classi cations are primar- ily manually de ned, although it is possible to support faceted browsing for data that has been structured using a database schema, and techniques for construct- ing faceted classi cations automatically are being studied.  e major advantage of manually de ning facets is that the categories are in general easier for the user to understand than automatically generated cluster labels.  e disadvantages are that a classi cation has to be de ned for each new application and domain, and manual classi cations tend to be static and not as responsive to new data as dy- namically constructed clusters.
Facets are very common in e-commerce sites. Figure 6.7 shows the set of cat- egories returned for the query “tropical  sh” for a search on a popular retailer’s site.  e numbers refer to the number of products in each category that match the query.  ese categories are displayed to the side of the search results, similar to the clusters discussed earlier. If the “Home & Garden” category is selected, Fig- ure6.8showsthatwhatisdisplayedisalistofsubcategories,suchas“petsupplies”, together with facets for this category, which include the brand name, supplier or vendor name, discount level, and price. A given product, such as an aquarium, can be found under “pet supplies” and in the appropriate price level, discount level,
6.3 ShowingtheResults 225
etc.  is type of organization provides the user with both guidance and  exibility in browsing the search results.
Books (7,845)  Home & Garden (2,477)  Apparel (236)  Home Improvement (169)  Jewelry & Watches (76)  Sports & Outdoors (71)  Office Products (68)  Toys & Games (62)  Everything Else (44)  Electronics (26) 
Baby (25)   
 
DVD (12) 
Music (11) 
Software (10) 
Gourmet Food (6) 
Beauty (4) 
Automotive (4) 
Magazine Subscriptions (3) 
Health & Personal Care (3) 
Wireless Accessories (2) 
Video Games (1)  
 
                     Fig. 6.7. Categories returned for the query “tropical  sh” in a popular online retailer
Discount   
Up to 25% off (563)  25%   50% off (472)  50%   70% off (46)  70% off or more (46)   
Price   
$0 $24 (1,032)  $25 $49 (394)  $50 $99 (797)  $100 $199 (206)  $200 $499 (39)  $500 $999 (9)  $1000 $1999 (5)  $5000 $9999 (7) 
Fig. 6.8. Subcategories and facets for the “Home & Garden” category
Home & Garden  
Kitchen & Dining (149)  Furniture & Décor (1,776)  Pet Supplies (368)  Bedding & Bath (51)  Patio & Garden (22)  Art & Craft Supplies (12)  Home Appliances (2)  Vacuums, Cleaning & Storage  (107) 
 
Brand   
     <brand names> 
Seller  
     <vendor names>    
226 6 QueriesandInterfaces
6.4 Cross-Language Search
By translating queries for one or more monolingual search engines covering dif- ferent languages, it is possible to do cross-language search20 (see Figure 6.9). A cross-language search engine receives a query in one language (e.g., English) and retrieves documents in a variety of other languages (e.g., French and Chinese). Users typically will not be familiar with a wide range of languages, so a cross- language search system must do the query translation automatically. Since the system also retrieves documents in multiple languages, some systems also trans- late these for the user.
Translate Query
Translated  Queries
Search engines for  other languages
Retrieved  documents  in other 
languages
      User
    Translate
Fig. 6.9. Cross-language search
 e most obvious approach to automatic translation would be to use a large bilingual dictionary that contained the translation of a word in the source lan- guage (e.g., English) to the target language (e.g., French). Sentences would then be translated by looking up each word in the dictionary.  e main issue is how to deal with ambiguity, since many words have multiple translations. Simple dictionary- based translations are generally poor, but a number of techniques have been de- veloped, such as query expansion (section 6.2.3), that reduce ambiguity and in-
20 Alsocalledcross-languageinformationretrieval(CLIR),cross-lingualsearch,andmul- tilingual search.
 
6.4 Cross-LanguageSearch 227
crease the ranking effectiveness of a cross-language system to be comparable to a monolingual system.
 e most effective and general methods for automatic translation are based on statistical machine translation models (Manning & Schütze, 1999). When translat- ing a document or a web page, in contrast to a query, not only is ambiguity a prob- lem, but the translated sentences should also be grammatically correct. Words can change order, disappear, or become multiple words when a sentence is translated. Statistical translation models represent each of these changes with a probability.  is means that the model describes the probability that a word is translated into another word, the probability that words change order, and the probability that words disappear or become multiple words.  ese probabilities are used to calcu- late the most likely translation for a sentence.21
Although a model that is based on word-to-word translation probabilities has some similarities to a dictionary-based approach, if the translation probabilities are accurate, they can make a large difference to the quality of the translation. Un- usual translations for an ambiguous word can then be easily distinguished from more typical translations. More recent versions of these models, called phrase- based translation models, further improve the use of context in the translation by calculating the probabilities of translating sequences of words, rather than just in- dividual words. A word such as “ ight”, for example, could be more accurately translated as the phrase “commercial  ight”, instead of being interpreted as “bird  ight”.
 e probabilities in statistical machine translation models are estimated pri- marily by using parallel corpora.  ese are collections of documents in one lan- guage together with the translations into one or more other languages.  e cor- pora are obtained primarily from government organizations (such as the United Nations), news organizations, and by mining the Web, since there are hundreds of thousands of translated pages.  e sentences in the parallel corpora are aligned either manually or automatically, which means that sentences are paired with their translations.  e aligned sentences are then used for training the translation model.
21  e simplest form of a machine translation model is actually very similar to the query likelihood model described in section 7.3.1.  e main difference is the incorporation of a translation probability P (wi|wj ), which is the probability that a word wj can be translated into the word wi, in the estimation of P (Q|D). P (Q|D) is the probability of generating a query from a document, which in the translation model becomes the probability that a query is a translation of the document.
 
228 6 QueriesandInterfaces
Special attention has to be paid to the translation of unusual words, espe- cially proper nouns such as people’s names. For these words in particular, the Web is a rich resource. Automatic transliteration techniques are also used to ad- dress the problem of people’s names. Proper names are not usually translated into another language, but instead are transliterated, meaning that the name is writ- ten in the characters of another language according to certain rules or based on similar sounds.  is can lead to many alternative spellings for the same name. For example, the Libyan leader Muammar Qadda ’s name can found in many different transliterated variants on web pages, such as Qatha , Kadda , Qada , Gada , Gadda , Katha , Kadha , Qadha , Qazza , Kaza , Qaddafy, Qadafy, Quadhaffi, Gadhdha , al-Qadda , Al-Qadda , and Al Qadda . Similarly, there are a number of variants of “Bill Clinton” on Arabic web pages.
Although they are not generally regarded as cross-language search systems, web search engines can o en retrieve pages in a variety of languages. For that reason, many search engines have made translation available on the result pages. Figure 6.10 shows an example of a page retrieved for the query “pecheur france”, where the translation option is shown as a hyperlink. Clicking on this link pro- duces a translation of the page (not the snippet), which makes it clear that the page contains links to archives of the sports magazine Le pêcheur de France, which is translated as “ e  sherman of France”. Although the translation provided is not perfect, it typically provides enough information for someone to understand the contents and relevance of the page.  ese translations are generated automat- ically using machine translation techniques, since any human intervention would be prohibitively expensive.
Le pêcheur de France archives @ peche poissons - [ Translate this page ]
Le pêcheur de France Les média Revues de pêche Revue de presse Archives de la revue Le pêcheur de France janvier 2003 n°234 Le pêcheur de France mars 2003 ...
 
Fig. 6.10. A French web page in the results list for the query “pecheur france”
References and Further Reading
 is chapter has covered a wide range of topics that have been studied for a num- ber of years. Consequently, there are many references that are relevant and provide
  
6.4 Cross-LanguageSearch 229
more detail than we are able to cover here.  e following papers and books rep- resent some of the more signi cant contributions, but each contains pointers to other work for people interested in gaining a deeper understanding of a speci c topic.
 e advantages and disadvantages of Boolean queries relative to “natural lan- guage” or keyword queries has been discussed for more than 30 years.  is debate has been particularly active in the legal retrieval  eld, which saw the introduction of the  rst search engines using ranking and simple queries on large collections in the early 1990s. Turtle (1994) describes one of the few quantitative comparisons of expert Boolean searching to ranking based on simple queries, and found that simple queries are surprisingly effective, even in this professional environment.  e next chapter contains more discussion of the Boolean retrieval model.
A more detailed description of query-based stemming based on corpus analy- sis can be found in J. Xu and Cro  (1998). A good source for the earlier history of association measures such as Dice’s coefficient that have been used for informa- tion retrieval is van Rijsbergen (1979). Peng et al. (2007) describe a more recent version of corpus-based stemming for web search.
Kukich (1992) provides an overview of spelling correction techniques. For a more detailed introduction to minimum edit distance and the noisy channel model for spelling correction, see Jurafsky and Martin (2006). Guo et al. (2008) describe an approach that combines query re nement steps, such as spelling cor- rection, stemming, and identi cation of phrases, into a single model.  eir results indicate that the uni ed model can potentially improve effectiveness relative to carrying out these steps as separate processes.
Query expansion has been the subject of much research. E himiadis (1996) gives a general overview and history of query expansion techniques, including thesaurus-based expansion. As mentioned before, van Rijsbergen (1979) describes the development of association measures for information retrieval, including the mutual information measure. In computational linguistics, the paper by Church and Hanks (1989) is o en referred to for the use of the mutual information mea- sure in constructing lexicons (dictionaries). Manning and Schütze (1999) give a good overview of these and the other association measures mentioned in this chapter.
Jing and Cro  (1994) describe a technique for constructing an “association thesaurus” from virtual documents consisting of the words that co-occur with other words.  e use of query log data to support expansion is described in Beeferman and Berger (2000) and Cui et al. (2003).
230 6 QueriesandInterfaces
Rocchio (1971) pioneered the work on relevance feedback, which was then followed up by a large amount of work that is reviewed in Salton and McGill (1983) and van Rijsbergen (1979). J. Xu and Cro  (2000) is a frequently cited paper on pseudo-relevance feedback that compared “local” techniques based on top-ranked documents to “global” techniques based on the term associations in the collection.  e book, based on 10 years of TREC experiments (Voorhees & Harman, 2005), contains many descriptions of both relevance feedback and pseudo-relevance feedback techniques.
Context and personalization is a popular topic. Many publications can be found in workshops and conferences, such as the Information Interaction in Con- text Symposium (IIiX).22 Wei and Cro  (2007) describe an experiment that raises questions about the potential bene t of user pro les. Chen et al. (2006) and Zhou et al. (2005) both discuss index structures for efficiently processing local search queries, but also provide general overviews of local search. V. Zhang et al. (2006) discusses local search with an emphasis on analyzing query logs.
 e original work on text summarization was done by Luhn (1958). Goldstein et al. (1999) describe more recent work on summarization based on sentence se- lection.  e work of Berger and Mittal (2000), in contrast, generates summaries based on statistical models of the document. Sun et al. (2005) describe a tech- niques based on clickthrough data.  e papers of Clarke et al. (2007) and Turpin et al. (2007) focus speci cally on snippet generation.
Feng et al. (2007) give a general overview of the issues in sponsored search. Metzler et al. (2007) and Jones et al. (2006) discuss speci c techniques for match- ing queries to short advertisements. A discussion of the issues in contextual adver- tising (providing advertisements while browsing), as well as a speci c technique for selecting keywords from a web page, can be found in Yih et al. (2006).
As mentioned earlier, many visualization techniques have been proposed over the years for search results, and we have ignored most of these in this book. Hearst (1999) provides a good overview of the range of techniques. Leouski and Cro  (1996) presented one of the  rst evaluations of techniques for result clustering. Hearst and Pedersen (1996) show the potential bene ts of this technique, and Zamir and Etzioni (1999) emphasize the importance of clusters that made sense to the user and were easy to label. Lawrie and Cro  (2003) discuss a technique for building a hierarchical summary of the results, and Zeng et al. (2004) focus on the
22  is conference grew out of the Information Retrieval in Context (IRiX) workshops, whose proceedings can also be found on the Web.
 
6.4 Cross-LanguageSearch 231
selection of phrases from the results as the basis of clusters.  e relative advantages and disadvantages of clustering and facets are discussed in Hearst (2006).
More generally, there is a whole community of HCI23 (Human-Computer In- teraction) researchers concerned with the design and evaluation of interfaces for information systems. Shneiderman et al. (1998) is an example of this type of re- search, and Marchionini (2006) gives a good overview of the importance of the search interface for interactive, or exploratory, search.
Cross-language search has been studied at TREC (Voorhees & Harman, 2005) and at a European evaluation forum called CLEF24 for a number of years.  e  rst collection of papers in this area was in Grefenstette (1998). Issues that arise in speci c CLIR systems, such as transliteration (AbdulJaleel & Larkey, 2003), are discussed in many papers in the literature. Manning and Schütze (1999) and Jurafsky and Martin (2006) give overviews of statistical machine translation mod- els.
Finally, there has been a large body of work in the information science lit- erature that has looked at how people actually search and interact with search engines.  is research is complementary to the more systems-oriented approach taken in this chapter, and is a crucial part of understanding the process of looking for information and relevance.  e Journal of the American Society of Informa- tion Science and Technology (JASIST) is the best source for these type of papers, and Ingwersen and Järvelin (2005) provide an interesting comparison of the com- puter science and information science perspectives on search.
Exercises
6.1. Using the Wikipedia collection provided at the book website, create a sample of stem clusters by the following process:
1. Index the collection without stemming.
2. Identify the  rst 1,000 words (in alphabetical order) in the index.
3. Create stem classes by stemming these 1,000 words and recording which
words become the same stem.
4. Compute association measures (Dice’s coefficient) between all pairs of stems
in each stem class. Compute co-occurrence at the document level.
23 Sometimes referred to as CHI. 24 http://clef.isti.cnr.it/
 
232 6 QueriesandInterfaces
5. Create stem clusters by thresholding the association measure. All terms that are still connected to each other form the clusters.
Compare the stem clusters to the stem classes in terms of size and the quality (in your opinion) of the groupings.
6.2. Create a simple spelling corrector based on the noisy channel model. Use a single-word language model, and an error model where all errors with the same edit distance have the same probability. Only consider edit distances of 1 or 2. Implement your own edit distance calculator (example code can easily be found on the Web).
6.3. Implement a simple pseudo-relevance feedback algorithm for the Galago search engine. Provide examples of the query expansions that your algorithm does, and summarize the problems and successes of your approach.
6.4. Assuming you had a gazetteer of place names available, sketch out an algo- rithm for detecting place names or locations in queries. Show examples of the types of queries where your algorithm would succeed and where it would fail.
6.5. Describe the snippet generation algorithm in Galago. Would this algorithm work well for pages with little text content? Describe in detail how you would modify the algorithm to improve it.
6.6. Pick a commercial web search engine and describe how you think the query is matched to the advertisements for sponsored search. Use examples as evidence for your ideas. Do the same thing for advertisements shown with web pages.
6.7. Implement a simple algorithm that selects phrases from the top-ranked pages as the basis for result clusters. Phrases should be considered as any two-word se- quence. Your algorithm should take into account phrase frequency in the results, phrase frequency in the collection, and overlap in the clusters associated with the phrases.
6.8. Find four different types of websites that use facets, and describe them with examples.
6.9. Give  ve examples of web page translation that you think is poor. Why do you think the translation failed?
7
Retrieval Models
“ ere is no certainty, only opportunity.”
V, V for Vendetta
7.1 Overview of Retrieval Models
During the last 45 years of information retrieval research, one of the primary goals has been to understand and formalize the processes that underlie a person mak- ingthedecisionthatapieceoftextisrelevanttohisinformationneed.Todevelop a complete understanding would probably require understanding how language is represented and processed in the human brain, and we are a long way short of that. We can, however, propose theories about relevance in the form of mathemat- ical retrieval models and test those theories by comparing them to human actions. Good models should produce outputs that correlate well with human decisions on relevance. To put it another way, ranking algorithms based on good retrieval models will retrieve relevant documents near the top of the ranking (and conse- quently will have high effectiveness).
How successful has modeling been? As an example, ranking algorithms for general search improved in effectiveness by over 100% in the 1990s, as measured using the TREC test collections.  ese changes in effectiveness corresponded to improvements in the associated retrieval models. Web search effectiveness has also improved substantially over the past 10 years. In experiments with TREC web collections, the most effective ranking algorithms come from well-de ned retrieval models. In the case of commercial web search engines, it is less clear what the retrieval models are, but there is no doubt that the ranking algorithms rely on solid mathematical foundations.
It is possible to develop ranking algorithms without an explicit retrieval model through trial and error. Using a retrieval model, however, has generally proved to be the best approach. Retrieval models, like all mathematical models, provide a
 
234 7 RetrievalModels
framework for de ning new tasks and explaining assumptions. When problems are observed with a ranking algorithm, the retrieval model provides a structure for testing alternatives that will be much more efficient than a brute force (try ev- erything) approach.
In this discussion, we must not overlook the fact that relevance is a complex concept. It is quite difficult for a person to explain why one document is more relevant than another, and when people are asked to judge the relevance of docu- ments for a given query, they can o en disagree. Information scientists have writ- ten volumes about the nature of relevance, but we will not dive into that material here. Instead, we discuss two key aspects of relevance that are important for both retrieval models and evaluation measures.
 e  rst aspect is the difference between topical and user relevance, which was mentioned in section 1.1. A document is topically relevant to a query if it is judged to be on the same topic. In other words, the query and the document are about the same thing. A web page containing a biography of Abraham Lincoln would certainly be topically relevant to the query “Abraham Lincoln”, and would also be topically relevant to the queries “U.S. presidents” and “Civil War”. User relevance takes into account all the other factors that go into a user’s judgment of relevance.  is may include the age of the document, the language of the document, the intended target audience, the novelty of the document, and so on. A document containing just a list of all the U.S. presidents, for example, would be topically relevant to the query “Abraham Lincoln” but may not be considered relevant to the person who submitted the query because they were looking for more detail on Lincoln’s life. Retrieval models cannot incorporate all the additional factors involved in user relevance, but some do take these factors into consideration.
 e second aspect of relevance that we consider is whether it is binary or mul- tivalued. Binary relevance simply means that a document is either relevant or not relevant. It seems obvious that some documents are less relevant than others, but still more relevant than documents that are completely off-topic. For exam- ple, we may consider the document containing a list of U.S. presidents to be less topically relevant than the Lincoln biography, but certainly more relevant than an advertisement for a Lincoln automobile. Based on this observation, some re- trieval models and evaluation measures explicitly introduce relevance as a multi- valued variable. Multiple levels of relevance are certainly important in evaluation, when people are asked to judge relevance. Having just three levels (relevant, non- relevant, unsure) has been shown to make the judges’ task much easier. In the case of retrieval models, however, the advantages of multiple levels are less clear.  is
7.1 OverviewofRetrievalModels 235
is because most ranking algorithms calculate a probability of relevance and can represent the uncertainty involved.
Many retrieval models have been proposed over the years. Two of the oldest are the Boolean and vector space models. Although these models have been largely superseded by probabilistic approaches, they are o en mentioned in discussions about information retrieval, and so we describe them brie y before going into the details of other models.
7.1.1 Boolean Retrieval
 e Boolean retrieval model was used by the earliest search engines and is still in use today. It is also known as exact-match retrieval since documents are retrieved if they exactly match the query speci cation, and otherwise are not retrieved. Al- though this de nes a very simple form of ranking, Boolean retrieval is not gener- ally described as a ranking algorithm.  is is because the Boolean retrieval model assumes that all documents in the retrieved set are equivalent in terms of rele- vance, in addition to the assumption that relevance is binary.  e name Boolean comes from the fact that there only two possible outcomes for query evaluation (TRUE and FALSE) and because the query is usually speci ed using operators from Booleanlogic(AND,OR,NOT).AsmentionedinChapter6,proximityoperators and wildcard characters are also commonly used in Boolean queries. Searching with a regular expression utility such as grep is another example of exact-match retrieval.
 ere are some advantages to Boolean retrieval.  e results of the model are very predictable and easy to explain to users.  e operands of a Boolean query can be any document feature, not just words, so it is straightforward to incorporate metadata such as a document date or document type in the query speci cation. From an implementation point of view, Boolean retrieval is usually more efficient than ranked retrieval because documents can be rapidly eliminated from consid- eration in the scoring process.
Despite these positive aspects, the major drawback of this approach to search is that the effectiveness depends entirely on the user. Because of the lack of a so- phisticated ranking algorithm, simple queries will not work well. All documents containing the speci ed query words will be retrieved, and this retrieved set will be presented to the user in some order, such as by publication date, that has lit- tle to do with relevance. It is possible to construct complex Boolean queries that narrow the retrieved set to mostly relevant documents, but this is a difficult task
236 7 RetrievalModels
that requires considerable experience. In response to the difficulty of formulat- ing queries, a class of users known as search intermediaries (mentioned in the last chapter) became associated with Boolean search systems.  e task of an interme- diary is to translate a user’s information need into a complex Boolean query for a particular search engine. Intermediaries are still used in some specialized areas, such as in legal offices.  e simplicity and effectiveness of modern search engines, however, has enabled most people to do their own searches.
As an example of Boolean query formulation, consider the following queries for a search engine that has indexed a collection of news stories.  e simple query:
lincoln
would retrieve a large number of documents that mention Lincoln cars and places named Lincoln in addition to stories about President Lincoln. All of these doc- uments would be equivalent in terms of ranking in the Boolean retrieval model, regardless of how many times the word “lincoln” occurs or in what context it oc- curs. Given this, the user may attempt to narrow the scope of the search with the following query:
president AND lincoln
 is query will retrieve a set of documents that contain both words, occurring anywhere in the document. If there are a number of stories involving the manage- ment of the Ford Motor Company and Lincoln cars, these will be retrieved in the same set as stories about President Lincoln, for example:
Ford Motor Company today announced that Darryl Hazel will succeed Brian Kelley as president of Lincoln Mercury.
If enough of these types of documents were retrieved, the user may try to eliminate documents about cars by using the NOT operator, as follows:
president AND lincoln AND NOT (automobile OR car)
 is would remove any document that contains even a single mention of the words “automobile” or “car” anywhere in the document.  e use of the NOT op- erator, in general, removes too many relevant documents along with non-relevant documents and is not recommended. For example, one of the top-ranked docu- ments in a web search for “President Lincoln” was a biography containing the sentence:
Lincoln’s body departs Washington in a nine-car funeral train.
7.1 OverviewofRetrievalModels 237
Using NOT (automobile OR car) in the query would have removed this document. If the retrieved set is still too large, the user may try to further narrow the query by adding in additional words that should occur in biographies:
president AND lincoln AND biography AND life AND birthplace AND get- tysburg AND NOT (automobile OR car)
Unfortunately, in a Boolean search engine, putting too many search terms into the query with the AND operator o en results in nothing being retrieved. To avoid this, the user may try using an OR instead:
president AND lincoln AND (biography OR life OR birthplace OR gettysburg) AND NOT (automobile OR car)
 is will retrieve any document containing the words “president” and “lincoln”, along with any one of the words “biography”, “life”, “birthplace”, or “gettysburg” (and does not mention “automobile” or “car”).
A er all this, we have a query that may do a reasonable job at retrieving a set containing some relevant documents, but we still can’t specify which words are more important or that having more of the associated words is better than any one of them. For example, a document containing the following text was retrieved at rank 500 by a web search (which does use measures of word importance):
President’s Day - Holiday activities - cra s, mazes, word searches, ... “ e Life of Washington” Read the entire book online! Abraham Lincoln Re- search Site ...
A Boolean retrieval system would make no distinction between this document and the other 499 that are ranked higher by the web search engine. It could, for example, be the  rst document in the result list.
 e process of developing queries with a focus on the size of the retrieved set has been called searching by numbers, and is a consequence of the limitations of the Boolean retrieval model. To address these limitations, researchers developed models, such as the vector space model, that incorporate ranking.
7.1.2 The Vector Space Model
 e vector space model was the basis for most of the research in information re- trieval in the 1960s and 1970s, and papers using this model continue to appear at conferences. It has the advantage of being a simple and intuitively appealing framework for implementing term weighting, ranking, and relevance feedback.
238 7 RetrievalModels
Historically, it was very important in introducing these concepts, and effective techniques have been developed through years of experimentation. As a retrieval model, however, it has major  aws. Although it provides a convenient computa- tional framework, it provides little guidance on the details of how weighting and ranking algorithms are related to relevance.
In this model, documents and queries are assumed to be part of a t-dimensional vector space, where t is the number of index terms (words, stems, phrases, etc.). A document Di is represented by a vector of index terms:
Di = (di1,di2,...,dit),
where dij represents the weight of the jth term. A document collection contain- ing n documents can be represented as a matrix of term weights, where each row represents a document and each column describes weights that were assigned to a term for a particular document:
Term1 Term2 ... Termt Doc1 d11 d12 ... d1t Doc2 d21 d22 ... d2t
. .
Docn dn1 dn2 ... dnt
Figure 7.1 gives a simple example of the vector representation for four docu- ments.  e term-document matrix has been rotated so that now the terms are the rows and the documents are the columns.  e term weights are simply the count of the terms in the document. Stopwords are not indexed in this example, and the words have been stemmed. Document D3, for example, is represented by the vector(1,1,0,2,0,1,0,1,0,0,1).
Queries are represented the same way as documents.  at is, a query Q is rep- resented by a vector of t weights:
Q = (q1,q2,...,qt),
where qj is the weight of the jth term in the query. If, for example the query was “tropical  sh”, then using the vector representation in Figure 7.1, the query would be (0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1). One of the appealing aspects of the vector space model is the use of simple diagrams to visualize the documents and queries. Typ- ically, they are shown as points or vectors in a three-dimensional picture, as in
7.1 OverviewofRetrievalModels 239
D1 Tropical Freshwater Aquarium Fish.
D2 Tropical Fish, Aquarium Care, Tank Setup.
D3 Keeping Tropical Fish and Goldfish in Aquariums,
and Fish Bowls.
D4 The Tropical Tank Homepage - Tropical Fish and
Aquariums.
Terms
Documents
D1 D2 D3 D4
aquarium 1111 bowl 0010 care 0100 fish 1121 freshwater 1000 goldfish 0010 homepage 0001 keep 0010 setup 0100 tank 0101 tropical 1112
Fig. 7.1. Term-document matrix for a collection of four documents
Figure 7.2. Although this can be helpful for teaching, it is misleading to think that an intuition developed using three dimensions can be applied to the actual high-dimensional document space. Remember that the t terms represent all the document features that are indexed. In enterprise and web applications, this cor- responds to hundreds of thousands or even millions of dimensions.
Given this representation, documents could be ranked by computing the dis- tance between the points representing the documents and the query. More com- monly, a similarity measure is used (rather than a distance or dissimilarity mea- sure), so that the documents with the highest scores are the most similar to the query. A number of similarity measures have been proposed and tested for this purpose.  e most successful of these is the cosine correlation similarity measure.  e cosine correlation measures the cosine of the angle between the query and the document vectors. When the vectors are normalized so that all documents and queries are represented by vectors of equal length, the cosine of the angle be- tween two identical vectors will be 1 (the angle is zero), and for two vectors that do not share any non-zero terms, the cosine will be 0.  e cosine measure is de-  ned as:
240 7 RetrievalModels
  Doc1
Term3
Doc2
Query
   Fig. 7.2. Vector representation of documents and queries
∑t
dij ·qj
Cosine(Di, Q) = √ j=1
   e numerator of this measure is the sum of the products of the term weights for the matching query and document terms (known as the dot product or in- ner product).  e denominator normalizes this score by dividing by the product of the lengths of the two vectors.  ere is no theoretical reason why the cosine correlation should be preferred to other similarity measures, but it does perform somewhat better in evaluations of search quality.
As an example, consider two documents D1 = (0.5, 0.8, 0.3) and D2 = (0.9, 0.4, 0.2) indexed by three terms, where the numbers represent term weights. Given the query Q = (1.5, 1.0, 0) indexed by the same terms, the cosine mea- sures for the two documents are:
∑t j=1
dij2 ·
∑t j=1
qj2
Term2
7.1 OverviewofRetrievalModels 241
Cosine(D1, Q) = √ (0.5 × 1.5) + (0.8 × 1.0) (0.52 + 0.82 + 0.32)(1.52 + 1.02)
= √ 1.55 = 0.87 (0.98 × 3.25)
Cosine(D2, Q) = √ (0.9 × 1.5) + (0.4 × 1.0) (0.92 + 0.42 + 0.22)(1.52 + 1.02)
= √ 1.75 = 0.97 (1.01 × 3.25)
 e second document has a higher score because it has a high weight for the  rst term, which also has a high weight in the query. Even this simple example shows that ranking based on the vector space model is able to re ect term importance and the number of matching terms, which is not possible in Boolean retrieval.
In this discussion, we have yet to say anything about the form of the term weighting used in the vector space model. In fact, many different weighting schemes have been tried over the years. Most of these are variations on tf.idf weighting, which was described brie y in Chapter 2.  e term frequency component, tf, re-  ects the importance of a term in a document Di (or query).  is is usually com- puted as a normalized count of the term occurrences in a document, for example by
        tfik = fik
 j=1
where tfik is the term frequency weight of term k in document Di, and fik is the number of occurrences of term k in the document. In the vector space model, normalization is part of the cosine measure. A document collection can contain documents of many different lengths. Although normalization accounts for this to some degree, long documents can have many terms occurring once and others occurring hundreds of times. Retrieval experiments have shown that to reduce the impact of these frequent terms, it is effective to use the logarithm of the number of term occurrences in tf weights rather than the raw count.
 e inverse document frequency component (idf ) re ects the importance of the term in the collection of documents.  e more documents that a term occurs in, the less discriminating the term is between documents and, consequently, the less useful it will be in retrieval.  e typical form of this weight is
∑t
fij
242 7 RetrievalModels
idfk = log N nk
 where idfk is the inverse document frequency weight for term k, N is the number of documents in the collection, and nk is the number of documents in which term k occurs.  e form of this weight was developed by intuition and experiment, although an argument can be made that idf measures the amount of information carried by the term, as de ned in information theory (Robertson, 2004).
 e effects of these two weights are combined by multiplying them (hence the name tf.idf).  e reason for combining them this way is, once again, mostly empirical. Given this, the typical form of document term weighting in the vector space model is:
(log(fik) + 1) · log(N/nk)
dik = √ ∑t
[(log(fik) + 1.0) · log(N/nk)]2 k=1
 e form of query term weighting is essentially the same. Adding 1 to the term fre- quency component ensures that terms with frequency 1 have a non-zero weight. Note that, in this model, term weights are computed only for terms that occur in the document (or query). Given that the cosine measure normalization is incor- porated into the weights, the score for a document is computed using simply the dot product of the document and query vectors.
Although there is no explicit de nition of relevance in the vector space model, there is an implicit assumption that relevance is related to the similarity of query and document vectors. In other words, documents “closer” to the query are more likely to be relevant.  is is primarily a model of topical relevance, although fea- tures related to user relevance could be incorporated into the vector representa- tion. No assumption is made about whether relevance is binary or multivalued.
In the last chapter we described relevance feedback, a technique for query modi cation based on user-identi ed relevant documents.  is technique was  rst introduced using the vector space model.  e well-known Rocchio algorithm (Rocchio, 1971) was based on the concept of an optimal query, which maximizes the difference between the average vector representing the relevant documents and the average vector representing the non-relevant documents. Given that only limited relevance information is typically available, the most common (and effec- tive) form of the Rocchio algorithm modi es the initial weights in query vector Q to produce a new query Q′ according to
  
7.2 ProbabilisticModels 243
′1∑1∑
qj = α.qj + β.|Rel| dij − γ.|Nonrel| dij
Di ∈Rel Di ∈N onrel
where qj is the initial weight of query term j, Rel is the set of identi ed relevant documents, Nonrel is the set of non-relevant documents, |.| gives the size of a set, dij is the weight of the jth term in document i, and α, β, and γ are parame- ters that control the effect of each component. Previous studies have shown that the set of non-relevant documents is best approximated by all unseen documents (i.e., all documents not identi ed as relevant), and that reasonable values for the parameters are 8, 16, and 4 for α, β, and γ, respectively.
 is formula modi es the query term weights by adding a component based on the average weight in the relevant documents and subtracting a component based on the average weight in the non-relevant documents. Query terms with weights that are negative are dropped.  is results in a longer or expanded query because terms that occur frequently in the relevant documents but not in the orig- inal query will be added (i.e., they will have non-zero positive weights in the mod- i ed query). To restrict the amount of expansion, typically only a certain number (say, 50) of the terms with the highest average weights in the relevant documents will be added to the query.
7.2 Probabilistic Models
One of the features that a retrieval model should provide is a clear statement about the assumptions upon which it is based.  e Boolean and vector space approaches make implicit assumptions about relevance and text representation that impact the design and effectiveness of ranking algorithms.  e ideal situation would be to show that, given the assumptions, a ranking algorithm based on the retrieval model will achieve better effectiveness than any other approach. Such proofs are actually very hard to come by in information retrieval, since we are trying to for- malize a complex human activity.  e validity of a retrieval model generally has to be validated empirically, rather than theoretically.
One early theoretical statement about effectiveness, known as the Probabil- ity Ranking Principle (Robertson, 1977/1997), encouraged the development of probabilistic retrieval models, which are the dominant paradigm today.  ese models have achieved this status because probability theory is a strong founda- tion for representing and manipulating the uncertainty that is an inherent part
  
244 7 RetrievalModels
of the information retrieval process.  e Probability Ranking Principle, as origi- nally stated, is as follows:
If a reference retrieval system’s1 response to each request is a ranking of the documents in the collection in order of decreasing probability of rel- evance to the user who submitted the request, where the probabilities are estimated as accurately as possible on the basis of whatever data have been made available to the system for this purpose, the overall effectiveness of the system to its user will be the best that is obtainable on the basis of those data.
Given some assumptions, such as that the relevance of a document to a query is independent of other documents, it is possible to show that this statement is true, in the sense that ranking by probability of relevance will maximize preci- sion, which is the proportion of relevant documents, at any given rank (for exam- ple, in the top 10 documents). Unfortunately, the Probability Ranking Principle doesn’t tell us how to calculate or estimate the probability of relevance.  ere are many probabilistic retrieval models, and each one proposes a different method for estimating this probability. Most of the rest of this chapter discusses some of the most important probabilistic models.
In this section, we start with a simple probabilistic model based on treating information retrieval as a classi cation problem. We then describe a popular and effective ranking algorithm that is based on this model.
7.2.1 Information Retrieval as Classification
In any retrieval model that assumes relevance is binary, there will be two sets of documents, the relevant documents and the non-relevant documents, for each query. Given a new document, the task of a search engine could be described as deciding whether the document belongs in the relevant set or the non-relevant2 set.  at is, the system should classify the document as relevant or non-relevant, and retrieve it if it is relevant.
Given some way of calculating the probability that the document is relevant and the probability that it is non-relevant, then it would seem reasonable to clas- sify the document into the set that has the highest probability. In other words,
1 A “reference retrieval system” would now be called a search engine.
2 Note that we never talk about “irrelevant” documents in information retrieval; instead
they are “non-relevant.”
 
7.2 ProbabilisticModels 245
we would decide that a document D is relevant if P (R|D) > P (N R|D), where P(R|D) is a conditional probability representing the probability of relevance given the representation of that document, and P(NR|D) is the conditional probability of non-relevance (Figure 7.3).  is is known as the Bayes Decision Rule, and a system that classi es documents this way is called a Bayes classi er.
In Chapter 9, we discuss other applications of classi cation (such as spam  l- tering) and other classi cation techniques, but here we focus on the ranking algo- rithm that results from this probabilistic retrieval model based on classi cation.
 P(R|D)
P(NR|D)
Relevant Documents
Non Relevant  Documents
  The rain in Spain falls mainly in the plain  The rain in Spain falls mainly in the plain 
The rain in Spain falls mainly in the plain  The rain in Spain falls mainly in the plain
  Document
Fig. 7.3. Classifying a document as relevant or non-relevant
 e question that faces us now is how to compute these probabilities. To start with, let’s focus on P (R|D). It’s not clear how we would go about calculating this, but given information about the relevant set, we should be able to calcu- late P (D|R). For example, if we had information about how o en speci c words occurred in the relevant set, then, given a new document, it would be relatively straightforward to calculate how likely it would be to see the combination of words in the document occurring in the relevant set. Let’s assume that the prob- ability of the word “president” in the relevant set is 0.02, and the probability of “lincoln” is 0.03. If a new document contains the words “president” and “lincoln”, we could say that the probability of observing that combination of words in the
246 7 RetrievalModels
relevant set is 0.02 × 0.03 = 0.0006, assuming that the two words occur inde- pendently.3
So how does calculating P(D|R) get us to the probability of relevance? It turns out there is a relationship between P (R|D) and P (D|R) that is expressed by Bayes’ Rule:4
P (R|D) = P (D|R)P (R) P(D)
where P (R) is the a priori probability of relevance (in other words, how likely any document is to be relevant), and P (D) acts as a normalizing constant. Given this, we can express our decision rule in the following way: classify a document as relevant if P (D|R)P (R) > P (D|N R)P (N R).  is is the same as classifying a document as relevant if:
P(D|R) > P(NR) P (D|N R) P (R)
 e le -hand side of this equation is known as the likelihood ratio. In most clas- si cation applications, such as spam  ltering, the system must decide which class the document belongs to in order to take the appropriate action. For information retrieval, a search engine only needs to rank documents, rather than make that de- cision (which is hard). If we use the likelihood ratio as a score, the highly ranked documents will be those that have a high likelihood of belonging to the relevant set.
To calculate the document scores, we still need to decide how to come up with values for P (D|R) and P (D|N R).  e simplest approach is to make the same assumptions that we made in our earlier example; that is, we represent doc- uments as a combination of words and the relevant and non-relevant sets using word probabilities. In this model, documents are represented as a vector of binary features,D=(d1,d2,...,dt),wheredi =1iftermiispresentinthedocument, and 0 otherwise.  e other major assumption we make is term independence (also known as the Naïve Bayes assumption).  is means we can estimate P (D|R) by the product of the individual term probabilities ∏ti=1 P (di |R) (and similarly for P(D|NR)). Because this model makes the assumptions of term independence and binary features in documents, it is known as the binary independence model.
3 Given two events A and B, the joint probability P (A ∩ B) is the probability of both events occurring together. In general, P (A ∩ B) = P (A|B)P (B). If A and B are independent, this means that P (A ∩ B) = P (A)P (B).
4 Named a er  omas Bayes, a British mathematician.
    
7.2 ProbabilisticModels 247
Words obviously do not occur independently in text. If the word “Microso ” occurs in a document, it is very likely that the word “Windows” will also occur.  e assumption of term independence, however, is a common one since it usually simpli es the mathematics involved in the model. Models that allow some form of dependence between terms will be discussed later in this chapter.
Recall that a document in this model is a vector of 1s and 0s representing the presence and absence of terms. For example, if there were  ve terms indexed, one of the document representations might be (1, 0, 0, 1, 1), meaning that the doc- ument contains terms 1, 4, and 5. To calculate the probability of this document occurring in the relevant set, we need the probabilities that the terms are 1 or 0 in the relevant set. If pi is the probability that term i occurs (has the value 1) in a document from the relevant set, then the probability of our example document occurringintherelevantsetisp1 ×(1−p2)×(1−p3)×p4 ×p5. eprob- ability (1 − p2) is the probability of term 2 not occurring in the relevant set. For the non-relevant set, we use si to represent the probability of term i occurring.5
Going back to the likelihood ratio, using pi and si gives us a score of P(D|R) ∏ pi ∏ 1−pi
P(D|NR)= s· 1−s i:di=1 i i:di=0 i
   where ∏ means that it is a product over the terms that have the value 1 in i:di =1
the document. We can now do a bit of mathematical manipulation to get:
∏ pi ∏ 1−si ∏ 1−pi ∏ 1−pi s·( 1−p· 1−s)· 1−s
    i:di=1 i i:di=1 i i:di=1 i i:di=0 i ∏ pi(1−si) ∏1−pi
= si(1 − pi) · 1 − si i:di =1 i
   e second product is over all terms and is therefore the same for all documents, so we can ignore it for ranking. Since multiplying lots of small numbers can lead to problems with the accuracy of the result, we can equivalently use the logarithm of the product, which means that the scoring function is:
∑ logpi(1−si) i:di=1 si(1 − pi)
5 In many descriptions of this model, pi and qi are used for these probabilities. We use si to avoid confusion with the qi used to represent query terms.
  
248 7 RetrievalModels
You might be wondering where the query has gone, given that this is a doc- ument ranking algorithm for a speci c query. In many cases, the query provides us with the only information we have about the relevant set. We can assume that, in the absence of other information, terms that are not in the query will have the same probability of occurrence in the relevant and non-relevant documents (i.e., pi = si). In that case, the summation will only be over terms that are both in the query and in the document.  is means that, given a query, the score for a document is simply the sum of the term weights for all matching terms.
If we have no other information about the relevant set, we could make the additional assumptions that pi is a constant and that si could be estimated by using the term occurrences in the whole collection as an approximation. We make the second assumption based on the fact that the number of relevant documents is much smaller than the total number of documents in the collection. With a value of 0.5 for pi in the scoring function described earlier, this gives a term weight for term i of
0.5(1− ni ) log N
= log
N −ni ni
   ni (1 − 0.5) N
 where ni is the number of documents that contain term i, and N is the number of documents in the collection.  is shows that, in the absence of information about the relevant documents, the term weight derived from the binary independence model is very similar to an idf weight.  ere is no tf component, because the documents were assumed to have binary features.
If we do have information about term occurrences in the relevant and non- relevant sets, it can be summarized in a contingency table, shown in Table 7.1.  is information could be obtained through relevance feedback, where users identify relevant documents in initial rankings. In this table, ri is the number of relevant documents containing term i, ni is the number of documents containing term i, N is the total number of documents in the collection, and R is the number of relevant documents for this query.
Table 7.1. Contingency table of term occurrences for a particular query
    Relevant
Non-relevant
  Total
  di = 1 di = 0
   ri
R − ri
 ni − ri
N −ni −R+ri
  ni
N − ni
    Total
 R
N−R
  N
   
7.2 ProbabilisticModels 249
Given this table, the obvious estimates6 for pi and si would be pi = ri/R (the number of relevant documents that contain a term divided by the total number of relevant documents) and si = (ni − ri)/(N − R) (the number of non-relevant documents that contain a term divided by the total number of non-relevant doc- uments). Using these estimates could cause a problem, however, if some of the entries in the contingency table were zeros. If ri was zero, for example, the term weight would be log 0. To avoid this, a standard solution is to add 0.5 to each count (and 1 to the totals), which gives us estimates of pi = (ri + 0.5)/(R + 1) andsi =(ni−ri+0.5)/(N−R+1.0).Puttingtheseestimatesintothescoring function gives us:
∑ log (ri +0.5)/(R−ri +0.5) i:di=qi=1 (ni −ri +0.5)/(N −ni −R+ri +0.5)
Although this document score sums term weights for just the matching query terms, with relevance feedback the query can be expanded to include other impor- tant terms from the relevant set. Note that if we have no relevance information, we can set r and R to 0, which would give a pi value of 0.5, and would produce the idf-like term weight discussed before.
So how good is this document score when used for ranking? Not very good, it turns out. Although it does provide a method of incorporating relevance in- formation, in most cases we don’t have this information and instead would be using term weights that are similar to idf weights.  e absence of a tf compo- nent makes a signi cant difference to the effectiveness of the ranking, and most effectiveness measures will drop by about 50% if the ranking ignores this informa- tion.  is means, for example, that we might see 50% fewer relevant documents in the top ranks if we used the binary independence model ranking instead of the best tf.idf ranking.
It turns out, however, that the binary independence model is the basis for one of the most effective and popular ranking algorithms, known as BM25.7
6 We use the term estimate for a probability value calculated using data such as a contin- gency table because this value is only an estimate for the true value of the probability and would change if more data were available.
7 BM stands for Best Match, and 25 is just a numbering scheme used by Robertson and his co-workers to keep track of weighting variants (Robertson & Walker, 1994).
  
250 7 RetrievalModels
7.2.2 The BM25 Ranking Algorithm
BM25 extends the scoring function for the binary independence model to in- clude document and query term weights.  e extension is based on probabilistic arguments and experimental validation, but it is not a formal model.
BM25 has performed very well in TREC retrieval experiments and has in u- enced the ranking algorithms of commercial search engines, including web search engines.  ere are some variations of the scoring function for BM25, but the most common form is:
∑log (ri +0.5)/(R−ri +0.5) ·(k1 +1)fi ·(k2 +1)qfi i∈Q (ni −ri +0.5)/(N −ni −R+ri +0.5) K +fi k2 +qfi
where the summation is now over all terms in the query; and N, R, ni, and ri are the same as described in the last section, with the additional condition that r and R are set to zero if there is no relevance information; fi is the frequency of term i in the document; qfi is the frequency of term i in the query; and k1, k2, and K are parameters whose values are set empirically.
 e constant k1 determines how the tf component of the term weight changes as fi increases. If k1 = 0, the term frequency component would be ignored and only term presence or absence would matter. If k1 is large, the term weight compo- nent would increase nearly linearly with fi. In TREC experiments, a typical value for k1 is 1.2, which causes the effect of fi to be very non-linear, similar to the use of log f in the term weights discussed in section 7.1.2.  is means that a er three or four occurrences of a term, additional occurrences will have little impact.  e constant k2 has a similar role in the query term weight. Typical values for this pa- rameter are in the range 0 to 1,000, meaning that performance is less sensitive to k2 than it is to k1.  is is because query term frequencies are much lower and less variable than document term frequencies.
K is a more complicated parameter that normalizes the tf component by doc- ument length. Speci cally
K=k1((1−b)+b· dl ) avdl
where b is a parameter, dl is the length of the document, and avdl is the average length of a document in the collection.  e constant b regulates the impact of the length normalization, where b = 0 corresponds to no length normalization, and
    
7.2 ProbabilisticModels 251
b = 1 is full normalization. In TREC experiments, a value of b = 0.75 was found to be effective.
As an example calculation, let’s consider a query with two terms, “president” and “lincoln”, each of which occurs only once in the query (qf = 1). We will con- sider the typical case where we have no relevance information (r and R are zero). Let’s assume that we are searching a collection of 500,000 documents (N), and that in this collection, “president” occurs in 40,000 documents (n1 = 40, 000) and“lincoln”occursin300documents(n2 =300).Inthedocumentwearescor- ing (which is about President Lincoln), “president” occurs 15 times (f1 = 15) and“lincoln”occurs25times(f2 =25). edocumentlengthis90%oftheaver- agelength(dl/avdl=0.9). eparametervaluesweusearek1 =1.2,b=0.75, and k2 = 100. With these values, K = 1.2 · (0.25 + 0.75 · 0.9) = 1.11, and the document score is:
BM25(Q,D) =
log (0 + 0.5)/(0 − 0 + 0.5)
(40000 − 0 + 0.5)/(500000 − 40000 − 0 + 0 + 0.5)
×(1.2 + 1)15 × (100 + 1)1 1.11 + 15 100 + 1
   (0 + 0.5)/(0 − 0 + 0.5)
(300 − 0 + 0.5)/(500000 − 300 − 0 + 0 + 0.5)
+log
×(1.2 + 1)25 × (100 + 1)1
   1.11 + 25 100 + 1
= log 460000.5/40000.5 · 33/16.11 · 101/101 + log 499700.5/300.5 · 55/26.11 · 101/101
= 2.44 · 2.05 · 1 + 7.42 · 2.11 · 1 = 5.00 + 15.66 = 20.66
Notice the impact from the  rst part of the weight that, without relevance in- formation, is nearly the same as an idf weight (as we discussed in section 7.2.1). Because the term “lincoln” is much less frequent in the collection, it has a much higher idf component (7.42 versus 2.44). Table 7.2 gives scores for different num- bers of term occurrences.  is shows the importance of the “lincoln” term and that even one occurrence of a term can make a large difference in the score. Re- ducing the number of term occurrences from 25 or 15 to 1 makes a signi cant but
252 7 RetrievalModels
not dramatic difference.  is example also demonstrates that it is possible for a document containing a large number of occurrences of a single important term to score higher than a document containing both query terms (15.66 versus 12.74).
   Frequency of “lincoln”
    25 1 0 25 25
        Frequency of “president” 15
15
15
1
0
BM25 score 20.66 12.74 5.00 18.2 15.66
  Table 7.2. BM25 scores for an example document
 e score calculation may seem complicated, but remember that some of the calculation of term weights can occur at indexing time, before processing any query. If there is no relevance information, scoring a document simply involves adding the weights for matching query terms, with a small additional calculation if query terms occur more than once (i.e., if qf > 1). Another important point is that the parameter values for the BM25 ranking algorithm can be tuned (i.e., ad- justed to obtain the best effectiveness) for each application.  e process of tuning is described further in section 7.7 and Chapter 8.
To summarize, BM25 is an effective ranking algorithm derived from a model of information retrieval viewed as classi cation.  is model focuses on topical relevance and makes an explicit assumption that relevance is binary. In the next section, we discuss another probabilistic model that incorporates term frequency directly in the model, rather than being added in as an extension to improve per- formance.
7.3 Ranking Based on Language Models
Language models are used to represent text in a variety of language technologies, such as speech recognition, machine translation, and handwriting recognition.  e simplest form of language model, known as a unigram language model, is a probability distribution over the words in the language.  is means that the lan- guage model associates a probability of occurrence with every word in the in-
7.3 RankingBasedonLanguageModels 253
dex vocabulary for a collection. For example, if the documents in a collection contained just  ve different words, a possible language model for that collection might be (0.2, 0.1, 0.35, 0.25, 0.1), where each number is the probability of a word occurring. If we treat each document as a sequence of words, then the proba- bilities in the language model predict what the next word in the sequence will be. For example, if the  ve words in our language were “girl”, “cat”, “the”, “boy”, and “touched”, then the probabilities predict which of these words will be next.  ese words cover all the possibilities, so the probabilities must add to 1. Because this is a unigram model, the previous words have no impact on the prediction. With this model, for example, it is just as likely to get the sequence “girl cat” (probability 0.2 × 0.1) as “girl touched” (probability 0.2 × 0.1).
In applications such as speech recognition, n-gram language models that pre- dict words based on longer sequences are used. An n-gram model predicts a word based on the previous n − 1 words.  e most common n-gram models are bi- gram (predicting based on the previous word) and trigram (predicting based on the previous two words) models. Although bigram models have been used in in- formation retrieval to represent two-word phrases (see section 4.3.5), we focus our discussion on unigram models because they are simpler and have proven to be very effective as the basis for ranking algorithms.
For search applications, we use language models to represent the topical con- tent of a document. A topic is something that is talked about o en but rarely de-  ned in information retrieval discussions. In this approach, we de ne a topic as a probability distribution over words (in other words, a language model). For exam- ple, if a document is about  shing in Alaska, we would expect to see words associ- ated with  shing and locations in Alaska with high probabilities in the language model. If it is about  shing in Florida, some of the high-probability words will be the same, but there will be more high probability words associated with locations in Florida. If instead the document is about  shing games for computers, most of the high-probability words will be associated with game manufacturers and com- puter use, although there will still be some important words about  shing. Note that a topic language model, or topic model for short, contains probabilities for all words, not just the most important. Most of the words will have “default” proba- bilities that will be the same for any text, but the words that are important for the topic will have unusually high probabilities.
A language model representation of a document can be used to “generate” new text by sampling words according to the probability distribution. If we imagine the language model as a big bucket of words, where the probabilities determine
254 7 RetrievalModels
how many instances of a word are in the bucket, then we can generate text by reaching in (without looking), drawing out a word, writing it down, putting the word back in the bucket, and drawing again. Note that we are not saying that we can generate the original document by this process. In fact, because we are only using a unigram model, the generated text is going to look pretty bad, with no syntactic structure. Important words for the topic of the document will, however, appear o en. Intuitively, we are using the language model as a very approximate model for the topic the author of the document was thinking about when he was writing it.
When text is modeled as a  nite sequence of words, where at each point in the sequence there are t different possible words, this corresponds to assuming a multinomial distribution over words. Although there are alternatives, multino- mial language models are the most common in information retrieval.8 One of the limitations of multinomial models that has been pointed out is that they do not describe text burstiness well, which is the observation that once a word is “pulled out of the bucket,” it tends to be pulled out repeatedly.
In addition to representing documents as language models, we can also repre- sent the topic of the query as a language model. In this case, the intuition is that the language model is a representation of the topic that the information seeker had in mind when she was writing the query.  is leads to three obvious possi- bilities for retrieval models based on language models: one based on the proba- bility of generating the query text from a document language model, one based on generating the document text from a query language model, and one based on comparing the language models representing the query and document topics. In the next two sections, we describe these retrieval models in more detail.
7.3.1 Query Likelihood Ranking
In the query likelihood retrieval model, we rank documents by the probability that the query text could be generated by the document language model. In other words, we calculate the probability that we could pull the query words out of the “bucket” of words representing the document.  is is a model of topical relevance, in the sense that the probability of query generation is the measure of how likely it is that a document is about the same topic as the query.
Since we start with a query, we would in general like to calculate P (D|Q) to rank the documents. Using Bayes’ Rule, we can calculate this by
8 We discuss the multinomial model in the context of classi cation in Chapter 9.
 
7.3 RankingBasedonLanguageModels 255
where the symbol = , as we mentioned previously, means that the right-hand side is rank equivalent to the le -hand side (i.e., we can ignore the normalizing constant P (Q)), P (D) is the prior probability of a document, and P (Q|D) is the query likelihood given the document. In most cases, P (D) is assumed to be uniform (the same for all documents), and so will not affect the ranking. Mod- els that assign non-uniform prior probabilities based on, for example, document date or document length can be useful in some applications, but we will make the simpler uniform assumption here. Given that assumption, the retrieval model speci es ranking documents by P (Q|D), which we calculate using the unigram language model for the document
∏n i=1
where qi is a query word, and there are n words in the query.
To calculate this score, we need to have estimates for the language model prob-
abilities P (qi|D).  e obvious estimate would be P(qi|D) = fqi,D
|D|
where fqi,D is the number of times word qi occurs in document D, and |D| is the number of words in D. For a multinomial distribution, this is the maximum likelihood estimate, which means this this is the estimate that makes the observed value of fqi,D most likely.  e major problem with this estimate is that if any of the query words are missing from the document, the score given by the query like- lihood model for P (Q|D) will be zero.  is is clearly not appropriate for longer queries. For example, missing one word out of six should not produce a score of zero. We will also not be able to distinguish between documents that have differ- ent numbers of query words missing. Additionally, because we are building a topic model for a document, words associated with that topic should have some prob- ability of occurring, even if they were not mentioned in the document. For ex- ample, a language model representing a document about computer games should have some non-zero probability for the word “RPG” even if that word was not mentioned in the document. A small probability for that word will enable the document to receive a non-zero score for the query “RPG computer games”, al- though it will be lower than the score for a document that contains all three words.
rank
p(D|Q) = P (Q|D)P (D)
rank
P (Q|D) =
P (qi|D)
 
256 7 RetrievalModels
Smoothing is a technique for avoiding this estimation problem and overcom- ing data sparsity, which means that we typically do not have large amounts of text to use for the language model probability estimates.  e general approach to smoothing is to lower (or discount) the probability estimates for words that are seen in the document text, and assign that “le over” probability to the esti- mates for the words that are not seen in the text.  e estimates for unseen words are usually based on the frequency of occurrence of words in the whole document collection. If P (qi|C) is the probability for query word i in the collection language model for document collection C, then the estimate we use for an unseen word in a document is αDP(qi|C), where αD is a coefficient controlling the probability assigned to unseen words.9 In general, αD can depend on the document. In order that the probabilities sum to one, the probability estimate for a word that is seen in a document is (1 − αD)P(qi|D) + αDP(qi|C).
To make this clear, consider a simple example where there are only three words, w1, w2, and w3, in our index vocabulary. If the collection probabilities for these three words, based on maximum likelihood estimates, are 0.3, 0.5, and 0.2, and the document probabilities based on maximum likelihood estimates are 0.5, 0.5, and 0.0, then the smoothed probability estimates for the document language model are:
P(w1|D) = (1 − αD)P(w1|D) + αDP(w1|C) =(1−αD)·0.5+αD ·0.3
P(w2|D)=(1−αD)·0.5+αD ·0.5 P(w3|D)=(1−αD)·0.0+αD ·0.2=αD ·0.2
Note that term w3 has a non-zero probability estimate, even though it did not occur in the document text. If we add these three probabilities, we get
P(w1|D) + P(w2|D) + P(w3|D) = (1 − αD) · (0.5 + 0.5) +αD · (0.3 + 0.5 + 0.2)
=1−αD +αD =1
which con rms that the probabilities are consistent.
9  e collection language model probability is also known as the background language model probability, or just the background probability.
 
7.3 RankingBasedonLanguageModels 257
Different forms of estimation result from specifying the value of αD.  e sim- plestchoicewouldbetosetittoaconstant,i.e.,αD =λ. ecollectionlanguage model probability estimate we use for word qi is cqi /|C|, where cqi is the num- ber of times a query word occurs in the collection of documents, and |C| is the total number of word occurrences in the collection.  is gives us an estimate for P(qi|D)of:
p(qi|D)=(1−λ)fqi,D +λcqi |D| |C|
 is form of smoothing is known as the Jelinek-Mercer method. Substituting this estimate in the document score for the query-likelihood model gives:
P(Q|D)=
As we have said before, since multiplying many small numbers together can lead to accuracy problems, we can use logarithms to turn this score into a rank-equivalent sum as follows:
  ∏n fqi ,D cqi
  i=1
((1−λ)|D| +λ|C|)
∑n fq ,D log((1−λ) i
i=1
Small values of λ produce less smoothing, and consequently the query tends to act more like a Boolean AND since the absence of any query word will penalize the score substantially. In addition, the relative weighting of words, as measured by the maximum likelihood estimates, will be important in determining the score. As λ approaches 1, the relative weighting will be less important, and the query acts more like a Boolean OR or a coordination level match.10 In TREC evaluations, it has been shown that values of λ around 0.1 work well for short queries, whereas values around 0.7 are better for much longer queries. Short queries tend to contain only signi cant words, and a low λ value will favor documents that contain all the query words. With much longer queries, missing a word is much less important, and a high λ places more emphasis on documents that contain a number of the high-probability words.
At this point, it may occur to you that the query likelihood retrieval model doesn’t have anything that looks like a tf.idf weight, and yet experiments show
10 A coordination level match simply ranks documents by the number of matching query terms.
logP(Q|D)=
|D|
cq +λ i )
  |C|
 
258 7 RetrievalModels
that it is as least as effective as the BM25 ranking algorithm. We can, however, demonstrate a relationship to tf.idf weights by manipulating the query likelihood score in the following way:
logP(Q|D)= =
∑n fq ,D log((1−λ) i
cq +λ i )
  |D|
∑ fq,D cq ∑ cq
i=1
i:fqi,D>0 ∑
|C|
log((1−λ) i +λ i)+ log(λ i)
   |D| |C| i:fqi,D=0 |C|
((1−λ)fqi,D +λcqi) ∑n
c log(λ qi )
  =
i:fqi,D>0
log
|D| |C| λcqi
 |C|  ((1 − λ)fqi,D
+
|C| i=1
   ∑
rank |D|
 =
log λ cqi + 1 |C|
  i:fqi ,D >0
In the second line, we split the score into the words that occur in the document andthosethatdon’toccur(fqi,D =0).Inthethirdline,weadd
∑
i:fqi ,D >0
to the last term and subtract it from the  rst (where it ends up in the denomina- tor), so there is no net effect.  e last term is now the same for all documents and can be ignored for ranking.  e  nal expression gives the document score in terms of a “weight” for matching query terms. Although this weight is not identical to a tf.idf weight, there are clear similarities in that it is directly proportional to the document term frequency and inversely proportional to the collection frequency.
A different form of estimation, and one that is generally more effective, comes from using a value of αD that is dependent on document length.  is approach is known as Dirichlet smoothing, for reasons we will discuss later, and uses
αD= μ |D| + μ
where μ is a parameter whose value is set empirically. Substituting this expression for αD in (1 − αD)P(qi|D) + αDP(qi|C) results in the probability estimation formula
log(λ cqi ) |C|
  
7.3 RankingBasedonLanguageModels 259
fq ,D +μcqi p(qi|D) = i |C|
  |D| + μ which in turn leads to the following document score:
∑n logP(Q|D) =
i=1
log
fq ,D +μcqi i |C|
|D| + μ
  Similar to the Jelinek-Mercer smoothing, small values of the parameter (μ in this case) give more importance to the relative weighting of words, and large val- ues favor the number of matching terms. Typical values of μ that achieve the best results in TREC experiments are in the range 1,000 to 2,000 (remember that col- lection probabilities are very small), and Dirichlet smoothing is generally more effective than Jelinek-Mercer, especially for the short queries that are common in most search applications.
So where does Dirichlet smoothing come from? It turns out that a Dirichlet distribution11 is the natural way to specify prior knowledge when estimating the probabilities in a multinomial distribution.  e process of Bayesian estimation determines probability estimates based on this prior knowledge and the observed text.  e resulting probability estimate can be viewed as combining actual word counts from the text with pseudo-counts from the Dirichlet distribution. If we had no text, the probability estimate for term qi would be μ(cqi /|C|)/μ, which is a reasonable guess based on the collection.  e more text we have (i.e., for longer documents), the less in uence the prior knowledge will have.
We can demonstrate the calculation of query likelihood document scores us- ing the example given in section 7.2.2.  e two query terms are “president” and “lincoln”. For the term “president”, fqi ,D = 15, and let’s assume that cqi = 160,000. For the term “lincoln”, fqi ,D = 25, and we will assume that cqi = 2,400.  e num- ber of word occurrences in the document |d| is assumed to be 1,800, and the num- ber of word occurrences in the collection is 109 (500,000 documents times an average of 2,000 words).  e value of μ used is 2,000. Given these numbers, the score for the document is:
11 Named a er the German mathematician Johann Peter Gustav Lejeune Dirichlet (the  rst name used seems to vary).
 
260 7 RetrievalModels
QL(Q, D) = log 15 + 2000 × (1.6 × 105/109) 1800 + 2000
+ log 25 + 2000 × (2400/109) 1800 + 2000
= log(15.32/3800) + log(25.005/3800) = −5.51 + −5.02 = −10.53
A negative number? Remember that we are taking logarithms of probabilities in this scoring function, and the probabilities of word occurrence are small.  e im- portant issue is the effectiveness of the rankings produced using these scores. Ta- ble 7.3 shows the query likelihood scores for the same variations of term occur- rences that were used in Table 7.2. Although the scores look very different for BM25 and QL, the rankings are similar, with the exception that the document containing 15 occurrences of “president” and 1 of “lincoln” is ranked higher than the document containing 0 occurrences of “president” and 25 occurrences of “lin- coln” in the QL scores, whereas the reverse is true for BM25.
     Frequency of “lincoln”
  25 1 0 25 25
Frequency of “president”
QL score
 15     –10.53 15     –13.75 15     –19.05
1     –12.99 0     –14.40
Table 7.3. Query likelihood scores for an example document
To summarize, query likelihood is a simple probabilistic retrieval model that directly incorporates term frequency.  e problem of coming up with effective term weights is replaced by probability estimation, which is better understood and has a formal basis.  e basic query likelihood score with Dirichlet smooth- ing has similar effectiveness to BM25, although it does do better on most TREC collections. If more sophisticated smoothing based on topic models is used (de- scribed further in section 7.6), query likelihood consistently outperforms BM25.  is means that instead of smoothing using the collection probabilities for words, we instead use word probabilities from similar documents.
 e simplicity of the language model framework, combined with the ability to describe a variety of retrieval applications and the effectiveness of the associated
 
7.3 RankingBasedonLanguageModels 261
ranking algorithms, make this approach a good choice for a retrieval model based on topical relevance.
7.3.2 Relevance Models and Pseudo-Relevance Feedback
Although the basic query likelihood model has a number of advantages, it is lim- ited in terms of how it models information needs and queries. It is difficult, for example, to incorporate information about relevant documents into the ranking algorithm, or to represent the fact that a query is just one of many possible queries that could be used to describe a particular information need. In this section, we show how this can be done by extending the basic model.
In the introduction to section 7.3, we mentioned that it is possible to represent the topic of a query as a language model. Instead of calling this the query language model, we use the name relevance model since it represents the topic covered by relevant documents.  e query can be viewed as a very small sample of text gener- ated from the relevance model, and relevant documents are much larger samples of text from the same model. Given some examples of relevant documents for a query, we could estimate the probabilities in the relevance model and then use this model to predict the relevance of new documents. In fact, this is a version of the classi cation model presented in section 7.2.1, where we interpret P (D|R) as the probability of generating the text in a document given a relevance model.  is is also called the document likelihood model. Although this model, unlike the bi- nary independence model, directly incorporates term frequency, it turns out that P (D|R) is difficult to calculate and compare across documents.  is is because documents contain a large and extremely variable number of words compared to a query. Consider two documents Da and Db, for example, containing 5 and 500 words respectively. Because of the large difference in the number of words involved, the comparison of P(Da|R) and P(Db|R) for ranking will be more difficult than comparing P (Q|Da) and P (Q|Db), which use the same query and smoothed representations for the documents. In addition, we still have the prob- lem of obtaining examples of relevant documents.
 ere is, however, another alternative. If we can estimate a relevance model from a query, we can compare this language model directly with the model for a document. Documents would then be ranked by the similarity of the document model to the relevance model. A document with a model that is very similar to the relevance model is likely to be on the same topic.  e obvious next question is how to compare two language models. A well-known measure from probabil- ity theory and information theory, the Kullback-Leibler divergence (referred to as
262 7 RetrievalModels
KL-divergence in this book),12 measures the difference between two probability distributions. Given the true probability distribution P and another distribution Q that is an approximation to P , the KL divergence is de ned as:
K L(P ||Q) =
Since KL-divergence is always positive and is larger for distributions that are fur- ther apart, we use the negative KL-divergence as the basis for the ranking function (i.e., smaller differences mean higher scores). In addition, KL-divergence is not symmetric, and it matters which distribution we pick as the true distribution. If we assume the true distribution to be the relevance model for the query (R) and the approximation to be the document language model (D), then the negative KL-divergence can be expressed as
∑∑
w∈V
P (w|R) log P (w|D) − P (w|R) log P (w|R) w∈V
∑ P(x)
 x
P (x) log Q(x)
where the summation is over all words w in the vocabulary V .  e second term on the right-hand side of this equation does not depend on the document, and can be ignored for ranking. Given a simple maximum likelihood estimate for P (w|R), based on the frequency in the query text (fw,Q) and the number of words in the query (|Q|), the score for a document will be:
∑ fw,Q log P (w|D) w∈V |Q|
Although this summation is over all words in the vocabulary, words that do not occur in the query have a zero maximum likelihood estimate and will not con- tribute to the score. Also, query words with frequency k will contribute k × log P (w|D) to the score.  is means that this score is rank equivalent to the query likelihood score described in the previous section. In other words, query likelihood is a special case of a retrieval model that ranks by comparing a rele- vance model based on a query to a document language model.
 e advantage of the more general model is that it is not restricted to the sim- ple method of estimating the relevance model using query term frequencies. If we
12 KL-divergence is also called information divergence, information gain, or relative en- tropy.
  
7.3 RankingBasedonLanguageModels 263
regard the query words as a sample from the relevance model, then it seems rea- sonable to base the probability of a new sample word on the query words we have seen. In other words, the probability of pulling a word w out of the “bucket” rep- resenting the relevance model should depend on the n query words we have just pulled out. More formally, we can relate the probability of w to the conditional probability of observing w given that we just observed the query words q1 . . . qn by the approximation:
P(w|R) ≈ P(w|q1 ...qn)
By de nition, we can express the conditional probability in terms of the joint
probability of observing w with the query words: P(w|R) ≈ P(w,q1 ...qn)
P(q1 ...qn)
P (q1 . . . qn) is a normalizing constant and is calculated as:
∑
w∈V
Now the question is how to estimate the joint probability P (w, q1 . . . qn). Given a set of documents C represented by language models, we can calculate the joint probability as follows:
∑
P(w,q1 ...qn) =
We can also make the assumption that:
P(w,q1 ...qn|D) = P(w|D)
When we substitute this expression for P (w, q1 . . . qn|D) into the previous equa-
∑ ∏n
P(w,q1 ...qn) = P(D)P(w|D) P(qi|D)
D∈C i=1
How do we interpret this formula?  e prior probability P(D) is usually as- sumedtobeuniformandcanbeignored. eexpression∏ni=1 P(qi|D)is,infact,
 P(q1 ...qn) =
P(w,q1 ...qn)
P(qi|D) tion, we get the following estimate for the joint probability:
D∈C
p(D)P(w,q1 ...qn|D)
∏n i=1
264 7 RetrievalModels
the query likelihood score for the document D.  is means that the estimate for P (w, q1 . . . qn) is simply a weighted average of the language model probabilities for w in a set of documents, where the weights are the query likelihood scores for those documents.
Ranking based on relevance models actually requires two passes.  e  rst pass ranks documents using query likelihood to obtain the weights that are needed for relevance model estimation. In the second pass, we use KL-divergence to rank documents by comparing the relevance model and the document model. Note also that we are in effect adding words to the query by smoothing the relevance model using documents that are similar to the query. Many words that had zero probabilities in the relevance model based on query frequency estimates will now have non-zero values. What we are describing here is exactly the pseudo-relevance feedback process described in section 6.2.4. In other words, relevance models pro- vide a formal retrieval model for pseudo-relevance feedback and query expansion.  e following is a summary of the steps involved in ranking using relevance mod- els:
1. Rank documents using the query likelihood score for query Q.
2. Select some number of the top-ranked documents to be the set C.
3. Calculate the relevance model probabilities P (w|R) using the estimate for
P(w,q1 ...qn).
4. Rank documents again using the KL-divergence score:13
∑
P (w|R) log P (w|D) w
Some of these steps require further explanation. In steps 1 and 4, the docu- ment language model probabilities (P (w|D)) should be estimated using Dirich- let smoothing. In step 2, the model allows the set C to be the whole collection, but because low-ranked documents have little effect on the estimation of P (w|R), usually only 10–50 of the top-ranked documents are used.  is also makes the computation of P (w|R) substantially faster.
For similar reasons, the summation in step 4 is not done over all words in the vocabulary. Typically only a small number (10–25) of the highest-probability words are used. In addition, the importance of the original query words is em- phasized by combining the original query frequency estimates with the relevance
13 More accurately, this score is the negative cross entropy because we removed the term
P (w|R) log P (w|R).
 ∑
w∈V
7.3 RankingBasedonLanguageModels 265
model estimates using a similar approach to Jelinek-Mercer, i.e., λP (w|Q) + (1 − λ)P (w|R), where λ is a mixture parameter whose value is determined empiri- cally (0.5 is a typical value for TREC experiments).  is combination makes it clear that estimating relevance models is basically a process for query expansion and smoothing.
 e next important question, as for all retrieval models, is how well it works. Based on TREC experiments, ranking using relevance models is one of the best pseudo-relevance feedback techniques. In addition, relevance models produce a signi cant improvement in effectiveness compared to query likelihood ranking averaged over a number of queries. Like all current pseudo-relevance feedback techniques, however, the improvements are not consistent, and some queries can produce worse rankings or strange results.
Tables7.4and7.5showthe16highest-probabilitywordsfromrelevancemod- els estimated using this technique with some example queries and a large collec- tion of TREC news stories from the 1990s.14 Table 7.4 uses the top 10 documents from the query likelihood ranking to construct the relevance model, whereas Ta- ble 7.5 uses the top 50 documents.
 e  rst thing to notice is that, although the words are reasonable, they are very dependent on the collection of documents that is used. In the TREC news collection, for example, many of the stories that mention Abraham Lincoln are on the topic of the Lincoln Bedroom in the White House, which President Clinton used for guests and President Lincoln used as an office during the Civil War.  ese types of stories are re ected in the top probability words for the queries “president lincoln” and “abraham lincoln”. Expanding the query using these words would clearly favor the retrieval of this type of story rather than more general biographies of Lincoln.  e second observation is that there is not much difference between the words based on 10 documents and the words based on 50 documents.  e words based on 50 documents are, however, somewhat more general because the larger set of documents contains a greater variety of topics. In the case of the query “tropical  sh”, the relevance model words based on 10 documents are clearly more related to the topic.
In summary, ranking by comparing a model of the query to a model of the document using KL-divergence is a generalization of query likelihood scoring.
14  is is a considerably larger collection than was used to generate the term association tables in Chapter 6.  ose tables were based on the ROBUST track data, which con- sists of just over half a million documents.  ese tables were generated using all the TREC news collections, which total more than six million documents.
 
266 7 RetrievalModels
president lincoln
lincoln president room bedroom house white america guest serve bed washington old office war long abraham
tropical  sh
 sh tropic japan aquarium water species aquatic fair china coral source tank reef animal tarpon  shery
  abraham lincoln
   shing
 lincoln america president faith guest abraham new room christian history public bedroom war politics old national
                  sh farm salmon new wild water caught catch tag time eat raise city people  shermen boat
                                Table 7.4. Highest-probability terms from relevance model for four example queries (es- timated using top 10 documents)
 is generalization allows for more accurate queries that re ect the relative im- portance of the words in the topic that the information seeker had in mind when he was writing the query. Relevance model estimation is an effective pseudo- relevance feedback technique based on the formal framework of language mod- els, but as with all these techniques, caution must be used in applying relevance model–based query expansion to a speci c retrieval application.
Language models provide a formal but straightforward method of describing retrieval models based on topical relevance. Even more sophisticated models can be developed by incorporating term dependence and phrases, for example. Top- ical relevance is, however, only part of what is needed for effective search. In the next section, we focus on a retrieval model for combining all the pieces of evidence that contribute to user relevance, which is what people who use a search engine really care about.
  abraham lincoln
   shing
 lincoln president america abraham war man civil new history two room booth time politics public guest
                  sh water catch reef  shermen river
new year time bass boat world farm angle  y trout
                              president lincoln
lincoln president america new national great white war washington clinton house history time center kennedy room
7.4 ComplexQueriesandCombiningEvidence 267
tropical  sh
 sh tropic water storm species boat sea river country tuna world million state time japan mile
  Table 7.5. Highest-probability terms from relevance model for four example queries (es- timated using top 50 documents)
7.4 Complex Queries and Combining Evidence
Effective retrieval requires the combination of many pieces of evidence about a document’s potential relevance. In the case of the retrieval models described in previous sections, the evidence consists of word occurrences that re ect top- ical content. In general, however, there can be many other types of evidence that should be considered. Even considering words, we may want to take into account whether certain words occur near each other, whether words occur in particular document structures, such as section headings or titles, or whether words are re- lated to each other. In addition, evidence such as the date of publication, the doc- ument type, or, in the case of web search, the PageRank number will also be im- portant. Although a retrieval algorithm such as query likelihood or BM25 could be extended to include some of these types of evidence, it is difficult not to resort to heuristic “ xes” that make the retrieval algorithm difficult to tune and adapt to new retrieval applications. Instead, what we really need is a framework where we can describe the different types of evidence, their relative importance, and how they should be combined.  e inference network retrieval model, which has been
268 7 RetrievalModels
used in both commercial and open source search engines (and is incorporated in Galago), is one approach to doing this.
 e inference network model is based on the formalism of Bayesian networks and is a probabilistic model.  e model provides a mechanism for de ning and evaluating operators in a query language. Some of these operators are used to spec- ify types of evidence, and others describe how it should be combined.  e version of the inference network we will describe uses language models to estimate the probabilities that are needed to evaluate the queries.
In this section, we  rst give an overview of the inference network model, and then show how that model is used as the basis of a powerful query language for search applications. In the next section, we describe web search and explain how the inference network model would be used to combine the many sources of evi- dence required for effective ranking.
Queries described using the inference network query language appear to be much more complicated than a simple text query with two or three words. Most users will not understand this language, just as most relational database users do not understand Structured Query Language (SQL). Instead, applications trans- late simple user queries into more complex inference network versions.  e more complex query incorporates additional features and weights that re ect the best combination of evidence for effective ranking.  is point will become clearer as we discuss examples in the next two sections.
7.4.1 The Inference Network Model
A Bayesian network is a probabilistic model that is used to specify a set of events and the dependencies between them.  e networks are directed, acyclic graphs (DAGs), where the nodes in the graph represent events with a set of possible outcomes and arcs represent probabilistic dependencies between the events.  e probability, or belief,15 of a particular event outcome can be determined given the probabilities of the parent events (or a prior probability in the case of a root node). When used as a retrieval model, the nodes represent events such as observ- ing a particular document, or a particular piece of evidence, or some combination of pieces of evidence.  ese events are all binary, meaning that TRUE and FALSE are the only possible outcomes.
15 Belief network is the name for a range of techniques used to model uncertainty. A Bayesian network is a probabilistic belief network.
 
7.4 ComplexQueriesandCombiningEvidence 269
  D
 title
 title  body
 body
       h1  h1
                    r1...rN r1...rN r1...rN
    q1
q2
   I
Fig. 7.4. Example inference network model
Figure 7.4 shows an inference net where the evidence being combined are words in a web page’s title, body, and <h1> headings. In this  gure, D is a docu- ment node.  is node corresponds to the event that a document (the web page) is observed.  ere is one document node for every document in the collection, and we assume that only one document is observed at any time.  e ri or represen- tation nodes are document features (evidence), and the probabilities associated with those features are based on language models θ estimated using the parame- ters μ.  ere is one language model for each signi cant document structure (title, body, or headings). In addition to features based on word occurrence, ri nodes also represent proximity features. Proximity features take a number of different forms, such as requiring words to co-occur within a certain “window” (length) of text, and will be described in detail in the next section. Features that are not based on language models, such as document date, are allowed but not shown in this example.
 e query nodes qi are used to combine evidence from representation nodes and other query nodes.  ese nodes represent the occurrence of more complex ev- idence and document features. A number of forms of combination are available, with Boolean AND and OR being two of the simplest.  e network as a whole computes P (I |D, μ), which is the probability that an information need is met
270 7 RetrievalModels
given the document and the parameters μ.  e information need node I is a spe- cial query node that combines all of the evidence from the other query nodes into a single probability or belief score.  is score is used to rank documents. Con- ceptually, this means we must evaluate an inference network for every document in the collection, but as with every other ranking algorithm, indexes are used to speed up the computation. In general, representation nodes are indexed, whereas query nodes are speci ed for each query by the user or search application.  is means that indexes for a variety of proximity features, in addition to words, will be created (as described in Chapter 5), signi cantly expanding the size of the indexes. In some applications, the probabilities associated with proximity features are com- puted at query time in order to provide more  exibility in specifying queries.
 e connections in the inference network graph are de ned by the query and the representation nodes connected to every document in the collection.  e probabilities for the representation nodes are estimated using language models for each document. Note that these nodes do not represent the occurrence of a particular feature in a document, but instead capture the probability that the fea- ture is characteristic of the document, in the sense that the language model could generate it. For example, a node for the word “lincoln” represents the binary event that a document is about that topic (or not), and the language model for the doc- ument is used to calculate the probability of that event being TRUE.
Since all the events in the inference network are binary, we cannot really use a multinomial model of a document as a sequence of words. Instead, we use a multiple-Bernoulli16 model, which is the basis for the binary independence model in section 7.2.1. In that case, a document is represented as a binary feature vec- tor, which simply records whether a feature is present or not. In order to capture term frequency information, a different multiple-Bernoulli model is used where the document is represented by a multiset17 of vectors, with one vector for each term occurrence (Metzler, Lavrenko, & Cro , 2004). It turns out that with the appropriate choice of parameters, the probability estimate based on the multiple- Bernoulli distribution is the same as the estimate for the multinomial distribution with Dirichlet smoothing, which is
16 Nameda ertheSwissmathematicianJakobBernoulli(alsoknownasJamesorJacques, and one of eight famous mathematicians in the same family).  e multiple-Bernoulli model is discussed further in Chapter 9.
17 A multiset (also called a bag) is a set where each member has an associated number recording the number of times it occurs.
 
7.4 ComplexQueriesandCombiningEvidence 271
P(ri|D,μ)= fri,D +μP(ri|C) |D| + μ
where fi,D is the number of times feature ri occurs in document D, P(ri|C) is the collection probability for feature ri, and μ is the Dirichlet smoothing param- eter. To be more precise, for the model shown in Figure 7.4 we would use fi,D counts, collection probabilities, and a value for μ that are speci c to the docu- ment structure of interest. For example, if fi,D was the number of times feature ri occurs in a document title, the collection probabilities would be estimated from the collection of all title texts, and the μ parameter would be speci c to titles. Also note that the same estimation formula is used for proximity-based features as for words. For example, for a feature such as “New York” where the words must occur next to each other, fi,D is the number of times “New York” occurs in the text.
 e query nodes, which specify how to combine evidence, are the basis of the operators in the query language. Although Bayesian networks permit arbitrary combinations (constrained by the laws of probability), the inference network re- trieval model is based on operators that can be computed efficiently. At each node in the network, we need to specify the probability of each outcome given all pos- sible states of the parent nodes. When the number of parent nodes is large, this could clearly get expensive. Fortunately, many of the interesting combinations can be expressed as simple formulas.
As an example of the combination process and how it can be done efficiently, consider Boolean AND. Given a simple network for a query node q with two par- ent nodes a and b, as shown in Figure 7.5, we can describe the conditional prob- abilities as shown in Table 7.6.
  ab
q
   Fig. 7.5. Inference network with three nodes
We can refer to the values in the  rst column of Table 7.6 using pij, where i and j refer to the states of the parents. For example, p10 refers to the probability
272 7 RetrievalModels
P (q = TRUE|a, b) b
0 FALSE 0 TRUE 0 FALSE 1 TRUE
Table 7.6. Conditional probabilities for example network
that q is TRUE given that a is TRUE and b is FALSE. To compute the probability of q, we use this table and the probabilities of the parent nodes (which come from the representation nodes) as follows:
beland(q) = p00P (a = FALSE)P (b = FALSE) +p01P (a = FALSE)P (b = TRUE) +p10P (a = TRUE)P (b = FALSE)
+p11P(a = TRUE)P(b = TRUE) =0·(1−pa)(1−pb)+0·(1−pa)pb +0·pa(1−pb)+1·papb = papb
where pa is the probability that a is true, and pb is the probability that b is true. We use the name beland(q) to indicate that this is the belief value (probability) that results from an AND combination.
 is means that the AND combination of evidence is computed by simply mul- tiplying the probabilities of the parent nodes. If one of the parent probabilities is low (or zero if smoothing is not used), then the combination will have a low probability.  is seems reasonable for this type of combination. We can de ne a number of other combination operators in the same way. If a q node has n par- ents with probability of being true pi, then the following list de nes the common operators:
belnot(q) = 1 − p1
   a
  FALSE FALSE TRUE TRUE
        belor(q) = 1 − ∏n
beland(q) =
pi
∏n
(1 − pi)
i
i
7.4 ComplexQueriesandCombiningEvidence 273
i
∏n
i
pwti
belmax(q) = max{p1, p2, . . . , pn}
belwand(q) =
∑ ni p i belsum(q) = n
∑ni wtipi belwsum(q) = ∑ni wti
where wti is a weight associated with the ith parent, which indicates the relative importanceofthatevidence.NotethatNOTisaunaryoperator(i.e.,hasonlyone parent).
 e weighted AND operator is very important and one of the most commonly used in the query language described in the next section. Using this form of com- bination and restricting the evidence (representation nodes) to individual words gives the same ranking as query likelihood.
Given this description of the underlying model and combination operators, we can now de ne a query language that can be used in a search engine to produce rankings based on complex combinations of evidence.
7.4.2 The Galago Query Language
 e Galago query language presented here is similar to query languages used in open source search engines that are based on the inference network retrieval model.18  is version focuses on the most useful aspects of those languages for a variety of search applications, and adds the ability to use arbitrary features. Note that the Galago search engine is not based on a speci c retrieval model, but in- stead provides an efficient framework for implementing retrieval models.
Although the query language can easily handle simple unstructured text docu- ments, many of the more interesting features make use of evidence based on docu- ment structure. We assume that structure is speci ed using tag pairs, as in HTML or XML. Consider the following document:
<html>
<head>
<title>Department Descriptions</title> </head>
18 Such as Inquery and Indri.
   
274
7 RetrievalModels
<body>
The following list describes ... <h1>Agriculture</h1> ... <h1>Chemistry</h1> ... <h1>Computer Science</h1> ... <h1>Electrical Engineering</h1> ... </body>
</html>
In the Galago query language, a document is viewed as a sequence of text that may contain arbitrary tags. In the example just shown, the document consists of text marked up with HTML tags.
For each tag type T within a document (e.g., title, body, h1, etc.), we de ne the context19 of T to be all of the text and tags that appear within tags of type T. In the example, all of the text and tags appearing between <body> and </body> tags de ne the body context. A single context is generated for each unique tag name.  erefore, a context de nes a subdocument. Note that because of nested tags, certain word occurrences may appear in many contexts. It is also the case that there may be nested contexts. For example, within the <body> context there is a nested <h1> context made up of all of the text and tags that appear within the body context and within <h1> and </h1> tags. Here are the tags for the title, h1, and body contexts in this example document:
title context:
<title>Department Descriptions</title>
h1 context:
<h1>Agriculture</h1> <h1>Chemistry</h1> ... <h1>Computer Science</h1> ... <h1>Electrical Engineering</h1> ...
body context:
<body> The following list describes ... <h1>Agriculture</h1> ... <h1>Chemistry</h1> ...
19 Contexts are sometimes referred to as  elds.
 
7.4 ComplexQueriesandCombiningEvidence 275
<h1>Computer Science</h1> ... <h1>Electrical Engineering</h1> ... </body>
Each context is made up of one or more extents. An extent is a sequence of text that appears within a single begin/end tag pair of the same type as the con- text. For this example, in the <h1> context, there are extents <h1>Agriculture</h1>, <h1>Chemistry<h1>, etc. Both the title and body contexts contain only a single ex- tent because there is only a single pair of <title> and <body> tags, respectively.  e number of extents for a given tag type is determined by the number of tag pairs of that type that occur within the document.
In addition to the structure de ned when a document is created, contexts are also used to represent structure added by feature extraction tools. For example, dates, people’s names, and addresses can be identi ed in text and tagged by a fea- ture extraction tool. As long as this information is represented using tag pairs, it can be referred to in the query language in the same way as other document struc- tures.
Terms are the basic building blocks of the query language, and correspond to representation nodes in the inference network model. A variety of types of terms can be de ned, such as simple terms, ordered and unordered phrases, synonyms, and others. In addition, there are a number of options that can be used to specify that a term should appear within a certain context, or that it should be scored using a language model that is estimated using a given context.
Simple terms:
term – term that will be normalized and stemmed. ”term” – term is not normalized or stemmed. Examples:
presidents
”NASA”
Proximity terms:
#od:N( ... ) – ordered window – terms must appear ordered, with at most N-1 terms between each.
#od( ... ) – unlimited ordered window – all terms must appear ordered anywhere within current context.
#uw:N( ... ) – unordered window – all terms must appear within a window of length N in any order.
276
7 RetrievalModels
#uw( ... ) – unlimited unordered window – all terms must appear within current context in any order.
Examples:
#od:1(white house) – matches “white house” as an exact phrase. #od:2(white house) – matches “white * house” (where * is any word or null). #uw:2(white house) – matches “white house” and “house white”.
Synonyms:
#syn( ... )
#wsyn( ... )
 e  rst two expressions are equivalent.  ey each treat all of the terms listed as synonyms.  e #wsyn operator treats the terms as synonyms, and allows weights to be assigned to each term.  e arguments given to these operators can only be simple terms or proximity terms.
Examples:
#syn(dogcanine) –simplesynonymbasedontwoterms.
#syn( #od:1(united states) #od:1(united states of america) ) – creates a syn- onym from two proximity terms.
#wsyn( 1.0 donald 0.8 don 0.5 donnie ) – weighted synonym indicating relative importance of terms.
Anonymous terms:
#any:.() – used to match extent types
Examples:
#any:person() – matches any occurrence of a person extent.
#od:1(lincoln died in #any:date()) – matches exact phrases of the form:“lincoln died in <date>...</date>”.
Context restriction and evaluation:
expression.C1„...,CN – matches when the expression appears in all con- texts C1 through CN.
expression.(C1,...,CN) – evaluates the expression using the language model de ned by the concatenation of contexts C1...CN within the document. Examples:
dog.title – matches the term “dog” appearing in a title extent.
#uw(smith jones).author – matches when the two names “smith” and “jones” appear in an author extent.
dog.(title) – evaluates the term based on the title language model for the
7.4 ComplexQueriesandCombiningEvidence 277
document.  is means that the estimate of the probability of occurrence for dog for a given document will be based on the number of times that the word occurs in the title  eld for that document and will be normalized us- ing the number of words in the title rather than the document. Similarly, smoothing is done using the probabilities of occurrence in the title  eld over the whole collection.
#od:1(abraham lincoln).person.(header) – builds a language model from all of the “header” text in the document and evaluates #od:1(abraham lin- coln).person in that context (i.e., matches only the exact phrase appearing within a person extent within the header context).
Belief operators are used to combine evidence about terms, phrases, etc.  ere are both unweighted and weighted belief operators. With the weighted operator, the relative importance of the evidence can be speci ed.  is allows control over how much each expression within the query impacts the  nal score.  e  lter operator is used to screen out documents that do not contain required evidence. All belief operators can be nested.
Belief operators:
#combine(...) – this operator is a normalized version of the beland(q) op- erator in the inference network model. See the discussion later for more details.
#weight(...) –thisisanormalizedversionofthebelwand(q)operator. #filter(...) – this operator is similar to #combine, but with the difference that the document must contain at least one instance of all terms (simple, proximity, synonym, etc.).  e evaluation of nested belief operators is not changed.
Examples:
#combine( #syn(dog canine) training ) – rank by two terms, one of which is a synonym.
#combine( biography #syn(#od:1(president lincoln) #od:1(abraham lincoln)) ) – rank using two terms, one of which is a synonym of “president lincoln” and “abraham lincoln”.
#weight( 1.0 #od:1(civil war) 3.0 lincoln 2.0 speech ) – rank using three terms, and weight the term “lincoln” as most important, followed by “speech”, then “civil war”.
#filter( aquarium #combine(tropical fish) ) – consider only those documents containing the word “aquarium” and “tropical” or “ sh”, and rank them
278 7 RetrievalModels
according to the query #combine(aquarium #combine(tropical fish)). #filter( #od:1(john smith).author) #weight( 2.0 europe 1.0 travel ) – rank documents about “europe” or “travel” that have “John Smith” in the au- thor context.
As we just described, the #combine and #weight operators are normalized ver- sions of the beland and belwand operators, respectively.  e beliefs of these oper- ators are computed as follows:
∏n belcombine = p1/n
i i
∏n ∑n pwti/ i′ wti′
i i
 is normalization is done in order to make the operators behave more like the original belsum and belwsum operators, which are both normalized. One advan- tage of the normalization is that it allows us to describe the belief computa- tion of these operators in terms of various types of means (averages). For exam- ple, belsum computes the arithmetic mean over the beliefs of the parent nodes, whereas belwsum computes a weighted arithmetic mean. Similarly, belcombine and belwand compute a geometric mean and weighted geometric mean, respectively.
 e  lter operator also could be used with numeric and date  eld operators so that non-textual evidence can be combined into the score. For example, the query
#filter(news.doctype #dateafter(12/31/1999).docdate
#uw:20( brown.person #any:company() #syn( money cash payment ) )
ranks documents that are news stories, that appeared a er 1999, and that con- tained at least one text segment of length 20 that mentioned a person named “brown”, a company name, and at least one of the three words dealing with money.  e inference network model can easily deal with the combination of this type of evidence, but for simplicity, we have not implemented these operators in Galago.
Another part of the inference network model that we do support in the Galago query language is document priors. Document priors allow the speci cation of a prior probability over the documents in a collection.  ese prior probabilities in uence the rankings by preferring documents with certain characteristics, such as those that were written recently or are short.
belweight =
Prior:
#prior:name() – uses the document prior speci ed by the name given. Pri- ors are  les or functions that provide prior probabilities for each docu- ment.
Example:
#combine(#prior:recent() global warming) – uses a prior named recent to give greater weight to documents that were published more recently.
As a more detailed example of the use of this query language, in the next sec- tion we discuss web search and the types of evidence that have to be combined for effective ranking.  e use of the #feature operator to de ne arbitrary features (new evidence) is discussed in Chapter 11.
7.5 Web Search
Measured in terms of popularity, web search is clearly the most important search application. Millions of people use web search engines every day to carry out an enormous variety of tasks, from shopping to research. Given its importance, web search is the obvious example to use for explaining how the retrieval models we have discussed are applied in practice.
 ere are some major differences between web search and an application that provides search for a collection of news stories, for example.  e primary ones are the size of the collection (billions of documents), the connections between doc- uments (i.e., links), the range of document types, the volume of queries (tens of millions per day), and the types of queries. Some of these issues we have discussed in previous chapters, and others, such as the impact of spam, will be discussed later. In this section, we will focus on the features of the queries and documents that are most important for the ranking algorithm.
 ere are a number of different types of search in a web environment. One popular way of describing searches was suggested by Broder (2002). In this tax- onomy, searches are either informational, navigational, or transactional. An in- formational search has the goal of  nding information about some topic that may be on one or more web pages. Since every search is looking for some type of in- formation, we call these topical searches in this book. A navigational search has the goal of  nding a particular web page that the user has either seen before or
7.5 WebSearch 279
280 7 RetrievalModels
assumes must exist.20 A transactional search has the goal of  nding a site where a task such as shopping or downloading music can be performed. Each type of search has an information need associated with it, but a different type of infor- mation need. Retrieval models based on topical relevance have focused primarily on the  rst type of information need (and search). To produce effective rankings for the other types of searches, a retrieval model that can combine evidence related to user relevance is required.
Commercial web search engines incorporate hundreds of features (types of ev- idence) in their ranking algorithms, many derived from the huge collection of user interaction data in the query logs.  ese can be broadly categorized into features relating to page content, page metadata, anchor text, links (e.g., PageRank), and user behavior. Although anchor text is derived from the links in a page, it is used in a different way than features that come from an analysis of the link structure of pages, and so is put into a separate category. Page metadata refers to informa- tion about a page that is not part of the content of the page, such as its “age,” how o en it is updated, the URL of the page, the domain name of its site, and the amount of text content in the page relative to other material, such as images and advertisements.
It is interesting to note that understanding the relative importance of these features and how they can be manipulated to obtain better search rankings for a web page is the basis of search engine optimization (SEO). A search engine op- timizer may, for example, improve the text used in the title tag of the web page, improve the text in heading tags, make sure that the domain name and URL con- tain important keywords, and try to improve the anchor text and link structure related to the page. Some of these techniques are not viewed as appropriate by the web search engine companies, and will be discussed further in section 9.1.5.
In the TREC environment, retrieval models have been compared using test collections of web pages and a mixture of query types.  e features related to user behavior and some of the page metadata features, such as frequency of update, are not available in the TREC data. Of the other features, the most important for navigational searches are the text in the title, body, and heading (h1, h2, h3, and h4) parts of the document; the anchor text of all links pointing to the document; the PageRank number; and the inlink count (number of links pointing to the page).
20 In the TREC world, navigational searches are called home-page and named-page searches.Topicalsearchesarecalledadhocsearches.Navigationalsearchesaresimilarto known-item searches, which have been discussed in the information retrieval literature for many years.
 
7.5 WebSearch 281
Note that we are not saying that other features do not affect the ranking in web search engines, just that these were the ones that had the most signi cant impact in TREC experiments.
Given the size of the Web, many pages will contain all the query terms. Some ranking algorithms rank only those pages which, in effect,  lters the results us- ing a Boolean AND.  is can cause problems if only a subset of the Web is used (such as in a site search application) and is particularly risky with topical searches. For example, only about 50% of the pages judged relevant in the TREC topical web searches contain all the query terms. Instead of  ltering, the ranking algo- rithm should strongly favor pages that contain all query terms. In addition, term proximity will be important.  e additional evidence of terms occurring near each other will signi cantly improve the effectiveness of the ranking. A number of re- trieval models incorporating term proximity have been developed.  e following approach is designed to work in the inference network model, and produces good results.21
 e dependence model is based on the assumption that query terms are likely to appear in close proximity to each other within relevant documents. For example, given the query “Green party political views”, relevant documents will likely con- tain the phrases “green party” and “political views” within relatively close prox- imity to one another. If the query is treated as a set of terms Q, we can de ne SQ as the set of all non-empty subsets of Q. A Galago query attempts to capture dependencies between query terms as follows:
1. Every s ∈ SQ that consists of contiguous query terms is likely to appear as an exact phrase in a relevant document (i.e., represented using the #od:1 opera- tor).
2. Every s ∈ SQ such that |s| > 1 is likely to appear (ordered or unordered) within a reasonably sized window of text in a relevant document (i.e., in a window represented as #uw:8 for |s| = 2 and #uw:12 for |s| = 3 ).
As an example, this model produces the Galago query language representation shown in Figure 7.6 for the TREC query “embryonic stem cells”, where the weights were determined empirically to produce the best results.
Given the important pieces of evidence for web search ranking, we can now give an example of a Galago query that combines this evidence into an effective ranking. For the TREC query “pet therapy”, we would produce the Galago query shown in Figure 7.7.  e  rst thing to note about this query is that it clearly shows
21  e formal model is described in Metzler and Cro  (2005b).
 
282 7 RetrievalModels
#weight(
0.8 #combine(embryonic stem cells)
0.1 #combine( #od:1(stem cells) #od:1(embryonic stem) #od:1(embryonic stem cells))
0.1 #combine( #uw:8(stem cells) #uw:8(embryonic cells) #uw:8(embryonic stem) #uw:12(embryonic stem cells)))
Fig. 7.6. Galago query for the dependence model
how a complex query expression can be generated from a simple user query. A number of proximity terms have been added, and all terms are evaluated using contexts based on anchor text, title text, body text, and heading text. From an efficiency perspective, the proximity terms may be indexed, even though this will increase the index size substantially.  e bene t is that these relatively large query expressions will be able to be evaluated very efficiently at query time.
#weight(
0.1 #weight( 0.6 #prior(pagerank) 0.4 #prior(inlinks)) 1.0 #weight(
0.9 #combine(
#weight( 1.0 pet.(anchor) 1.0 pet.(title)
3.0 pet.(body) 1.0 pet.(heading)) #weight( 1.0 therapy.(anchor) 1.0 therapy.(title)
3.0 therapy.(body) 1.0 therapy.(heading)))
0.1 #weight(
1.0 #od:1(pet therapy).(anchor) 1.0 #od:1(pet therapy).(title) 3.0 #od:1(pet therapy).(body) 1.0 #od:1(pet therapy).(heading))
0.1 #weight(
1.0 #uw:8(pet therapy).(anchor) 1.0 #uw:8(pet therapy).(title) 3.0 #uw:8(pet therapy).(body) 1.0 #uw:8(pet therapy).(heading)))
)
Fig. 7.7. Galago query for web data
 e PageRank and inlink evidence is incorporated into this query as prior probabilities. In other words, this evidence is independent of speci c queries and can be calculated at indexing time.  e weights in the query were determined by
7.6 MachineLearningandInformationRetrieval 283
experiments with TREC Web page collections, which are based on a crawl of the .gov domain.  e relative importance of the evidence could be different for the full Web or for other collections.  e text in the main body of the page was found to be more important than the other parts of the document and anchor text, and this is re ected in the weights.
Experiments with the TREC data have also shown that much of the evi- dence that is crucial for effective navigational search is not important for top- ical searches. In fact, the only features needed for topical search are the simple terms and proximity terms for the body part of the document.  e other fea- tures do not improve effectiveness, but they also do not reduce it. Another dif- ference between topical and navigational searches is that query expansion using pseudo-relevance feedback was found to help topical searches, but made naviga- tional searches worse. Navigational searches are looking for a speci c page, so it is not surprising that smoothing the query by adding a number of extra terms may increase the “noise” in the results. If a search was known to be in the topical cat- egory, query expansion could be used, but this is difficult to determine reliably, and since the potential effectiveness bene ts of expansion are variable and some- what unpredictable, this technique is generally not used. Given that the evidence needed to identify good sites for transaction searches seems to be similar to that needed for navigational searches, this means that the same ranking algorithm can be used for the different categories of web search.
Other research has shown that user behavior information, such as clickthrough data (e.g., which documents have been clicked on in the past, which rank posi- tions were clicked) and browsing data (e.g., dwell time on page, links followed), can have a signi cant impact on the effectiveness of the ranking.  is type of ev- idence can be added into the inference network framework using additional op- erators, but as the number of pieces of evidence grows, the issue of how to deter- mine the most effective way of combining and weighting the evidence becomes more important. In the next section, we discuss techniques for learning both the weights and the ranking algorithm using explicit and implicit feedback data from the users.
7.6 Machine Learning and Information Retrieval
 ere has been considerable overlap between the  elds of information retrieval and machine learning. In the 1960s, relevance feedback was introduced as a tech- nique to improve ranking based on user feedback about the relevance of docu-
284 7 RetrievalModels
ments in an initial ranking.  is was an example of a simple machine-learning algorithm that built a classi er to separate relevant from non-relevant documents based on training data. In the 1980s and 1990s, information retrieval researchers used machine learning approaches to learn ranking algorithms based on user feed- back. In the last 10 years, there has been a lot of research on machine-learning ap- proaches to text categorization. Many of the applications of machine learning to information retrieval, however, have been limited by the amount of training data available. If the system is trying to build a separate classi er for every query, there is very little data about relevant documents available, whereas other machine- learning applications may have hundreds or even thousands of training examples. Even the approaches that tried to learn ranking algorithms by using training data from all the queries were limited by the small number of queries and relevance judgments in typical information retrieval test collections.
With the advent of web search engines and the huge query logs that are col- lected from user interactions, the amount of potential training data is enormous.  is has led to the development of new techniques that are having a signi cant impact in the  eld of information retrieval and on the design of search engines. In the next section, we describe techniques for learning ranking algorithms that can combine and weight the many pieces of evidence that are important for web search.
Another very active area of machine learning has been the development of so- phisticated statistical models of text. In section 7.6.2, we describe how these mod- els can be used to improve ranking based on language models.
7.6.1 Learning to Rank
All of the probabilistic retrieval models presented so far fall into the category of generative models. A generative model for text classi cation assumes that docu- ments were generated from some underlying model (in this case, usually a multi- nomial distribution) and uses training data to estimate the parameters of the model.  e probability of belonging to a class (i.e., the relevant documents for a query) is then estimated using Bayes’ Rule and the document model. A discrim- inative model, in contrast, estimates the probability of belonging to a class directly from the observed features of the document based on the training data.22 In gen- eral classi cation problems, a generative model performs better with low num- bers of training examples, but the discriminative model usually has the advantage
22 We revisit the discussion of generative versus discriminative classi ers in Chapter 9.
 
7.6 MachineLearningandInformationRetrieval 285
given enough data. Given the amount of potential training data available to web search engines, discriminative models may be expected to have some advantages in this application. It is also easier to incorporate new features into a discrimina- tive model and, as we have mentioned, there can be hundreds of features that are considered for web ranking.
Early applications of learning a discriminative model (discriminative learning) in information retrieval used logistic regression to predict whether a document belonged to the relevant class.  e problem was that the amount of training data and, consequently, the effectiveness of the technique depended on explicit rele- vance judgments obtained from people. Even given the resources of a commer- cial web search company, explicit relevance judgments are costly to obtain. On the other hand, query logs contain a large amount of implicit relevance informa- tion in the form of clickthroughs and other user interactions. In response to this, discriminative learning techniques based on this form of training data have been developed.
 e best-known of the approaches used to learn a ranking function for search is based on the Support Vector Machine (SVM) classi er.  is technique will be discussed in more detail in Chapter 9, so in this section we will just give a brief description of how a Ranking SVM can learn to rank.23
 e input to the Ranking SVM is a training set consisting of partial rank in- formation for a set of queries
(q1,r1),(q2,r2),...,(qn,rn)
where qi is a query and ri is partial information about the desired ranking, or rele- vance level, of documents for that query.  is means that if document da should be ranked higher than db, then (da, db) ∈ ri; otherwise, (da, db) ∈/ ri. Where do these rankings come from? If relevance judgments are available, the desired rank- ing would put all documents judged to be at a higher relevance level above those at a lower level. Note that this accommodates multiple levels of relevance, which are o en used in evaluations of web search engines.
If relevance judgments are not available, however, the ranking can be based on clickthrough and other user data. For example, if a person clicks on the third document in a ranking for a query and not on the  rst two, we can assume that it should be ranked higher in r. If d1, d2, and d3 are the documents in the  rst,
23  is description is based on Joachims’ paper on learning to rank using clickthrough data (Joachims, 2002b).
 
286 7 RetrievalModels
second, and third rank of the search output, the clickthrough data will result in pairs (d3, d1) and (d3, d2) being in the desired ranking for this query.  is ranking data will be noisy (because clicks are not relevance judgments) and incomplete, but there will be a lot of it, and experiments have shown that this type of training data can be used effectively.
Let’s assume that we are learning a linear ranking function w⃗.d⃗a, where w⃗ is a weight vector that is adjusted by learning, and d⃗a is the vector representation of the features of document da.  ese features are, as we described in the last sec- tion, based on page content, page metadata, anchor text, links, and user behavior. Instead of language model probabilities, however, the features used in this model that depend on the match between the query and the document content are usu- ally simpler and less formal. For example, there may be a feature for the number of words in common between the query and the document body, and similar fea- tures for the title, header, and anchor text.  e weights in the w⃗ vector determine the relative importance of these features, similar to the weights in the inference network operators. If a document is represented by three features with integer val- ues d⃗ = (2, 4, 1) and the weights w⃗ = (2, 1, 2), then the score computed by the ranking function is just:
w⃗.d⃗= (2,1,2).(2,4,1) = 2.2+1.4+2.1 = 10
Given the training set of queries and rank information, we would like to  nd a
weight vector w⃗ that would satisfy as many of the following conditions as possible:
∀(di,dj)∈r1 :w⃗.d⃗i >w⃗.d⃗j ...
∀(di,dj)∈rn :w⃗.d⃗i >w⃗.d⃗j
 is simply means that for all document pairs in the rank data, we would like the score for the document with the higher relevance rating (or rank) to be greater than the score for the document with the lower relevance rating. Unfortunately, there is no efficient algorithm to  nd the exact solution for w⃗. We can, however, reformulate this problem as a standard SVM optimization as follows:
7.6 MachineLearningandInformationRetrieval 287
1∑ minimize: 2w⃗.w⃗ +C ξi,j,k
subject to:
∀(di,dj)∈r1 :w⃗.d⃗i >w⃗.d⃗j +1−ξi,j,1
...
∀(di,dj)∈rn :w⃗.d⃗i >w⃗.d⃗j +1−ξi,j,n
∀i∀j∀k : ξi,j,k ≥ 0
where ξ, known as a slack variable, allows for misclassi cation of difficult or noisy training examples, and C is a parameter that is used to prevent over tting. Over t- ting happens when the learning algorithm produces a ranking function that does very well at ranking the training data, but does not do well at ranking documents for a new query. So ware packages are available24 that do this optimization and produce a classi er.
Where did this optimization come from?  e impatient reader will have to jump ahead to the explanation for a general SVM classi er in Chapter 9. For the time being, we can say that the SVM algorithm will  nd a classi er (i.e., the vector w⃗ ) that has the following property. Each pair of documents in our training data can be represented by the vector (d⃗i − d⃗j ). If we compute the score for this pair as w⃗ .(d⃗i − d⃗j ), the SVM classi er will  nd a w⃗ that makes the smallest score as large as possible.  e same thing is true for negative examples (pairs of documents that are not in the rank data).  is means that the classi er will make the differences in scores as large as possible for the pairs of documents that are hardest to rank.
Note that this model does not specify the features that should be used. It could even be used to learn the weights for features corresponding to scores from com- pletely different retrieval models, such as BM25 and language models. Combin- ing multiple searches for a given query has been shown to be effective in a number of experiments, and is discussed further in section 10.5.1. It should also be noted that the weights learned by Ranking SVM (or some other discriminative tech- nique) can be used directly in the inference network query language.
Although linear discriminative classi ers such as Ranking SVM may have an advantage for web search, there are other search applications where there will be less training data and less features available. For these applications, the generative models of topical relevance may be more effective, especially as the models con- tinue to improve through better estimation techniques.  e next section discusses
24 Such as SV Mlight; see http://svmlight.joachims.org.
  
288 7 RetrievalModels
how estimation can be improved by modeling a document as a mixture of topic models.
7.6.2 Topic Models and Vocabulary Mismatch
One of the important issues in general information retrieval is vocabulary mis- match.  is refers to a situation where relevant documents do not match a query, because they are using different words to describe the same topic. In the web en- vironment, many documents will contain all the query words, so this may not ap- pear to be an issue. In search applications with smaller collections, however, it will be important, and even in web search, TREC experiments have shown that topi- cal queries produce better results using query expansion. Query expansion (using, for example, pseudo-relevance feedback) is the standard technique for reducing vocabulary mismatch, although stemming also addresses this issue to some extent. A different approach would be to expand the documents by adding related terms. For documents represented as language models, this is equivalent to smoothing the probabilities in the language model so that words that did not occur in the text have non-zero probabilities. Note that this is different from smoothing us- ing the collection probabilities, which are the same for all documents. Instead, we need some way of increasing the probabilities of words that are associated with the topic of the document.
A number of techniques have been proposed to do this. If a document is known to belong to a category or cluster of documents, then the probabilities of words in that cluster can be used to smooth the document language model. We describe the details of this in Chapter 9. A technique known as Latent Seman- tic Indexing, or LSI,25 maps documents and terms into a reduced dimensionality space, so that documents that were previously indexed using a vocabulary of hun- dreds of thousands of words are now represented using just a few hundred fea- tures. Each feature in this new space is a mixture or cluster of many words, and it is this mixing that in effect smooths the document representation.
 e Latent Dirichlet Allocation (LDA) model, which comes from the machine learning community, models documents as a mixture of topics. A topic is a lan- guage model, just as we de ned previously. In a retrieval model such as query like- lihood, each document is assumed to be associated with a single topic.  ere are,
25  is technique is also called Latent Semantic Analysis or LSA (Deerwester et al., 1990). Note that “latent” is being used in the sense of “hidden.”
 
7.6 MachineLearningandInformationRetrieval 289
in effect, as many topics as there are documents in the collection. In the LDA ap- proach, in contrast, the assumption is that there is a  xed number of underlying (or latent) topics that can be used to describe the contents of documents. Each document is represented as a mixture of these topics, which achieves a smoothing effect that is similar to LSI. In the LDA model, a document is generated by  rst picking a distribution over topics, and then, for the next word in the document, we choose a topic and generate a word from that topic.
Using our “bucket” analogy for language models, we would need multiple buckets to describe this process. For each document, we would have one bucket of topics, with the number of instances of each topic depending on the distribution of topics we had picked. For each topic, there would be another bucket containing words, with the number of instances of the words depending on the probabilities in the topic language model.  en, to generate a document, we  rst select a topic from the topic bucket (still without looking), then go to the bucket of words for the topic that had been selected and pick out a word.  e process is then repeated for the next word.
More formally, the LDA process for generating a document is:
1. For each document D, pick a multinomial distribution θD from a Dirichlet distribution with parameter α.
2. For each word position in document D:
a) Pick a topic z from the multinomial distribution θD .
b) Choose a word w from P(w|z,β), a multinomial probability condi-
tioned on the topic z with parameter β.
A variety of techniques are available for learning the topic models and the θ distributions using the collection of documents as the training data, but all of these methods tend to be quite slow. Once we have these distributions, we can produce language model probabilities for the words in documents:
∑
Plda(w|D) = P(w|θD,β) =
 ese probabilities can then be used to smooth the document representation by
mixing them with the query likelihood probability as follows:
(fw,D+μcw )
z
P(w|z,β)P(z|θD)
 P (w|D) = λ |C| + (1 − λ)Plda(w|D) |D| + μ
 
290 7 RetrievalModels
So the  nal language model probabilities are, in effect, a mixture of the maximum likelihood probabilities, collection probabilities, and the LDA probabilities.
If the LDA probabilities are used directly as the document representation, the effectiveness of the ranking will be signi cantly reduced because the features are too smoothed. In TREC experiments, K (the number of topics) has a value of around 400.  is means that all documents in the collection are represented as mixtures of just 400 topics. Given that there can be millions of words in a col- lection vocabulary, matching on topics alone will lose some of the precision of matching individual words. When used to smooth the document language model, however, the LDA probabilities can signi cantly improve the effectiveness of query likelihood ranking. Table 7.7 shows the high-probability words from four LDA topics (out of 100) generated from a sample of TREC news stories.26 Note that the names of the topics were not automatically generated.
  Budgets
  Children
 million tax program budget billion federal year spending new state plan money programs government congress
                children women people child years families work parents says family welfare men percent care life
                            Arts
new
 lm show music movie play musical best actor  rst york opera theater actress love
Education
school students schools education teachers high public teacher bennett manigat namphy state president elementary haiti
  Table 7.7. Highest-probability terms from four topics in LDA model
 e main problem with using LDA for search applications is that estimating
the probabilities in the model is expensive. Until faster methods are developed, 26  is table is from Blei et al. (2003).
 
7.7 Application-BasedModels 291
this technique will be limited to smaller collections (hundreds of thousands of documents, but not millions).
7.7 Application-Based Models
In this chapter we have described a wide variety of retrieval models and ranking algorithms. From the point of view of someone involved in designing and imple- menting a search application, the question is which of these techniques should be used and when?  e answer depends on the application and the tools available. Most search applications involve much smaller collections than the Web and a lot less connectivity in terms of links and anchor text. Ranking algorithms that work well in web search engines o en do not produce the best rankings in other appli- cations. Customizing a ranking algorithm for the application will nearly always produce the best results.
 e  rst step in doing this is to construct a test collection of queries, docu- ments, and relevance judgments so that different versions of the ranking algo- rithm can be compared quantitatively. Evaluation is discussed in detail in Chapter 8, and it is the key to an effective search engine.
 e next step is to identify what evidence or features might be used to rep- resent documents. Simple terms and proximity terms are almost always useful. Signi cant document structure—such as titles, authors, and date  elds—are also nearly always important for search. In some applications, numeric  elds may be important.Textprocessingtechniquessuchasstemmingandstopwordsalsomust be considered.
Another important source of information that can be used for query expan- sion is an application-speci c thesaurus.  ese are surprisingly common since of- ten an attempt will have been made to build them either manually or automati- cally for a previous information system. Although they are o en very incomplete, the synonyms and related words they contain can make a signi cant difference to ranking effectiveness.
Having identi ed the various document features and other evidence, the next task is to decide how to combine it to calculate a document score. An open source search engine such as Galago makes this relatively easy since the combination and weighting of evidence can be expressed in the query language and many variations can be tested quickly. Other search engines do not have this degree of  exibility. If a search engine based on a simple retrieval model is being used for the search application, the descriptions of how scores are calculated in the BM25 or query
292 7 RetrievalModels
likelihood models and how they are combined in the inference network model can be used as a guide to achieve similar effects by appropriate query transforma- tions and additional code for scoring. For example, the synonym and related word information in a thesaurus should not be used to simply add words to a query. Un- less some version of the #syn operator is used, the effectiveness of the ranking will be reduced.  e implementation of #syn in Galago can be used as an example of how to add this operator to a search engine.
Much of the time spent in developing a search application will be spent on tuning the retrieval effectiveness of the ranking algorithm. Doing this without some concept of the underlying retrieval model can be very unrewarding.  e re- trieval models described in this chapter (namely BM25, query likelihood, rele- vance models, inference network, and Ranking SVM) provide the best possible blueprints for a successful ranking algorithm. For these models, good parame- ter values and weights are already known from extensive published experiments.  ese values can be used as a starting point for the process of determining whether modi cations are needed for an application. If enough training data is available, a discriminative technique such as Ranking SVM will learn the best weights di- rectly.
References and Further Reading
Since retrieval models are one of the most important topics in information re- trieval, there are many papers describing research in this area, starting in the 1950s. One of the most valuable aspects of van Rijsbergen’s book (van Rijsbergen, 1979) is the coverage of the older research in this area. In this book, we will focus on some of the major papers, rather than attempting to be comprehensive.  ese ref- erences will be discussed in the order of the topics presented in this chapter.
 e discussion of the nature of relevance has, understandably, been going on in information retrieval for a long time. One of the earlier papers that is o en cited is Saracevic (1975). A more recent article gives a review of work in this area (Mizzaro, 1997).
On the topic of Boolean versus ranked search, Turtle (1994) carried out an experiment comparing the performance of professional searchers using the best Boolean queries they could generate against keyword searches using ranked out- put and found no advantage for the Boolean search. When simple Boolean queries are compared against ranking, as in Turtle and Cro  (1991), the effectiveness of ranking is much higher.
7.7 Application-BasedModels 293
 e vector space model was  rst mentioned in Salton et al. (1975), and is de- scribed in detail in Salton and McGill (1983).  e most comprehensive paper in weighting experiments with this model is Salton and Buckley (1988), although the term-weighting techniques described in section 7.1.2 are a later improvement on those described in the paper.
 e description of information retrieval as a classi cation problem appears in van Rijsbergen (1979).  e best paper on the application of the binary indepen- dence model and its development into the BM25 ranking function is Sparck Jones et al. (2000).
 e use of language models in information retrieval started with Ponte and Cro  (1998), who described a retrieval model based on multiple-Bernoulli lan- guage models.  is was quickly followed by a number of papers that developed the multinomial version of the retrieval model (Hiemstra, 1998; F. Song & Cro , 1999). Miller et al. (1999) described the same approach using a Hidden Markov Model. Berger and Lafferty (1999) showed how translation probabilities for words could be incorporated into the language model approach. We will refer to this translation model again in section 10.3.  e use of non-uniform prior probabili- ties was studied by Kraaij et al. (2002). A collection of papers relating to language models and information retrieval appears in Cro  and Lafferty (2003).
Zhai and Lafferty (2004) give an excellent description of smoothing tech- niques for language modeling in information retrieval. Smoothing using clusters and nearest neighbors is described in Liu and Cro  (2004) and Kurland and Lee (2004).
An early term-dependency model was described in van Rijsbergen (1979). A bigram language model for information retrieval was described in F. Song and Cro  (1999), but the more general models in Gao et al. (2004) and Metzler and Cro  (2005b) produced signi cantly better retrieval results, especially with larger collections.
 e relevance model approach to query expansion appeared in Lavrenko and Cro  (2001). Lafferty and Zhai (2001) proposed a related approach that built a query model and compared it to document models.
 ere have been many experiments reported in the information retrieval liter- ature showing that the combination of evidence signi cantly improves the rank- ing effectiveness. Cro  (2000) reviews these results and shows that this is not surprising, given that information retrieval can be viewed as a classi cation prob- lem with a huge choice of features. Turtle and Cro  (1991) describe the infer- ence network model.  is model was used as the basis for the Inquery search en-
294 7 RetrievalModels
gine (Callan et al., 1992) and the WIN version of the commercial search engine WESTLAW (Pritchard-Schoch, 1993).  e extension of this model to include language model probabilities is described in Metzler and Cro  (2004).  is ex- tension was implemented as the Indri search engine (Strohman et al., 2005; Met- zler, Strohman, et al., 2004).  e Galago query language is based on the query language for Indri.
 e approach to web search described in section 7.5, which scores documents based on a combination or mixture of language models representing different parts of the document structure, is based on Ogilvie and Callan (2003).  e BM25F ranking function (Robertson et al., 2004) is an extension of BM25 that is also designed to effectively combine information from different document  elds.
Spam is of such importance in web search that an entire sub eld, called ad- versarial information retrieval, has developed to deal with search techniques for document collections that are being manipulated by parties with different inter- ests (such as spammers and search engine optimizers). We discuss the topic of spam in Chapter 9.
 e early work on learning ranking functions includes the use of logistic re- gression (Cooper et al., 1992). Fuhr and Buckley (1991) were the  rst to de- scribe clearly how using features that are independent of the actual query words (e.g., using a feature like the number of matching terms rather than which terms matched) enable the learning of ranking functions across queries.  e use of Ranking SVM for information retrieval was described by Joachims (2002b). Cao et al. (2006) describe modi cations of this approach that improve ranking effec- tiveness. RankNet (C. Burges et al., 2005) is a neural network approach to learn- ing a ranking function that is used in the Microso  web search engine. Agichtein, Brill, and Dumais (2006) describe how user behavior features can be incorporated effectively into ranking based on RankNet. Both Ranking SVMs and RankNet learn using partial rank information (i.e., pairwise preferences). Another class of learning models, called listwise models, use the entire ranked list for learning. Ex- amples of these models include the linear discriminative model proposed by Gao et al. (2005), which learns weights for features that are based on language models.  is approach has some similarities to the inference network model being used to combine language model and other features. Another listwise approach is the term dependence model proposed by Metzler and Cro  (2005b), which is also based on a linear combination of features. Both the Gao and Metzler models pro- vide a learning technique that maximizes average precision (an important infor-
7.7 Application-BasedModels 295
mation retrieval metric) directly. More information about listwise learning mod- els can be found in Xia et al. (2008).
Hofmann (1999) described a probabilistic version of LSI (pLSI) that intro- duced the modeling of documents as a mixture of topics.  e LDA model was described by Blei et al. (2003). A number of extensions of this model have been proposed since then, but they have not been applied to information retrieval.  e application of LDA to information retrieval was described in Wei and Cro  (2006).
Exercises
7.1. Use the “advanced search” feature of a web search engine to come up with three examples of searches using the Boolean operators AND, OR, and NOT that work better than using the same query in the regular search box. Do you think the search engine is using a strict Boolean model of retrieval for the advanced search?
7.2. Can you think of another measure of similarity that could be used in the vec- tor space model? Compare your measure with the cosine correlation using some example documents and queries with made-up weights. Browse the IR literature on the Web and see whether your measure has been studied (start with van Rijs- bergen’s book).
7.3. If each term represents a dimension in a t-dimensional space, the vector space model is making an assumption that the terms are orthogonal. Explain this as- sumption and discuss whether you think it is reasonable.
7.4. Derive Bayes’ Rule from the de nition of a conditional probability: P(A|B)= P(A∩B)
 P(B)
Give an example of a conditional and a joint probability using the occurrence of
words in documents as the events.
7.5. Implement a BM25 module for Galago. Show that it works and document it.
7.6. Show the effect of changing parameter values in your BM25 implementation.
296 7 RetrievalModels
7.7. What is the “bucket” analogy for a bigram language model? Give examples.
7.8. Using the Galago implementation of query likelihood, study the impact of short queries and long queries on effectiveness. Do the parameter settings make a difference?
7.9. Implement the relevance model approach to pseudo-relevance feedback in Galago. Show it works by generating some expansion terms for queries and doc- ument it.
7.10. Show that the belwand operator computes the query likelihood score with simple terms. What does the belwsum operator compute?
7.11. Implement a #not operator for the inference network query language in Galago. Show some examples of how it works.
7.12. Do a detailed design for numeric operators for the inference network query language in Galago.
7.13. Write an interface program that will take a user’s query as text and trans- form it into an inference network query. Make sure you use proximity operators. Compare the performance of the simple queries and the transformed queries.
8
Evaluating Search Engines
“Evaluation, Mr. Spock.”
Captain Kirk, Star Trek:  e Motion Picture
8.1 Why Evaluate?
Evaluation is the key to making progress in building better search engines. It is also essential to understanding whether a search engine is being used effectively in a speci c application. Engineers don’t make decisions about a new design for a commercial aircra  based on whether it feels better than another design. Instead, they test the performance of the design with simulations and experiments, eval- uate everything again when a prototype is built, and then continue to monitor and tune the performance of the aircra  a er it goes into service. Experience has shown us that ideas that we intuitively feel must improve search quality, or models that have appealing formal properties o en have little or no impact when tested using quantitative experiments.
One of the primary distinctions made in the evaluation of search engines is between effectiveness and efficiency. Effectiveness, loosely speaking, measures the ability of the search engine to  nd the right information, and efficiency measures how quickly this is done. For a given query, and a speci c de nition of relevance, we can more precisely de ne effectiveness as a measure of how well the ranking produced by the search engine corresponds to a ranking based on user relevance judgments. Efficiency is de ned in terms of the time and space requirements for the algorithm that produces the ranking. Viewed more generally, however, search is an interactive process involving different types of users with different informa- tion problems. In this environment, effectiveness and efficiency will be affected by many factors, such as the interface used to display search results and query re-  nement techniques, such as query suggestion and relevance feedback. Carrying out this type of holistic evaluation of effectiveness and efficiency, while impor-
 
298 8 EvaluatingSearchEngines
tant, is very difficult because of the many factors that must be controlled. For this reason, evaluation is more typically done in tightly de ned experimental settings, and this is the type of evaluation we focus on here.
Effectiveness and efficiency are related in that techniques that give a small boost to effectiveness may not be included in a search engine implementation if they have a signi cant adverse effect on an efficiency measure such as query throughput. Generally speaking, however, information retrieval research focuses on improving the effectiveness of search, and when a technique has been estab- lished as being potentially useful, the focus shi s to  nding efficient implemen- tations.  is is not to say that research on system architecture and efficiency is not important.  e techniques described in Chapter 5 are a critical part of build- ing a scalable and usable search engine and were primarily developed by research groups.  e focus on effectiveness is based on the underlying goal of a search en- gine, which is to  nd the relevant information. A search engine that is extremely fast is of no use unless it produces good results.
So is there a trade-off between efficiency and effectiveness? Some search en- gine designers discuss having “knobs,” or parameters, on their system that can be turned to favor either high-quality results or improved efficiency.  e current sit- uation, however, is that there is no reliable technique that signi cantly improves effectiveness that cannot be incorporated into a search engine due to efficiency considerations.  is may change in the future.
In addition to efficiency and effectiveness, the other signi cant consideration in search engine design is cost. We may know how to implement a particular search technique efficiently, but to do so may require a huge investment in pro- cessors, memory, disk space, and networking. In general, if we pick targets for any two of these three factors, the third will be determined. For example, if we want a particular level of effectiveness and efficiency, this will determine the cost of the system con guration. Alternatively, if we decide on efficiency and cost targets, it may have an impact on effectiveness. Two extreme cases of choices for these fac- tors are searching using a pattern-matching utility such as grep, or searching using an organization such as the Library of Congress. Searching a large text collection using grep will have poor effectiveness and poor efficiency, but will be very cheap. Searching using the staff analysts at the Library of Congress will produce excel- lent results (high effectiveness) due to the manual effort involved, will be efficient in terms of the user’s time (although it will involve a delay waiting for a response from the analysts), and will be very expensive. Searching directly using an effective search engine is designed to be a reasonable compromise between these extremes.
8.2  eEvaluationCorpus 299
An important point about terminology is the meaning of “optimization” as it is discussed in the context of evaluation.  e retrieval and indexing techniques in a search engine have many parameters that can be adjusted to optimize perfor- mance, both in terms of effectiveness and efficiency. Typically the best values for these parameters are determined using training data and a cost function. Training data is a sample of the real data, and the cost function is the quantity based on the data that is being maximized (or minimized). For example, the training data could be samples of queries with relevance judgments, and the cost function for a ranking algorithm would be a particular effectiveness measure.  e optimization process would use the training data to learn parameter settings for the ranking algorithm that maximized the effectiveness measure.  is use of optimization is very different from “search engine optimization”, which is the process of tailoring web pages to ensure high rankings from search engines.
In the remainder of this chapter, we will discuss the most important evalu- ation measures, both for effectiveness and efficiency. We will also describe how experiments are carried out in controlled environments to ensure that the results are meaningful.
8.2 The Evaluation Corpus
One of the basic requirements for evaluation is that the results from different techniques can be compared. To do this comparison fairly and to ensure that ex- periments are repeatable, the experimental settings and data used must be  xed. Starting with the earliest large-scale evaluations of search performance in the 1960s, generally referred to as the Cran eld1 experiments (Cleverdon, 1970), re- searchers assembled test collections consisting of documents, queries, and relevance judgments to address this requirement. In other language-related research  elds, such as linguistics, machine translation, or speech recognition, a text corpus is a large amount of text, usually in the form of many documents, that is used for sta- tistical analysis of various kinds.  e test collection, or evaluation corpus, in in- formation retrieval is unique in that the queries and relevance judgments for a particular search task are gathered in addition to the documents.
Test collections have changed over the years to re ect the changes in data and user communities for typical search applications. As an example of these changes,
1 Named a er the place in the United Kingdom where the experiments were done.
 
300 8 EvaluatingSearchEngines
the following three test collections were created at intervals of about 10 years, starting in the 1980s:
• CACM: Titles and abstracts from the Communications of the ACM from 1958–1979. Queries and relevance judgments generated by computer scien- tists.
• AP: Associated Press newswire documents from 1988–1990 (from TREC disks 1–3). Queries are the title  elds from TREC topics 51–150. Topics and relevance judgments generated by government information analysts.
• GOV2: Web pages crawled from websites in the .gov domain during early 2004. Queries are the title  elds from TREC topics 701–850. Topics and rel- evance judgments generated by government analysts.
 e CACM collection was created when most search applications focused on bib- liographic records containing titles and abstracts, rather than the full text of doc- uments. Table 8.1 shows that the number of documents in the collection (3,204) and the average number of words per document (64) are both quite small.  e total size of the document collection is only 2.2 megabytes, which is considerably less than the size of a single typical music  le for an MP3 player.  e queries for this collection of abstracts of computer science papers were generated by students and faculty of a computer science department, and are supposed to represent ac- tual information needs. An example of a CACM query is:
Security considerations in local networks, network operating systems, and dis- tributed systems.
Relevance judgments for each query were done by the same people, and were rel- atively exhaustive in the sense that most relevant documents were identi ed.  is was possible since the collection is small and the people who generated the ques- tions were very familiar with the documents. Table 8.2 shows that the CACM queries are quite long (13 words on average) and that there are an average of 16 relevant documents per query.
 e AP and GOV2 collections were created as part of the TREC conference series sponsored by the National Institute of Standards and Technology (NIST).  e AP collection is typical of the full-text collections that were  rst used in the early 1990s.  e availability of cheap magnetic disk technology and online text entry led to a number of search applications for full-text documents such as news
  Number of documents
   Size
   3,204 242,918 25,205,179
    2.2 MB 0.7 GB 426 GB
    Collection
CACM AP GOV2
8.2  eEvaluationCorpus 301
Average number of words/doc. 64
474
1073
  Table 8.1. Statistics for three example text collections.  e average number of words per document is calculated without stemming.
stories, legal documents, and encyclopedia articles.  e AP collection is much big- ger (by two orders of magnitude) than the CACM collection, both in terms of the number of documents and the total size.  e average document is also consider- ably longer (474 versus 64 words) since they contain the full text of a news story.  e GOV2 collection, which is another two orders of magnitude larger, was de- signed to be a testbed for web search applications and was created by a crawl of the .gov domain. Many of these government web pages contain lengthy policy de- scriptions or tables, and consequently the average document length is the largest of the three collections.
   Number of queries
   Average number of words/query
   64 100 150
     13.0 4.3 3.1
  Collection
CACM AP GOV2
Average number of relevant docs/query 16
220
180
  Table 8.2. Statistics for queries from example text collections
 e queries for the AP and GOV2 collections are based on TREC topics.  e topics were created by government information analysts employed by NIST.  e early TREC topics were designed to re ect the needs of professional analysts in government and industry and were quite complex. Later TREC topics were sup- posed to represent more general information needs, but they retained the TREC topic format. An example is shown in Figure 8.1. TREC topics contain three  elds indicated by the tags.  e title  eld is supposed to be a short query, more typical of a web application.  e description  eld is a longer version of the query, which as this example shows, can sometimes be more precise than the short query.  e narrative  eld describes the criteria for relevance, which is used by the people do-
302 8 EvaluatingSearchEngines
ing relevance judgments to increase consistency, and should not be considered as a query. Most recent TREC evaluations have focused on using the title  eld of the topic as the query, and our statistics in Table 8.2 are based on that  eld.
<top> 
<num> Number: 794 
 
<title> pet therapy 
 
<desc> Description:  How are pets or animals used in therapy for humans and what are the  benefits? 
 
<narr> Narrative:  Relevant documents must include details of how pet  or animal assisted  therapy is or has been used.  Relevant details include information  about pet therapy programs, descriptions of the circumstances in which  pet therapy is used, the benefits of this type of therapy, the degree  of success of this therapy, and any laws or regulations governing it. 
 
</top> 
 
Fig. 8.1. Example of a TREC topic
 e relevance judgments in TREC depend on the task that is being evalu- ated. For the queries in these tables, the task emphasized high recall, where it is important not to miss information. Given the context of that task, TREC an- alysts judged a document as relevant if it contained information that could be used to help write a report on the query topic. In Chapter 7, we discussed the difference between user relevance and topical relevance. Although the TREC rel- evance de nition does refer to the usefulness of the information found, analysts are instructed to judge all documents containing the same useful information as relevant.  is is not something a real user is likely to do, and shows that TREC is primarily focused on topical relevance. Relevance judgments for the CACM collections are binary, meaning that a document is either relevant or it is not.  is is also true of most of the TREC collections. For some tasks, multiple lev- els of relevance may be appropriate. Some TREC collections, including GOV2, were judged using three levels of relevance (not relevant, relevant, and highly rel- evant). We discuss effectiveness measures for both binary and graded relevance
8.2  eEvaluationCorpus 303
in section 8.4. Different retrieval tasks can affect the number of relevance judg- ments required, as well as the type of judgments and the effectiveness measure. For example, in Chapter 7 we described navigational searches, where the user is looking for a particular page. In this case, there is only one relevant document for the query.
Creating a new test collection can be a time-consuming task. Relevance judg- ments in particular require a considerable investment of manual effort for the high-recall search task. When collections were very small, most of the documents in a collection could be evaluated for relevance. In a collection such as GOV2, however, this would clearly be impossible. Instead, a technique called pooling is used. In this technique, the top k results (for TREC, k varied between 50 and 200) from the rankings obtained by different search engines (or retrieval algo- rithms) are merged into a pool, duplicates are removed, and the documents are presented in some random order to the people doing the relevance judgments. Pooling produces a large number of relevance judgments for each query, as shown in Table 8.2. However, this list is incomplete and, for a new retrieval algorithm that had not contributed documents to the original pool, this could potentially be a problem. Speci cally, if a new algorithm found many relevant documents that were not part of the pool, they would be treated as being not relevant, and conse- quently the effectiveness of that algorithm could be signi cantly underestimated. Studies with the TREC data, however, have shown that the relevance judgments are complete enough to produce accurate comparisons for new search techniques.
TREC corpora have been extremely useful for evaluating new search tech- niques, but they have limitations. A high-recall search task and collections of news articles are clearly not appropriate for evaluating product search on an e- commerce site, for example. New TREC “tracks” can be created to address im- portant new applications, but this process can take months or years. On the other hand, new search applications and new data types such as blogs, forums, and an- notated videos are constantly being developed. Fortunately, it is not that difficult to develop an evaluation corpus for any given application using the following ba- sic guidelines:
• Use a document collection that is representative for the application in terms of the number, size, and type of documents. In some cases, this may be the actual collection for the application; in others it will be a sample of the actual collec- tion, or even a similar collection. If the target application is very general, then more than one collection should be used to ensure that results are not corpus-
304 8 EvaluatingSearchEngines
speci c. For example, in the case of the high-recall TREC task, a number of
different news and government collections were used for evaluation.
•  e queries that are used for the test collection should also be representative of the queries submitted by users of the target application.  ese may be acquired either from a query log from a similar application or by asking potential users for examples of queries. Although it may be possible to gather tens of thou- sands of queries in some applications, the need for relevance judgments is a major constraint.  e number of queries must be sufficient to establish that a new technique makes a signi cant difference. An analysis of TREC experi- ments has shown that with 25 queries, a difference in the effectiveness measure MAP (section 8.4.2) of 0.05 will result in the wrong conclusion about which system is better in about 13% of the comparisons. With 50 queries, this error rate falls below 4%. A difference of 0.05 in MAP is quite large. If a signi cance test, such as those discussed in section 8.6.1, is used in the evaluation, a relative difference of 10% in MAP is sufficient to guarantee a low error rate with 50 queries. If resources or the application make more relevance judgments possi- ble, in terms of generating reliable results it will be more productive to judge more queries rather than to judge more documents from existing queries (i.e., increasing k). Strategies such as judging a small number (e.g., 10) of the top- ranked documents from many queries or selecting documents to judge that will make the most difference in the comparison (Carterette et al., 2006) have been shown to be effective. If a small number of queries are used, the results should be considered indicative, not conclusive. In that case, it is important that the queries should be at least representative and have good coverage in terms of the goals of the application. For example, if algorithms for local search were being tested, the queries in the test collection should include many dif-
ferent types of location information.
• Relevance judgments should be done either by the people who asked the ques-
tions or by independent judges who have been instructed in how to determine relevance for the application being evaluated. Relevance may seem to be a very subjective concept, and it is known that relevance judgments can vary depend- ing on the person making the judgments, or even vary for the same person at different times. Despite this variation, analysis of TREC experiments has shown that conclusions about the relative performance of systems are very stable. In other words, differences in relevance judgments do not have a sig- ni cant effect on the error rate for comparisons.  e number of documents that are evaluated for each query and the type of relevance judgments will de-
8.3 Logging 305
pend on the effectiveness measures that are chosen. For most applications, it is generally easier for people to decide between at least three levels of rele- vance: de nitely relevant, de nitely not relevant, and possibly relevant.  ese can be converted into binary judgments by assigning the “possibly relevant” level to either one of the other levels, if that is required for an effectiveness measure. Some applications and effectiveness measures, however, may support more than three levels of relevance.
As a  nal point, it is worth emphasizing that many user actions can be consid- ered implicit relevance judgments, and that if these can be exploited, this can sub- stantially reduce the effort of constructing a test collection. For example, actions such as clicking on a document in a result list, moving it to a folder, or sending it to a printer may indicate that it is relevant. In previous chapters, we have described how query logs and clickthrough can be used to support operations such as query expansion and spelling correction. In the next section, we discuss the role of query logs in search engine evaluation.
8.3 Logging
Query logs that capture user interactions with a search engine have become an extremely important resource for web search engine development. From an eval- uation perspective, these logs provide large amounts of data showing how users browse the results that a search engine provides for a query. In a general web search application, the number of users and queries represented can number in the tens of millions. Compared to the hundreds of queries used in typical TREC collec- tions, query log data can potentially support a much more extensive and realistic evaluation.  e main drawback with this data is that it is not as precise as explicit relevance judgments.
An additional concern is maintaining the privacy of the users.  is is par- ticularly an issue when query logs are shared, distributed for research, or used to construct user pro les (see section 6.2.5). Various techniques can be used to anonymize the logged data, such as removing identifying information or queries that may contain personal data, although this can reduce the utility of the log for some purposes.
A typical query log will contain the following data for each query:
• User identi er or user session identi er.  is can be obtained in a number of ways. If a user logs onto a service, uses a search toolbar, or even allows cookies,
306 8 EvaluatingSearchEngines
this information allows the search engine to identify the user. A session is a series of queries submitted to a search engine over a limited amount of time. In some circumstances, it may be possible to identify a user only in the context of a session.
• Query terms.  e query is stored exactly as the user entered it.
• List of URLs of results, their ranks on the result list, and whether they were
clicked on.2
• Timestamp(s).  e timestamp records the time that the query was submit-
ted. Additional timestamps may also record the times that speci c results were clicked on.
 e clickthrough data in the log (the third item) has been shown to be highly correlated with explicit judgments of relevance when interpreted appropriately, and has been used for both training and evaluating search engines. More detailed information about user interaction can be obtained through a client-side applica- tion, such as a search toolbar in a web browser. Although this information is not always available, some user actions other than clickthroughs have been shown to be good predictors of relevance. Two of the best predictors are page dwell time and search exit action.  e page dwell time is the amount of time the user spends on a clicked result, measured from the initial click to the time when the user comes back to the results page or exits the search application.  e search exit action is the way the user exits the search application, such as entering another URL, clos- ing the browser window, or timing out. Other actions, such as printing a page, are very predictive but much less frequent.
Although clicks on result pages are highly correlated with relevance, they can- not be used directly in place of explicit relevance judgments, because they are very biased toward pages that are highly ranked or have other features such as being popular or having a good snippet on the result page.  is means, for example, that pages at the top rank are clicked on much more frequently than lower-ranked pages, even when the relevant pages are at the lower ranks. One approach to re- moving this bias is to use clickthrough data to predict user preferences between pairs of documents rather than relevance judgments. User preferences were  rst mentioned in section 7.6, where they were used to train a ranking function. A preference for document d1 compared to document d2 means that d1 is more rel-
2 In some logs, only the clicked-on URLs are recorded. Logging all the results enables the generation of preferences and provides a source of “negative” examples for various tasks.
 
8.3 Logging 307
evant or, equivalently, that it should be ranked higher. Preferences are most ap- propriate for search tasks where documents can have multiple levels of relevance, and are focused more on user relevance than purely topical relevance. Relevance judgments (either multi-level or binary) can be used to generate preferences, but preferences do not imply speci c relevance levels.
 e bias in clickthrough data is addressed by “strategies,” or policies that gen- erate preferences.  ese strategies are based on observations of user behavior and veri ed by experiments. One strategy that is similar to that described in section 7.6 is known as Skip Above and Skip Next (Agichtein, Brill, Dumais, & Ragno, 2006).  is strategy assumes that given a set of results for a query and a clicked result at rank position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition, unclicked results immediately fol- lowing a clicked result are less relevant than the clicked result. For example, given a result list of ranked documents together with click data as follows:
d1
d2
d3 (clicked) d4,
this strategy will generate the following preferences:
d3 > d2 d3 > d1 d3 > d4
Since preferences are generated only when higher-ranked documents are ignored, a major source of bias is removed.
 e “Skip” strategy uses the clickthrough patterns of individual users to gener- ate preferences.  is data can be noisy and inconsistent because of the variability in users’ behavior. Since query logs typically contain many instances of the same query submitted by different users, clickthrough data can be aggregated to remove potential noise from individual differences. Speci cally, click distribution infor- mation can be used to identify clicks that have a higher frequency than would be expected based on typical click patterns.  ese clicks have been shown to corre- late well with relevance judgments. For a given query, we can use all the instances of that query in the log to compute the observed click frequency O(d, p) for the result d in rank position p. We can also compute the expected click frequency E(p) at rank p by averaging across all queries.  e click deviation CD(d, p) for a result d in position p is computed as:
308 8 EvaluatingSearchEngines
CD(d, p) = O(d, p) − E(p).
We can then use the value of CD(d, p) to “ lter” clicks and provide more reliable click information to the Skip strategy.
A typical evaluation scenario involves the comparison of the result lists for two or more systems for a given set of queries. Preferences are an alternate method of specifying which documents should be retrieved for a given query (relevance judg- ments being the typical method).  e quality of the result lists for each system is then summarized using an effectiveness measure that is based on either prefer- ences or relevance judgments.  e following section describes the measures that are most commonly used in research and system development.
8.4 Effectiveness Metrics
8.4.1 Recall and Precision
 e two most common effectiveness measures, recall and precision, were intro- duced in the Cran eld studies to summarize and compare search results. Intu- itively, recall measures how well the search engine is doing at  nding all the rele- vant documents for a query, and precision measures how well it is doing at reject- ing non-relevant documents.
 e de nition of these measures assumes that, for a given query, there is a set of documents that is retrieved and a set that is not retrieved (the rest of the doc- uments).  is obviously applies to the results of a Boolean search, but the same de nition can also be used with a ranked search, as we will see later. If, in addition, relevance is assumed to be binary, then the results for a query can be summarized as shown in Table 8.3. In this table, A is the relevant set of documents for the query, A is the non-relevant set, B is the set of retrieved documents, and B is the set of documents that are not retrieved.  e operator ∩ gives the intersection of two sets. For example, A ∩ B is the set of documents that are both relevant and retrieved.
A number of effectiveness measures can be de ned using this table.  e two we are particularly interested in are:
Recall = |A ∩ B| |A|
Precision= |A∩B| |B|
    
8.4 EffectivenessMetrics 309
Relevant Non-Relevant Retrieved A∩B A∩B Not Retrieved A ∩ B A ∩ B
Table 8.3. Sets of documents de ned by a simple search with binary relevance
where |.| gives the size of the set. In other words, recall is the proportion of rel- evant documents that are retrieved, and precision is the proportion of retrieved documents that are relevant.  ere is an implicit assumption in using these mea- sures that the task involves retrieving as many of the relevant documents as pos- sible and minimizing the number of non-relevant documents retrieved. In other words, even if there are 500 relevant documents for a query, the user is interested in  nding them all.
We can also view the search results summarized in Table 8.3 as the output of a binary classi er, as was mentioned in section 7.2.1. When a document is retrieved, it is the same as making a prediction that the document is relevant. From this per- spective, there are two types of errors that can be made in prediction (or retrieval).  ese errors are called false positives (a non-relevant document is retrieved) and false negatives (a relevant document is not retrieved). Recall is related to one type of error (the false negatives), but precision is not related directly to the other type of error. Instead, another measure known as fallout,3 which is the proportion of non-relevant documents that are retrieved, is related to the false positive errors:
Fallout= |A∩B| |A|
3 In the classi cation and signal detection literature, the errors are known as Type I and Type II errors. Recall is o en called the true positive rate, or sensitivity. Fallout is called the false positive rate, or the false alarm rate. Another measure, speci city, is 1 – fallout. Precision is known as the positive predictive value, and is o en used in medical diag- nostic tests where the probability that a positive test is correct is particularly important.  e true positive rate and the false positive rate are used to draw ROC (receiver op- erating characteristic) curves that show the trade-off between these two quantities as the discrimination threshold varies.  is threshold is the value at which the classi er makes a positive prediction. In the case of search, the threshold would correspond to a position in the document ranking. In information retrieval, recall-precision graphs are generally used instead of ROC curves.
           
310 8 EvaluatingSearchEngines
Given that fallout and recall together characterize the effectiveness of a search as a classi er, why do we use precision instead?  e answer is simply that precision is more meaningful to the user of a search engine. If 20 documents were retrieved for a query, a precision value of 0.7 means that 14 out of the 20 retrieved doc- uments would be relevant. Fallout, on the other hand, will always be very small because there are so many non-relevant documents. If there were 1,000,000 non- relevant documents for the query used in the precision example, fallout would be 6/1000000 = 0.000006. If precision fell to 0.5, which would be noticeable to the user, fallout would be 0.00001.  e skewed nature of the search task, where most of the corpus is not relevant to any given query, also means that evaluating a search engine as a classi er can lead to counterintuitive results. A search engine trained to minimize classi cation errors would tend to retrieve nothing, since classifying a document as non-relevant is always a good decision!
 e F measure is an effectiveness measure based on recall and precision that is used for evaluating classi cation performance and also in some search applica- tions. It has the advantage of summarizing effectiveness in a single number. It is de ned as the harmonic mean of recall and precision, which is:
F= 1 =2RP 1(1 +1) (R+P)
Why use the harmonic mean instead of the usual arithmetic mean or average?  e harmonic mean emphasizes the importance of small values, whereas the arith- metic mean is affected more by values that are unusually large (outliers). A search result that returned nearly the entire document collection, for example, would have a recall of 1.0 and a precision near 0.  e arithmetic mean of these values is 0.5, but the harmonic mean will be close to 0.  e harmonic mean is clearly a better summary of the effectiveness of this retrieved set.4
Most of the retrieval models we have discussed produce ranked output. To use recall and precision measures, retrieved sets of documents must be de ned based on the ranking. One possibility is to calculate recall and precision values at every
4  e more general form of the F measure is the weighted harmonic mean, which allows weights re ecting the relative importance of recall and precision to be used.  is mea- sure is F = RP /(αR + (1 − α)P ), where α is a weight.  is is o en transformed usingα=1/(β2+1),whichgivesFβ =(β2+1)RP/(R+β2P). ecommonF measure is in fact F1, where recall and precision have equal importance. In some eval- uations, precision or recall is emphasized by varying the value of β. Values of β > 1 emphasize recall.
     2RP
 
8.4 EffectivenessMetrics 311
rank position. Figure 8.2 shows the top ten documents of two possible rankings, together with the recall and precision values calculated at every rank position for a query that has six relevant documents.  ese rankings might correspond to, for example, the output of different retrieval algorithms or search engines.
At rank position 10 (i.e., when ten documents are retrieved), the two rankings have the same effectiveness as measured by recall and precision. Recall is 1.0 be- cause all the relevant documents have been retrieved, and precision is 0.6 because both rankings contain six relevant documents in the retrieved set of ten docu- ments. At higher rank positions, however, the  rst ranking is clearly better. For example, at rank position 4 (four documents retrieved), the  rst ranking has a re- call of 0.5 (three out of six relevant documents retrieved) and a precision of 0.75 (three out of four retrieved documents are relevant).  e second ranking has a recall of 0.17 (1/6) and a precision of 0.25 (1/4).
                Ranking #1
Ranking #2
= the relevant documents
Recall     0.17  0.17   0.33   0.5   0.67  0.83  0.83  0.83  0.83   1.0 Precision     1.0   0.5    0.67  0.75   0.8   0.83  0.71  0.63  0.56   0.6
Recall     0.0   0.17   0.17  0.17  0.33   0.5   0.67   0.67  0.83   1.0 Precision     0.0    0.5    0.33  0.25   0.4    0.5   0.57    0.5    0.56   0.6
          Fig. 8.2. Recall and precision values for two rankings of six relevant documents
If there are a large number of relevant documents for a query, or if the relevant documents are widely distributed in the ranking, a list of recall-precision values for every rank position will be long and unwieldy. Instead, a number of techniques have been developed to summarize the effectiveness of a ranking.  e  rst of these is simply to calculate recall-precision values at a small number of prede ned rank positions. In fact, to compare two or more rankings for a given query, only the
312 8 EvaluatingSearchEngines
precision at the prede ned rank positions needs to be calculated. If the precision for a ranking at rank position p is higher than the precision for another ranking, the recall will be higher as well.  is can be seen by comparing the corresponding recall-precision values in Figure 8.2.  is effectiveness measure is known as pre- cision at rank p.  ere are many possible values for the rank position p, but this measure is typically used to compare search output at the top of the ranking, since that is what many users care about. Consequently, the most common versions are precision at 10 and precision at 20. Note that if these measures are used, the im- plicit search task has changed to  nding the most relevant documents at a given rank, rather than  nding as many relevant documents as possible. Differences in search output further down the ranking than position 20 will not be considered.  is measure also does not distinguish between differences in the rankings at po- sitions 1 to p, which may be considered important for some tasks. For example, the two rankings in Figure 8.2 will be the same when measured using precision at 10.
Another method of summarizing the effectiveness of a ranking is to calculate precision at  xed or standard recall levels from 0.0 to 1.0 in increments of 0.1. Each ranking is then represented using 11 numbers.  is method has the advan- tage of summarizing the effectiveness of the ranking of all relevant documents, rather than just those in the top ranks. Using the recall-precision values in Figure 8.2 as an example, however, it is clear that values of precision at these standard re- call levels are o en not available. In this example, only the precision values at the standard recall levels of 0.5 and 1.0 have been calculated. To obtain the precision values at all of the standard recall levels will require interpolation.5 Since standard recall levels are used as the basis for averaging effectiveness across queries and gen- erating recall-precision graphs, we will discuss interpolation in the next section.
 e third method, and the most popular, is to summarize the ranking by av- eraging the precision values from the rank positions where a relevant document was retrieved (i.e., when recall increases). If a relevant document is not retrieved for some reason,6 the contribution of this document to the average is 0.0. For the  rst ranking in Figure 8.2, the average precision is calculated as:
(1.0+0.67+0.75+0.8+0.83+0.6)/6 = 0.78
5 Interpolation refers to any technique for calculating a new point between two existing data points.
6 One common reason is that only a limited number of the top-ranked documents (e.g., 1,000) are considered.
 
For the second ranking, it is:
(0.5+0.4+0.5+0.57+0.56+0.6)/6 = 0.52
Average precision has a number of advantages. It is a single number that is based on the ranking of all the relevant documents, but the value depends heavily on the highly ranked relevant documents.  is means it is an appropriate measure for evaluating the task of  nding as many relevant documents as possible while still re ecting the intuition that the top-ranked documents are the most important.
All three of these methods summarize the effectiveness of a ranking for a single query. To provide a realistic assessment of the effectiveness of a retrieval algorithm, it must be tested on a number of queries. Given the potentially large set of results from these queries, we will need a method of summarizing the performance of the retrieval algorithm by calculating the average effectiveness for the entire set of queries. In the next section, we discuss the averaging techniques that are used in most evaluations.
8.4.2 Averaging and Interpolation
In the following discussion of averaging techniques, the two rankings shown in Figure 8.3 are used as a running example.  ese rankings come from using the same ranking algorithm on two different queries.  e aim of an averaging tech- nique is to summarize the effectiveness of a speci c ranking algorithm across a collection of queries. Different queries will o en have different numbers of rel- evant documents, as is the case in this example. Figure 8.3 also gives the recall- precision values calculated for the top 10 rank positions.
Given that the average precision provides a number for each ranking, the sim- plest way to summarize the effectiveness of rankings from multiple queries would be to average these numbers.  is effectiveness measure, mean average precision,7 or MAP, is used in most research papers and some system evaluations.8 Since
7  is sounds a lot better than average average precision!
8 In some evaluations the geometric mean of the average precision (GMAP) is used in-
stead of the arithmetic mean.  is measure, because it multiplies average precision val- ues, emphasizes the impact of queries with low performance. It is de ned as
1 ∑n
GMAP =expn
where n is the number of queries, and APi is the average precision for query i.
8.4 EffectivenessMetrics 313
 logAPi
 i=1
314
8 EvaluatingSearchEngines
Ranking #1
     = relevant documents for query 1
          Recall    0.2    0.2    0.4    0.4    0.4     0.6    0.6    0.6     0.8    1.0 Precision    1.0    0.5    0.67   0.5    0.4     0.5   0.43  0.38   0.44   0.5
= relevant documents for query 2
Ranking #2
Recall    0.0   0.33   0.33  0.33  0.67  0.67   1.0    1.0    1.0    1.0 Precision    0.0    0.5    0.33  0.25  0.4   0.33   0.43  0.38  0.33  0.3
Fig. 8.3. Recall and precision values for rankings from two different queries
             it is based on average precision, it assumes that the user is interested in  nding many relevant documents for each query. Consequently, using this measure for comparison of retrieval algorithms or systems can require a considerable effort to acquire the relevance judgments, although methods for reducing the number of judgments required have been suggested (e.g., Carterette et al., 2006).
For the example in Figure 8.3, the mean average precision is calculated as fol- lows:
average precision query 1 = (1.0 + 0.67 + 0.5 + 0.44 + 0.5)/5 = 0.62 average precision query 2 = (0.5 + 0.4 + 0.43)/3 = 0.44
mean average precision = (0.62 + 0.44)/2 = 0.53
 e MAP measure provides a very succinct summary of the effectiveness of a ranking algorithm over many queries. Although this is o en useful, sometimes too much information is lost in this process. Recall-precision graphs, and the ta- bles of recall-precision values they are based on, give more detail on the effective- ness of the ranking algorithm at different recall levels. Figure 8.4 shows the recall- precision graph for the two queries in the example from Figure 8.3. Graphs for individualquerieshaveverydifferentshapesandaredifficulttocompare.Togen-
1
0.8
0.6
0.4
0.2
0
erate a recall-precision graph that summarizes effectiveness over all the queries, the recall-precision values in Figure 8.3 should be averaged. To simplify the aver- aging process, the recall-precision values for each query are converted to precision values at standard recall levels, as mentioned in the last section.  e precision val- ues for all queries at each standard recall level can then be averaged.9
 e standard recall levels are 0.0 to 1.0 in increments of 0.1. To obtain pre- cision values for each query at these recall levels, the recall-precision data points, such as those in Figure 8.3, must be interpolated.  at is, we have to de ne a func- tion based on those data points that has a value at each standard recall level.  ere are many ways of doing interpolation, but only one method has been used in in-
9  is is called a macroaverage in the literature. A macroaverage computes the measure of interest for each query and then averages these measures. A microaverage combines all the applicable data points from every query and computes the measure from the combined data. For example, a microaverage precision at rank 5 would be calculated as ∑ni=1 ri/5n, where ri is the number of relevant documents retrieved in the top  ve documents by query i, and n is the number of queries. Macroaveraging is used in most retrieval evaluations.
8.4 EffectivenessMetrics 315
                        0 0.2 0.4 0.6 0.8 1 Recall
Fig. 8.4. Recall-precision graphs for two queries
 Precision
316 8 EvaluatingSearchEngines
formation retrieval evaluations since the 1970s. In this method, we de ne the pre-
cision P at any standard recall level R as
P(R)=max{P′ :R′ ≥R∧(R′,P′)∈S}
where S is the set of observed (R, P ) points.  is interpolation, which de nes the precision at any recall level as the maximum precision observed in any recall- precision point at a higher recall level, produces a step function, as shown in Figure 8.5.
1
0.8
0.6
0.4
0.2
0
Fig. 8.5. Interpolated recall-precision graphs for two queries
Because search engines are imperfect and nearly always retrieve some non- relevant documents, precision tends to decrease with increasing recall (although this is not always true, as is shown in Figure 8.4).  is interpolation method is consistent with this observation in that it produces a function that is monoton- ically decreasing.  is means that precision values always go down (or stay the same) with increasing recall.  e interpolation also de nes a precision value for the recall level of 0.0, which would not be obvious otherwise!  e general intu- ition behind this interpolation is that the recall-precision values are de ned by the
                        0 0.2 0.4 0.6 0.8 1 Recall
Precision
8.4 EffectivenessMetrics 317
sets of documents in the ranking with the best possible precision values. In query 1, for example, there are three sets of documents that would be the best possi- ble for the user to look at in terms of  nding the highest proportion of relevant documents.
 e average precision values at the standard recall levels are calculated by sim- ply averaging the precision values for each query. Table 8.4 shows the interpolated precision values for the two example queries, along with the average precision val- ues.  e resulting average recall-precision graph is shown in Figure 8.6.
Recall 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Ranking1 1.0 1.0 1.0 0.67 0.67 0.5 0.5 0.5 0.5 0.5 0.5 Ranking2 0.5 0.5 0.5 0.5 0.43 0.43 0.43 0.43 0.43 0.43 0.43 Average 0.75 0.75 0.75 0.59 0.55 0.47 0.47 0.47 0.47 0.47 0.47
Table 8.4. Precision values at standard recall levels calculated using interpolation
                        
                              
Fig. 8.6. Average recall-precision graph using standard recall levels
         
318 8 EvaluatingSearchEngines
 e average recall-precision graph is plotted by simply joining the average pre- cision points at the standard recall levels, rather than using another step function. Although this is somewhat inconsistent with the interpolation method, the inter- mediate recall levels are never used in evaluation. When graphs are averaged over many queries, they tend to become smoother. Figure 8.7 shows a typical recall- precision graph from a TREC evaluation using 50 queries.
1
0.8
0.6
0.4
0.2
0
Fig. 8.7. Typical recall-precision graph for 50 queries from TREC
8.4.3 Focusing on the Top Documents
In many search applications, users tend to look at only the top part of the ranked result list to  nd relevant documents. In the case of web search, this means that many users look at just the  rst page or two of results. In addition, tasks such as navigational search (Chapter 7) or question answering (Chapter 1) have just a single relevant document. In these situations, recall is not an appropriate measure. Instead, the focus of an effectiveness measure should be on how well the search engine does at retrieving relevant documents at very high ranks (i.e., close to the top of the ranking).
               0 0.2 0.4 0.6 0.8 1 Recall
Precision
8.4 EffectivenessMetrics 319
One measure with this property that has already been mentioned is precision at rank p, where p in this case will typically be 10.  is measure is easy to compute, can be averaged over queries to produce a single summary number, and is readily understandable.  e major disadvantage is that it does not distinguish between different rankings of a given number of relevant documents. For example, if only one relevant document was retrieved in the top 10, according to the precision measure a ranking where that document is in the top position would be the same as one where it was at rank 10. Other measures have been proposed that are more sensitive to the rank position.
 e reciprocal rank measure has been used for applications where there is typ- ically a single relevant document. It is de ned as the reciprocal of the rank at which the  rst relevant document is retrieved.  e mean reciprocal rank (MRR) is the average of the reciprocal ranks over a set of queries. For example, if the top  ve documents retrieved for a query were dn, dr, dn, dn, dn, where dn is a non- relevant document and dr is a relevant document, the reciprocal rank would be 1/2 = 0.5. Even if more relevant documents had been retrieved, as in the rank- ing dn, dr, dn, dr, dn, the reciprocal rank would still be 0.5.  e reciprocal rank is very sensitive to the rank position. It falls from 1.0 to 0.5 from rank 1 to 2, and the ranking dn, dn, dn, dn, dr would have a reciprocal rank of 1/5 = 0.2.  e MRR for these two rankings would be (0.5 + 0.2)/2 = 0.35.
 e discounted cumulative gain (DCG) has become a popular measure for eval- uating web search and related applications (Järvelin & Kekäläinen, 2002). It is based on two assumptions:
• Highly relevant documents are more useful than marginally relevant docu- ments.
•  e lower the ranked position of a relevant document (i.e., further down the ranked list), the less useful it is for the user, since it is less likely to be examined.
 ese two assumptions lead to an evaluation that uses graded relevance as a measure of the usefulness, or gain, from examining a document.  e gain is accu- mulated starting at the top of the ranking and may be reduced, or discounted, at lower ranks.  e DCG is the total gain accumulated at a particular rank p. Specif- ically, it is de ned as:
∑p reli
log2 i
DCGp = rel1 +
 i=2
320 8 EvaluatingSearchEngines
where reli is the graded relevance level of the document retrieved at rank i. For example, web search evaluations have been reported that used manual relevance judgmentsonasix-pointscalerangingfrom“Bad”to“Perfect”(i.e.,0≤reli ≤ 5). Binary relevance judgments can also be used, in which case reli would be either 0or1.
 e denominator log2 i is the discount or reduction factor that is applied to the gain.  ere is no theoretical justi cation for using this particular discount fac- tor, although it does provide a relatively smooth (gradual) reduction.10 By varying the base of the logarithm, the discount can be made sharper or smoother. With base 2, the discount at rank 4 is 1/2, and at rank 8 it is 1/3. As an example, con- sider the following ranking where each number is a relevance level on the scale 0–3 (not relevant–highly relevant):
3,2,3,0,0,1,2,2,3,0
 ese numbers represent the gain at each rank.  e discounted gain would be:
3, 2/1, 3/1.59, 0, 0, 1/2.59, 2/2.81, 2/3, 3/3.17, 0 = 3, 2, 1.89, 0, 0, 0.39, 0.71, 0.67, 0.95, 0
 e DCG at each rank is formed by accumulating these numbers, giving: 3, 5, 6.89, 6.89, 6.89, 7.28, 7.99, 8.66, 9.61, 9.61
Similar to precision at rank p, speci c values of p are chosen for the evaluation, and the DCG numbers are averaged across a set of queries. Since the focus of this measure is on the top ranks, these values are typically small, such as 5 and 10. For thisexample,DCGatrank5is6.89andatrank10is9.61.Tofacilitateaveraging across queries with different numbers of relevant documents, these numbers can be normalized by comparing the DCG at each rank with the DCG value for the perfect ranking for that query. For example, if the previous ranking contained all
10 In some publications, DCG is de ned as: ∑p
 (2reli − 1)/log(1 + i)
For binary relevance judgments, the two de nitions are the same, but for graded rele- vance this de nition puts a strong emphasis on retrieving highly relevant documents.  is version of the measure is used by some search engine companies and, because of this, may become the standard.
DCGp =
i=1
8.4 EffectivenessMetrics 321
the relevant documents for that query, the perfect ranking would have gain values at each rank of:
3,3,3,2,2,2,1,0,0,0
which would give ideal DCG values of:
3, 6, 7.89, 8.89, 9.75, 10.52, 10.88, 10.88, 10.88, 10.88
Normalizing the actual DCG values by dividing by the ideal values gives us the
normalized discounted cumulative gain (NDCG) values: 1, 0.83, 0.87, 0.76, 0.71, 0.69, 0.73, 0.8, 0.88, 0.88
Note that the NDCG measure is ≤ 1 at any rank position. To summarize, the NDCG for a given query can be de ned as:
NDCGp = DCGp IDCGp
where IDCG is the ideal DCG value for that query.
8.4.4 Using Preferences
In section 8.3, we discussed how user preferences can be inferred from query logs. Preferences have been used for training ranking algorithms, and have been sug- gested as an alternative to relevance judgments for evaluation. Currently, how- ever, there is no standard effectiveness measure based on preferences.
In general, two rankings described using preferences can be compared using the Kendall tau coefficient (τ ). If P is the number of preferences that agree and Q is the number that disagree, Kendall’s τ is de ned as:
τ=P−Q P+Q
 is measure varies between 1 (when all preferences agree) and –1 (when they all disagree). If preferences are derived from clickthrough data, however, only a par- tial ranking is available. Experimental evidence shows that this partial informa- tion can be used to learn effective ranking algorithms, which suggests that effec- tiveness can be measured this way. Instead of using the complete set of preferences to calculate P and Q, a new ranking would be evaluated by comparing it to the known set of preferences. For example, if there were 15 preferences learned from
  
322 8 EvaluatingSearchEngines
clickthrough data, and a ranking agreed with 10 of these, the τ measure would be (10 − 5)/15 = 0.33. Although this seems reasonable, no studies are available that show that this effectiveness measure is useful for comparing systems.
For preferences derived from binary relevance judgments, the BPREF11 mea- sure has been shown to be robust with partial information and to give similar re- sults (in terms of system comparisons) to recall-precision measures such as MAP. In this measure, the number of relevant and non-relevant documents is balanced to facilitate averaging across queries. For a query with R relevant documents, only the  rst R non-relevant documents are considered.  is is equivalent to using R × R preferences (all relevant documents are preferred to all non-relevant doc- uments). Given this, the measure is de ned as:
1∑ Ndr BPREF=R (1− R)
dr
where dr is a relevant document and Ndr gives the number of non-relevant doc- uments (from the set of R non-relevant documents that are considered) that are rankedhigherthandr.Ifthisisexpressedintermsofpreferences,Ndr isactually a method for counting the number of preferences that disagree (for binary rele- vance judgments). Since R × R is the number of preferences being considered, an alternative de nition of BPREF is:
BPREF = P P+Q
which means it is very similar to Kendall’s τ.  e main difference is that BPREF varies between 0 and 1. Given that BPREF is a useful effectiveness measure, this suggests that the same measure or τ could be used with preferences associated with graded relevance.
8.5 Efficiency Metrics
Compared to effectiveness, the efficiency of a search system seems like it should be easier to quantify. Most of what we care about can be measured automatically with a timer instead of with costly relevance judgments. However, like effectiveness, it is important to determine exactly what aspects of efficiency we want to measure. Table 8.5 shows some of the measures that are used.
11 Binary Preference
    
Metric name
Elapsed indexing time
Indexing processor time
Query throughput Query latency
Indexing temporary space Index size
8.5 EfficiencyMetrics 323
Description
Measures the amount of time necessary to build a docu- ment index on a particular system.
Measures the CPU seconds used in building a document index.  is is similar to elapsed time, but does not count time waiting for I/O or speed gains from parallelism. Number of queries processed per second.
 e amount of time a user must wait a er issuing a query before receiving a response, measured in milliseconds.  is can be measured using the mean, but is o en more instructive when used with the median or a percentile bound.
Amount of temporary disk space used while creating an index.
Amount of storage necessary to store the index  les.
 Table 8.5. De nitions of some important efficiency metrics
 e most commonly quoted efficiency metric is query throughput, measured in queries processed per second.  roughput numbers are comparable only for the same collection and queries processed on the same hardware, although rough comparisons can be made between runs on similar hardware. As a single-number metric of efficiency, throughput is good because it is intuitive and mirrors the common problems we want to solve with efficiency numbers. A real system user will want to use throughput numbers for capacity planning, to help determine whether more hardware is necessary to handle a particular query load. Since it is simple to measure the number of queries per second currently being issued to a service, it is easy to determine whether a system’s query throughput is adequate to handle the needs of an existing service.
 e trouble with using throughput alone is that it does not capture latency. Latency measures the elapsed time the system takes between when the user issues a query and when the system delivers its response. Psychology research suggests that users consider any operation that takes less than about 150 milliseconds to be instantaneous. Above that level, users react very negatively to the delay they perceive.
 is brings us back to throughput, because latency and throughput are not orthogonal: generally we can improve throughput by increasing latency, and re-
324 8 EvaluatingSearchEngines
ducing latency leads to poorer throughput. To see why this is so, think of the dif- ference between having a personal chef and ordering food at a restaurant.  e per- sonal chef prepares your food with the lowest possible latency, since she has no other demands on her time and focuses completely on preparing your food. Un- fortunately, the personal chef has low throughput, since her focus on you leads to idle time when she is not completely occupied.  e restaurant is a high through- put operation with lots of chefs working on many different orders simultaneously. Having many orders and many chefs leads to certain economies of scale—for in- stance, when a single chef prepares many identical orders at the same time. Note that the chef is able to process these orders simultaneously precisely because some latency has been added to some orders: instead of starting to cook immediately upon receiving an order, the chef may decide to wait a few minutes to see if any- one else orders the same thing.  e result is that the chefs are able to cook food with high throughput but at some cost in latency.
Query processing works the same way. It is possible to build a system that han- dles just one query at a time, devoting all resources to the current query, just like the personal chef devotes all her time to a single customer.  is kind of system is low throughput, because only one query is processed at a time, which leads to idle resources.  e radical opposite approach is to process queries in large batches.  e system can then reorder the incoming queries so that queries that use common subexpressions are evaluated at the same time, saving valuable execution time. However, interactive users will hate waiting for their query batch to complete.
Like recall and precision in effectiveness, low latency and high throughput are both desirable properties of a retrieval system, but they are in con ict with each other and cannot be maximized at the same time. In a real system, query throughput is not a variable but a requirement: the system needs to handle every query the users submit.  e two remaining variables are latency (how long the users will have to wait for a response) and hardware cost (how many processors will be applied to the search problem). A common way to talk about latency is with percentile bounds, such as “99% of all queries will complete in under 100 milliseconds.” System designers can then add hardware until this requirement is met.
Query throughput and latency are the most visible system efficiency metrics, but we should also consider the costs of indexing. For instance, given enough time and space, it is possible to cache every possible query of a particular length. A system that did this would have excellent query throughput and query latency, but at the cost of enormous storage and indexing costs.  erefore, we also need
8.6 Training,Testing,andStatistics 325
to measure the size of the index structures and the time necessary to create them. Because indexing is o en a distributed process, we need to know both the total amount of processor time used during indexing and the elapsed time. Since the process of inversion o en requires temporary storage, it is interesting to measure the amount of temporary storage used.
8.6 Training, Testing, and Statistics
8.6.1 Significance Tests
Retrieval experiments generate data, such as average precision values or NDCG values. In order to decide whether this data shows that there is a meaningful dif- ference between two retrieval algorithms or search engines, signi cance tests are needed. Every signi cance test is based on a null hypothesis. In the case of a typical retrieval experiment, we are comparing the value of an effectiveness measure for rankings produced by two retrieval algorithms.  e null hypothesis is that there is no difference in effectiveness between the two retrieval algorithms.  e alter- native hypothesis is that there is a difference. In fact, given two retrieval algorithms A and B, where A is a baseline algorithm and B is a new algorithm, we are usually trying to show that the effectiveness of B is better than A, rather than simply  nd- ing a difference. Since the rankings that are compared are based on the same set of queries for both retrieval algorithms, this is known as a matched pair experiment.
We obviously cannot conclude that B is better than A on the basis of the re- sults of a single query, since A may be better than B on all other queries. So how many queries do we have to look at to make a decision about which is better? If, for example, B is better than A for 90% of 200 queries in a test collection, we should be more con dent that B is better for that effectiveness measure, but how con dent? Signi cance tests allow us to quantify the con dence we have in any judgment about effectiveness.
More formally, a signi cance test enables us to reject the null hypothesis in fa- vor of the alternative hypothesis (i.e., show that B is better than A) on the basis of the data from the retrieval experiments. Otherwise, we say that the null hypoth- esis cannot be rejected (i.e., B might not be better than A). As with any binary decision process, a signi cance test can make two types of error. A Type I error is when the null hypothesis is rejected when it is in fact true. A Type II error is when the null hypothesis is accepted when it is in fact false.12 Signi cance tests
12 Compare to the discussion of errors in section 8.4.1.
 
326 8 EvaluatingSearchEngines
are o en described by their power, which is the probability that the test will reject the null hypothesis correctly (i.e., decide that B is better than A). In other words, a test with high power will reduce the chance of a Type II error.  e power of a test can also be increased by increasing the sample size, which in this case is the number of queries in the experiment. Increasing the number of queries will also reduce the chance of a Type I error.
 e procedure for comparing two retrieval algorithms using a particular set of queries and a signi cance test is as follows:
1. Compute the effectiveness measure for every query for both rankings.
2. Compute a test statistic based on a comparison of the effectiveness measures for each query.  e test statistic depends on the signi cance test, and is simply a quantity calculated from the sample data that is used to decide whether or
not the null hypothesis should be rejected.
3.  e test statistic is used to compute a P-value, which is the probability that a
test statistic value at least that extreme could be observed if the null hypothesis
were true. Small P-values suggest that the null hypothesis may be false.
4.  enullhypothesis(nodifference)isrejectedinfavorofthealternatehypoth- esis (i.e., B is more effective than A) if the P-value is ≤ α, the signi cance level. Values for α are small, typically 0.05 and 0.1, to reduce the chance of a Type I
error.
In other words, if the probability of getting a speci c test statistic value is very small assuming the null hypothesis is true, we reject that hypothesis and conclude that ranking algorithm B is more effective than the baseline algorithm A.
 e computation of the test statistic and the corresponding P-value is usually done using tables or standard statistical so ware.  e signi cance tests discussed here are also provided in Galago.
 e procedure just described is known as a one-sided or one-tailed test since we want to establish that B is better than A. If we were just trying to establish that there is a difference between B and A, it would be a two-sided or two-tailed test, and the P-value would be doubled.  e “side” or “tail” referred to is the tail of a probability distribution. For example, Figure 8.8 shows a distribution for the possible values of a test statistic assuming the null hypothesis.  e shaded part of the distribution is the region of rejection for a one-sided test. If a test yielded the test statistic value x, the null hypothesis would be rejected since the probability of getting that value or higher (the P-value) is less than the signi cance level of 0.05.
8.6 Training,Testing,andStatistics 327
      Test statistic value
p = 0.05
x
Fig. 8.8. Probability distribution for test statistic values assuming the null hypothesis.  e shaded area is the region of rejection for a one-sided test.
 e signi cance tests most commonly used in the evaluation of search engines are the t-test,13 the Wilcoxon signed-rank test, and the sign test. To explain these tests, we will use the data shown in Table 8.6, which shows the effectiveness val- ues of the rankings produced by two retrieval algorithms for 10 queries.  e values in the table are arti cial and could be average precision or NDCG, for example, on a scale of 0–100 (instead of 0–1).  e table also shows the difference in the ef- fectiveness measure between algorithm B and the baseline algorithm A.  e small number of queries in this example data is not typical of a retrieval experiment.
In general, the t-test assumes that data values are sampled from normal dis- tributions. In the case of a matched pair experiment, the assumption is that the difference between the effectiveness values is a sample from a normal distribution.  e null hypothesis in this case is that the mean of the distribution of differences is zero.  e test statistic for the paired t-test is:
t = B − A.√N σB−A
where B − A is the mean of the differences, σB−A is the standard deviation14 of the differences, and N is the size of the sample (the number of queries). For
13 Also known as Student’s t-test, where “student” was the pen name of the inventor, William Gosset, not the type of person who should use it.
14 For a set of data values xi, the standard deviation can be calculated by σ = √∑Ni=1(xi − x)2/N, where x is the mean.
        
328 8 EvaluatingSearchEngines
Query
1 25 35 10
2 43 84 41
3 39 15 -24
4 75 75 0
5 43 68 25
6 15 85 70
7 20 80 60
8 52 50 -2
9 49 58 9
10 50 75 25
Table 8.6. Arti cial effectiveness data for two retrieval algorithms (A and B) over 10 queries.  e column B – A gives the difference in effectiveness.
thedatainTable8.6,B−A=21.4,σB−A =29.1,andt=2.33.Foraone- tailed test, this gives a P-value of 0.02, which would be signi cant at a level of α = 0.05.  erefore, for this data, the t-test enables us to reject the null hypothesis and conclude that ranking algorithm B is more effective than A.
 ere are two objections that could be made to using the t-test in search eval- uations.  e  rst is that the assumption that the data is sampled from normal distributions is generally not appropriate for effectiveness measures, although the distribution of differences can resemble a normal distribution for large N . Recent experimental results have supported the validity of the t-test by showing that it produces very similar results to the randomization test on TREC data (Smucker et al., 2007).  e randomization test does not assume the data comes from normal distributions, and is the most powerful of the nonparametric tests.15  e random- ization test, however, is much more expensive to compute than the t-test.
 e second objection that could be made is concerned with the level of mea- surement associated with effectiveness measures.  e t-test (and the randomiza- tion test) assume the the evaluation data is measured on an interval scale.  is means that the values can be ordered (e.g., an effectiveness of 54 is greater than an effectiveness of 53), and that differences between values are meaningful (e.g., the difference between 80 and 70 is the same as the difference between 20 and 10). Some people have argued that effectiveness measures are an ordinal scale, which
15 A nonparametric test makes less assumptions about the data and the underlying distri- bution than parametric tests.
 A B B–A
    
8.6 Training,Testing,andStatistics 329
means that the magnitude of the differences are not signi cant.  e Wilcoxon signed-rank test and the sign test, which are both nonparametric, make less as- sumptions about the effectiveness measure. As a consequence, they do not use all the information in the data, and it can be more difficult to show a signi cant dif- ference. In other words, if the effectiveness measure did satisfy the conditions for using the t-test, the Wilcoxon and sign tests have less power.
 e Wilcoxon signed-rank test assumes that the differences between the ef- fectiveness values for algorithms A and B can be ranked, but the magnitude is not important.  is means, for example, that the difference for query 8 in Table 8.6 will be ranked  rst because it is the smallest non-zero absolute value, but the magnitude of 2 is not used directly in the test.  e test statistic is:
∑N w= Ri
i=1
where Ri is a signed-rank, and N is the number of differences ̸= 0. To compute the signed-ranks, the differences are ordered by their absolute values (increasing), and then assigned rank values (ties are assigned the average rank).  e rank values are then given the sign of the original difference.  e null hypothesis for this test is that the sum of the positive ranks will be the same as the sum of the negative ranks.
For example, the nine non-zero differences from Table 8.6, in rank order of absolute value, are:
2,9,10,24,25,25,41,60,70  e corresponding signed-ranks are:
–1, +2, +3, –4, +5.5, +5.5, +7, +8, +9
Summing these signed-ranks gives a value of w = 35. For a one-tailed test, this gives a P-value of approximately 0.025, which means the null hypothesis can be rejected at a signi cance level of α = 0.05.
 e sign test goes further than the Wilcoxon signed-ranks test, and completely
ignores the magnitude of the differences.  e null hypothesis for this test is that
P(B > A) = P(A > B) = 1.Inotherwords,overalargesamplewewouldex- 2
pect that the number of pairs where B is “better” than A would be the same as the number of pairs where A is “better” than B.  e test statistic is simply the num- ber of pairs where B > A.  e issue for a search evaluation is deciding what dif- ference in the effectiveness measure is “better.” We could assume that even small
 
330 8 EvaluatingSearchEngines
differences in average precision or NDCG—such as 0.51 compared to 0.5—are signi cant.  is has the risk of leading to a decision that algorithm B is more ef- fective than A when the difference is, in fact, not noticeable to the users. Instead, an appropriate threshold for the effectiveness measure should be chosen. For ex- ample, an old rule of thumb in information retrieval is that there has to be at least 5% difference in average precision to be noticeable (10% for a more conserva- tive threshold).  is would mean that a difference of 0.51 − 0.5 = 0.01 would be considered a tie for the sign test. If the effectiveness measure was precision at rank 10, on the other hand, any difference might be considered signi cant since it would correspond directly to additional relevant documents in the top 10.
For the data in Table 8.6, we will consider any difference to be signi cant.  is means there are seven pairs out of ten where B is better than A.  e corresponding P-value is 0.17, which is the chance of observing seven “successes” in ten trials where the probability of success is 0.5 (just like  ipping a coin). Using the sign test, we cannot reject the null hypothesis. Because so much information from the effectiveness measure is discarded in the sign test, it is more difficult to show a difference, and more queries are needed to increase the power of the test. On the other hand, it can be used in addition to the t-test to provide a more user-focused perspective. An algorithm that is signi cantly more effective according to both the t-test and the sign test, perhaps using different effectiveness measures, is more likely to be noticeably better.
8.6.2 Setting Parameter Values
Nearly every ranking algorithm has parameters that can be tuned to improve the effectiveness of the results. For example, BM25 has the parameters k1, k2, and b used in term weighting, and query likelihood with Dirichlet smoothing has the parameter μ. Ranking algorithms for web search can have hundreds of parameters that give the weights for the associated features.  e values of these parameters can have a major impact on retrieval effectiveness, and values that give the best ef- fectiveness for one application may not be appropriate for another application, or even for a different document collection. Not only is choosing the right parameter values important for the performance of a search engine when it is deployed, it is an important part of comparing the effectiveness of two retrieval algorithms. An algorithm that has had its parameters tuned for optimal performance for the test collection may appear to be much more effective than it really is when compared to a baseline algorithm with poor parameter values.
8.6 Training,Testing,andStatistics 331
 e appropriate method of setting parameters for both maximizing effective- ness and making fair comparisons of algorithms is to use a training set and a test set of data.  e training set is used to learn the best parameter values, and the test set is used for validating these parameter values and comparing ranking algorithms.  e training and test sets are two separate test collections of documents, queries, and relevance judgments, although they may be created by splitting a single col- lection. In TREC experiments, for example, the training set is usually documents, queries, and relevance judgments from previous years. When there is not a large amount of data available, cross-validation can be done by partitioning the data into K subsets. One subset is used for testing, and K − 1 are used for training.  is is repeated using each of the subsets as a test set, and the best parameter values are averaged across the K runs.
Using training and test sets helps to avoid the problem of over tting (men- tioned in Chapter 7), which occurs when the parameter values are tuned to  t a particular set of data too well. If this was the only data that needed to be searched in an application, that would be appropriate, but a much more common situa- tion is that the training data is only a sample of the data that will be encountered when the search engine is deployed. Over tting will result in a choice of parame- ter values that do not generalize well to this other data. A symptom of over tting is that effectiveness on the training set improves but effectiveness on the test set gets worse.
A fair comparison of two retrieval algorithms would involve getting the best parameter values for both algorithms using the training set, and then using those values with the test set.  e effectiveness measures are used to tune the parameter values in multiple retrieval runs on the training data, and for the  nal comparison, which is a single retrieval run, on the test data.  e “cardinal sin” of retrieval exper- iments, which should be avoided in nearly all situations, is testing on the training data.  is typically will arti cially boost the measured effectiveness of a retrieval algorithm. It is particularly problematic when one algorithm has been trained in some way using the testing data and the other has not. Although it sounds like an easy problem to avoid, it can sometimes occur in subtle ways in more complex experiments.
Given a training set of data, there a number of techniques for  nding the best parameter settings for a particular effectiveness measure.  e most common method is simply to explore the space of possible parameter values by brute force.  is requires a large number of retrieval runs with small variations in parameter values (a parameter sweep). Although this could be computationally infeasible for
332 8 EvaluatingSearchEngines
large numbers of parameters, it is guaranteed to  nd the parameter settings that give the best effectiveness for any given effectiveness measure.  e Ranking SVM method described in section 7.6 is an example of a more sophisticated procedure for learning good parameter values efficiently with large numbers of parameters.  is method, as well as similar optimization techniques, will  nd the best possi- ble parameter values if the function being optimized meets certain conditions.16 Because many of the effectiveness measures we have described do not meet these conditions, different functions are used for the optimization, and the parame- ter values are not guaranteed to be optimal.  is is, however, a very active area of research, and new methods for learning parameters are constantly becoming available.
8.6.3 Online Testing
All of the evaluation strategies described thus far have assumed that training and testing are done offline.  at is, we have assumed that all of the training and test data are  xed ahead of time. However, with real search engines, it may be possible to test (or even train) using live traffic.  is is o en called online testing. For ex- ample, suppose that you just developed a new sponsored-search advertising algo- rithm. Rather than evaluating your system using human relevance judgments, it is possible to deploy the new ranking algorithm and observe the amount of revenue generated using the new algorithm versus some baseline algorithm.  is makes it possible to test various search engine components, such as ranking algorithms, query suggestion algorithms, and snippet generation algorithms, using live traffic and real users. Notice that this is similar to logging, which was discussed earlier in this chapter. With logging, evaluations are typically done retrospectively on “stale” data, whereas online testing uses live data.
 ere are several bene ts to online testing. First, it allows real users to inter- act with the system.  ese interactions provide information, such as click data, that can be used for various kinds of evaluation. Second, online testing is less bi- ased, since the evaluation is being done over a real sample of users and traffic.  is
16 Speci cally, the function should be convex (or concave; a function f(x) is concave if and only if −f (x) is convex). A convex function is a continuous function that satis es the following constraint for all λ in [0,1]:
f(λx1 + (1 − λ)x2) ≤ λf(x1) + (1 − λ)f(x2)
 
8.7  eBottomLine 333
is valuable because it is o en difficult to build test collections that accurately re-  ect real search engine users and traffic. Finally, online testing can produce a large amount of data very cheaply, since it does not require paying any humans to do relevance judgments.
Unfortunately, online testing also has its fair share of drawbacks.  e primary drawback is that the data collected is typically very noisy.  ere are many different reasons why users behave the way they do in an online environment. For example, if a user does not click on a search result, it does not necessarily mean the result is bad.  e user may have clicked on an advertisement instead, lost interest, or sim- ply gone to eat dinner.  erefore, typically a very large amount of online testing data is required to eliminate noise and produce meaningful conclusions. Another drawback to online testing is that it requires live traffic to be altered in poten- tially harmful ways. If the algorithm being tested is highly experimental, then it may signi cantly degrade retrieval effectiveness and drive users away. For this rea- son, online testing must be done very carefully, so as not to negatively affect the user experience. One way of minimizing the effect of an online test on the general user population is to use the experimental algorithm only for a small percentage, such as 1% to 5%, of the live traffic. Finally, online tests typically provide only a very speci c type of data—click data. As we described earlier in this section, click data is not always ideal for evaluating search engines, since the data is noisy and highly biased. However, for certain search engine evaluation metrics, such as clickthrough rate17 and revenue, online testing can be very useful.
 erefore, online testing can be a useful, inexpensive way of training or testing new algorithms, especially those that can be evaluated using click data. Special care must be taken to ensure that the data collected is analyzed properly and that the overall user experience is not degraded.
8.7 The Bottom Line
In this chapter, we have presented a number of effectiveness and efficiency mea- sures. At this point, it would be reasonable to ask which of them is the right mea- sure to use.  e answer, especially with regard to effectiveness, is that no single measure is the correct one for any search application. Instead, a search engine should be evaluated through a combination of measures that show different as-
17  e percentage of times that some item is clicked on.
 
334 8 EvaluatingSearchEngines
pects of the system’s performance. In many settings, all of the following measures and tests could be carried out with little additional effort:
• Mean average precision - single number summary, popular measure, pooled relevance judgments.
• Average NDCG - single number summary for each rank level, emphasizes top ranked documents, relevance judgments needed only to a speci c rank depth (typically to 10).
• Recall-precision graph - conveys more information than a single number mea- sure, pooled relevance judgments.
• Average precision at rank 10 - emphasizes top ranked documents, easy to un- derstand, relevance judgments limited to top 10.
Using MAP and a recall-precision graph could require more effort in relevance judgments, but this analysis could also be limited to the relevant documents found in the top 10 for the NDCG and precision at 10 measures.
All these evaluations should be done relative to one or more baseline searches. It generally does not make sense to do an effectiveness evaluation without a good baseline, since the effectiveness numbers depend strongly on the particular mix of queries and documents in the test collection.  e t-test can be used as the sig- ni cance test for the average precision, NDCG, and precision at 10 measures.
All of the standard evaluation measures and signi cance tests are available us- ing the evaluation program provided as part of Galago.
In addition to these evaluations, it is also very useful to present a summary of the number of queries that were improved and the number that were degraded, relative to a baseline. Figure 8.9 gives an example of this summary for a TREC run, where the query numbers are shown as a distribution over various percentage levels of improvement for a speci c evaluation measure (usually MAP). Each bar represents the number of queries that were better (or worse) than the baseline by the given percentage.  is provides a simple visual summary showing that many more queries were improved than were degraded, and that the improvements were sometimes quite substantial. By setting a threshold on the level of improvement that constitutes “noticeable,” the sign test can be used with this data to establish signi cance.
Given this range of measures, both developers and users will get a better pic- ture of where the search engine is performing well and where it may need improve- ment. It is o en necessary to look at individual queries to get a better understand-
8.7  eBottomLine 335
Fig. 8.9. Example distribution of query effectiveness improvements
ing of what is causing the ranking behavior of a particular algorithm. Query data
such as Figure 8.9 can be helpful in identifying interesting queries.
References and Further Reading
Despite being discussed for more than 40 years, the measurement of effectiveness in search engines is still a hot topic, with many papers being published in the ma- jor conferences every year.  e chapter on evaluation in van Rijsbergen (1979) gives a good historical perspective on effectiveness measurement in information retrieval. Another useful general source is the TREC book (Voorhees & Harman, 2005), which describes the test collections and evaluation procedures used and how they evolved.
Saracevic (1975) and Mizzaro (1997) are the best papers for general reviews of the critical topic of relevance.  e process of obtaining relevance judgments and the reliability of retrieval experiments are discussed in the TREC book. Zobel (1998) shows that some incompleteness of relevance judgments does not affect experiments, although Buckley and Voorhees (2004) suggest that substan- tial incompleteness can be a problem. Voorhees and Buckley (2002) discuss the
 25
20
15
Queries
10
5
0
                                                               <  100%[ 100%, [ 75%, [ 50%, [ 25%, [0%, [25%, [50%, [75%, > 100%  75%]  50%]  25%] 0%] 25%] 50%] 75%] 100%]
Percentage Gain or Loss
336 8 EvaluatingSearchEngines
error rates associated with different numbers of queries. Sanderson and Zobel (2005) show how using a signi cance test can affect the reliability of compar- isons and also compare shallow versus in-depth relevance judgments. Carterette et al. (2006) describe a technique for reducing the number of relevance judgments required for reliable comparisons of search engines. Kelly and Teevan (2003) re- view approaches to acquiring and using implicit relevance information. Fox et al. (2005) studied implicit measures of relevance in the context of web search, and Joachims et al. (2005) introduced strategies for deriving preferences based on clickthrough data. Agichtein, Brill, Dumais, and Ragno (2006) extended this approach and carried out more experiments introducing click distributions and deviation, and showing that a number of features related to user behavior are use- ful for predicting relevance.
 e F measure was originally proposed by van Rijsbergen (1979) in the form of E = 1 − F . He also provided a justi cation for the E measure in terms of mea- surement theory, raised the issue of whether effectiveness measures were interval or ordinal measures, and suggested that the sign and Wilcoxon tests would be appropriate for signi cance. Cooper (1968) wrote an important early paper that introduced the expected search length (ESL) measure, which was the expected number of documents that a user would have to look at to  nd a speci ed num- ber of relevant documents. Although this measure has not been widely used, it was the ancestor of measures such as NDCG (Järvelin & Kekäläinen, 2002) that fo- cus on the top-ranked documents. Another measure of this type that has recently been introduced is rank-biased precision (Moffat et al., 2007).
Yao (1995) provides one of the  rst discussions of preferences and how they could be used to evaluate a search engine.  e paper by Joachims (2002b) that showed how to train a linear feature-based retrieval model using preferences also used Kendall’s τ as the effectiveness measure for de ning the best ranking.  e recent paper by Carterette and Jones (2007) shows how search engines can be evaluated using relevance information directly derived from clickthrough data, rather than converting clickthrough to preferences.
A number of recent studies have focused on interactive information retrieval.  ese studies involve a different style of evaluation than the methods described in this chapter, but are more formal than online testing. Belkin (2008) describes the challenges of evaluating interactive experiments and points to some interesting papers on this topic.
Another area related to effectiveness evaluation is the prediction of query effec- tiveness. Cronen-Townsend et al. (2006) describe the Clarity measure, which is
8.7  eBottomLine 337
used to predict whether a ranked list for a query has good or bad precision. Other measures have been suggested that have even better correlations with average pre- cision.
 ere are very few papers that discuss guidelines for efficiency evaluations of search engines. Zobel et al. (1996) is an example from the database literature.
Exercises
8.1. Find three other examples of test collections in the information retrieval lit- erature. Describe them and compare their statistics in a table.
8.2. Imagine that you were going to study the effectiveness of a search engine for blogs. Specify the retrieval task(s) for this application, and then describe the test collection you would construct and how you would evaluate your ranking algo- rithms.
8.3. For one query in the CACM collection (provided at the book website), gen- erate a ranking using Galago, and then calculate average precision, NDCG at 5 and 10, precision at 10, and the reciprocal rank by hand.
8.4. For two queries in the CACM collection, generate two uninterpolated recall- precision graphs, a table of interpolated precision values at standard recall levels, and the average interpolated recall-precision graph.
8.5. Generate the mean average precision, recall-precision graph, average NDCG at 5 and 10, and precision at 10 for the entire CACM query set.
8.6. Compare the MAP value calculated in the previous problem to the GMAP value. Which queries have the most impact on this value?
8.7. Another measure that has been used in a number of evaluations is R-precision.  is is de ned as the precision at R documents, where R is the number of relevant documents for a query. It is used in situations where there is a large variation in the number of relevant documents per query. Calculate the average R-precision for the CACM query set and compare it to the other measures.
8.8. Generate another set of rankings for 10 CACM queries by adding structure to the queries manually. Compare the effectiveness of these queries to the simple queries using MAP, NDCG, and precision at 10. Check for signi cance using the t-test, Wilcoxon test, and the sign test.
338 8 EvaluatingSearchEngines
8.9. For one query in the CACM collection, generate a ranking and calculate BPREF. Show that the two formulations of BPREF give the same value.
8.10. Consider a test collection that contains judgments for a large number of time-sensitive queries, such as “olympics” and “miss universe”. Suppose that the judgments for these queries were made in 2002. Why is this a problem? How can online testing be used to alleviate the problem?
9
Classification and Clustering
“What kind of thing? I need a clear de nition.” Ripley, Alien
We now take a slight detour from search to look at classi cation and clustering. Classi cation and clustering have many things in common with document re- trieval. In fact, many of the techniques that proved to be useful for ranking doc- uments can also be used for these tasks. Classi cation and clustering algorithms are heavily used in most modern search engines, and thus it is important to have a basic understanding of how these techniques work and how they can be applied to real-world problems. We focus here on providing general background knowl- edge and a broad overview of these tasks. In addition, we provide examples of how they can be applied in practice. It is not our goal to dive too deeply into the details or the theory, since there are many other excellent references devoted en- tirely to these subjects, some of which are described in the “References and Future Reading” section at the end of this chapter. Instead, at the end of this chapter, you should know what classi cation and clustering are, the most commonly used algo- rithms, examples of how they are applied in practice, and how they are evaluated. On that note, we begin with a brief description of classi cation and clustering.
Classi cation, also referred to as categorization, is the task of automatically ap- plying labels to data, such as emails, web pages, or images. People classify items throughout their daily lives. It would be infeasible, however, to manually label ev- ery page on the Web according to some criteria, such as “spam” or “not spam.”  erefore, there is a need for automatic classi cation and categorization tech- niques. In this chapter, we describe several classi cation algorithms that are ap- plicable to a wide range of tasks, including spam detection, sentiment analysis, and applying semantic labels to web advertisements.
Clustering, the other topic covered in this chapter, can be broadly de ned as the task of grouping related items together. In classi cation, each item is assigned a
 
340 9 Classi cationandClustering
label, such as “spam” or “not spam.” In clustering, however, each item is assigned to one or more clusters, where the cluster does not necessarily correspond to a mean- ingful concept, such as “spam” or “not spam.” Instead, as we will describe later in this chapter, items are grouped together according to their similarity.  ere- fore, rather than mapping items onto a prede ned set of labels, clustering allows the data to “speak for itself ” by uncovering the implicit structure that relates the items.
Both classi cation and clustering have been studied for many years by infor- mation retrieval researchers, with the aim of improving the effectiveness, or in some cases the efficiency, of search applications. From another perspective, these two tasks are classic machine learning problems. In machine learning, the learning algorithms are typically characterized as supervised or unsupervised. In supervised learning, a model is learned using a set of fully labeled items, which is o en called the training set. Once a model is learned, it can be applied to a set of unlabeled items, called the test set, in order to automatically apply labels. Classi cation is o en cast as a supervised learning problem. For example, given a set of emails that have been labeled as “spam” or “not spam” (the training set), a classi cation model can be learned.  e model then can be applied to incoming emails in order to classify them as “spam” or “not spam”.
Unsupervised learning algorithms, on the other hand, learn entirely based on unlabeled data. Unsupervised learning tasks are o en posed differently than su- pervised learning tasks, since the input data is not mapped to a prede ned set of labels. Clustering is the most common example of unsupervised learning. As we will show, clustering algorithms take a set of unlabeled data as input and then group the items using some notion of similarity.
 ere are many other types of learning paradigms beyond supervised and un- supervised, such as semi-supervised learning, active learning, and online learning. However, these subjects are well beyond the scope of this book. Instead, in this chapter, we provide an overview of basic yet effective classi cation and clustering algorithms and methods for evaluating them.
9.1 Classification and Categorization
Applying labels to observations is a very natural task, and something that most of us do, o en without much thought, in our everyday lives. For example, consider a trip to the local grocery store. We o en implicitly assign labels such as “ripe” or “not ripe,” “healthy” or “not healthy,” and “cheap” or “expensive” to the groceries
9.1 Classi cationandCategorization 341
that we see.  ese are examples of binary labels, since there are only two options for each. It is also possible to apply multivalued labels to foods, such as “starch,” “meat,” “vegetable,” or “fruit.” Another possible labeling scheme would arrange categories into a hierarchy, in which the “vegetable” category would be split by color into subcategories, such as “green,” “red,” and “yellow.” Under this scheme, foods would be labeled according to their position within the hierarchy.  ese different labeling or categorization schemes, which include binary, multivalued, and hierarchical, are called ontologies (see Chapter 6).
It is important to choose an ontology that is appropriate for the underlying task. For example, for detecting whether or not an email is spam, it is perfectly reasonable to choose a label set that consists of “spam” and “not spam”. However, if one were to design a classi er to automatically detect what language a web page is written in, then the set of all possible languages would be a more reasonable ontology. Typically, the correct choice of ontology is dictated by the problem, but in cases when it is not, it is important to choose a set of labels that is expres- sive enough to be useful for the underlying task. However, since classi cation is a supervised learning task, it is important not to construct an overly complex on- tology, since most learning algorithms will fail (i.e., not generalize well to unseen data) when there is little or no data associated with one or more of the labels. In the web page language classi er example, if we had only one example page for each of the Asian languages, then, rather than having separate labels for each of the languages, such as “Chinese”, “Korean”, etc., it would be better to combine all of the languages into a single label called “Asian languages”.  e classi er will then be more likely to classify things as “Asian languages” correctly, since it has more training examples.
In order to understand how machine learning algorithms work, we must  rst take a look at how people classify items. Returning to the grocery store example, consider how we would classify a food as “healthy” or “not healthy.” In order to make this classi cation, we would probably look at the amount of saturated fat, cholesterol, sugar, and sodium in the food. If these values, either separately or in combination, are above some threshold, then we would label the food “healthy” or “unhealthy.” To summarize, as humans we classify items by  rst identifying a number of important features that will help us distinguish between the possible labels. We then extract these features from each item. We then combine evidence from the extracted features in some way. Finally, we classify the item using some decision mechanism based on the combined evidence.
342 9 Classi cationandClustering
In our example, the features are things such as the amount of saturated fat and the amount of cholesterol.  e features are extracted by reading the nutritional information printed on the packaging or by performing laboratory tests.  ere are various ways to combine the evidence in order to quantify the “healthiness” (denoted H) of the food, but one simple way is to weight the importance of each feature and then add the weighted feature values together, such as:
H(food) ≈ wfatfat(food) + wcholchol(food) + wsugarsugar(food) + wsodiumsodium(food)
where wfat, wchol, etc., are the weights associated with each feature. Of course, in this case, it is likely that each of the weights would be negative.
Once we have a healthiness score, H, for a given food, we must apply some decision mechanism in order to apply a “healthy” or “not healthy” label to the food. Again, there are various ways of doing this, but one of the most simple is to apply a simple threshold rule that says “a food is healthy if H(food) ≥ t” for some threshold value t.
Although this is an idealized model of how people classify items, it provides valuable insights into how a computer can be used to automatically classify items. Indeed, the two classi cation algorithms that we will now describe follow the same steps as we outlined earlier.  e only difference between the two algorithms is in the details of how each step is actually implemented.
9.1.1 Naïve Bayes
Wearenowreadytodescribehowitemscanbeautomaticallyclassi ed.Oneofthe most straightforward yet effective classi cation techniques is called Naïve Bayes. We introduced the Bayes classi er in Chapter 7 as a framework for a probabilistic retrieval model. In that case, there were just two classes of interest, the relevant class and the non-relevant class. In general, classi cation tasks can involve more than two labels or classes. In that situation, Bayes’ Rule, which is the basis of a Bayes classi er, states that:
P (C|D) = P (D|C)P (C) P(D)
= ∑ P(D|C)P(C)
c∈C P(D|C = c)P(C = c)
  
9.1 Classi cationandCategorization 343
where C and D are random variables. Random variables are commonly used when modeling uncertainty. Such variables do not have a  xed (deterministic) value. Instead, the value of the variable is random. Every random variable has a set of possible outcomes associated with it, as well as a probability distribution over the outcomes. As an example, the outcome of a coin toss can be modeled as a random variable X.  e possible outcomes of the random variable are “heads” (h) and “tails” (t). Given a fair coin, the probability associated with both the heads out- come and the tails outcome is 0.5.  erefore, P (X = h) = P (X = t) = 0.5.
Consider another example, where you have the algebraic expression Y = 10 + 2X. If X was a deterministic variable, then Y would be deterministic as well.  at is, for a  xed X, Y would always evaluate to the same value. How- ever, if X is a random variable, then Y is also a random variable. Suppose that X had possible outcomes –1 (with probability 0.1), 0 (with probability 0.25), and 1 (with probability 0.65).  e possible outcomes for Y would then be 8, 10, and 12,withP(Y = 8) = 0.1,P(Y = 10) = 0.25,andP(Y = 12) = 0.65.
In this chapter, we denote random variables with capital letters (e.g., C, D) and outcomes of random variables as lowercase letters (e.g., c, d). Furthermore, we denote the entire set of outcomes with caligraphic letters (e.g., C, D). Finally, for notational convenience, instead of writing P (X = x), we write P (x). Similarly for conditional probabilities, rather than writing P (X = x|Y = y), we write P (x|y).
Bayes’ Rule is important because it allows us to write a conditional probability (such as P (C|D)) in terms of the “reverse” conditional (P (D|C)).  is is a very powerful theorem, because it is o en easy to estimate or compute the conditional probability in one direction but not the other. For example, consider spam classi - cation, where D represents a document’s text and C represents the class label (e.g., “spam” or “not spam”). It is not immediately clear how to write a program that detects whether a document is spam; that program is represented by P(C|D). However, it is easy to  nd examples of documents that are and are not spam. It is possible to come up with estimates for P (D|C) given examples or training data.  e magic of Bayes’ Rule is that it tells us how to get what we want (P(C|D)), but may not immediately know how to estimate, from something we do know how to estimate (P (D|C)).
It is straightforward to use this rule to classify items if we let C be the ran- dom variable associated with observing a class label and let D be the random variable associated with observing a document, as in our spam example. Given
344 9 Classi cationandClustering
a document1 d (an outcome of random variable D) and a set of classes C = c1,...,cN (outcomesoftherandomvariableC),wecanuseBayes’Ruletocom- pute P (c1|d), . . . , P (cN |d), which computes the likelihood of observing class label ci given that document d was observed. Document d can then be labeled with the class with the highest probability of being observed given the document.  at is, Naïve Bayes classi es a document d as follows:
Class(d) = arg max P (c|d) c∈C
= arg max ∑ P (d|c)P (c) c∈C c∈C P(d|c)P(c)
where arg maxc∈C P (c|d) means “return the class c, out of the set of all possible classes C, that maximizes P(c|d).”  is is a mathematical way of saying that we are trying to  nd the most likely class c given the document d.
Instead of computing P (c|d) directly, we can compute P (d|c) and P (c) in- stead and then apply Bayes’ Rule to obtain P (c|d). As we explained before, one reason for using Bayes’ Rule is when it is easier to estimate the probabilities of one conditional, but not the other. We now explain how these values are typically estimated in practice.
We  rst describe how to estimate the class prior, P(c).  e estimation is straightforward. It is estimated according to:
P(c) = Nc N
where Nc is the number of training instances that have label c, and N is the total number of training instances.  erefore, P (c) is simply the proportion of training instances that have label c.
Estimating P (d|c) is a little more complicated because the same “counting” estimate that we were able to use for estimating P (c) would not work. (Why? See exercise 9.3.) In order to make the estimation feasible, we must impose the sim- plifying assumption that d can be represented as d = w1, . . . , wn and that wi is independent of wj for every i ̸= j. Simply stated, this says that document d
1  roughout most of this chapter, we assume that the items being classi ed are textual documents. However, it is important to note that the techniques described here can be used in a more general setting and applied to non-textual items such as images and videos.
   
9.1 Classi cationandCategorization 345
can be factored into a set of elements (terms) and that the elements (terms) are independent of each other.2  is assumption is the reason for calling the classi-  er naïve, because it requires documents to be represented in an overly simpli-  ed way. In reality, terms are not independent of each other. However, as we will show in Chapter 11, properly modeling term dependencies is possible, but typi- cally more difficult. Despite the independence assumption, the Naïve Bayes clas- si er has been shown to be robust and highly effective for various classi cation tasks.
 is naïve independence assumption allows us to invoke a classic result from probability that states that the joint probability of a set of (conditionally) inde- pendent random variables can be written as the product of the individual condi- tional probabilities.  at means that P (d|c) can be written as:
∏n i=1
 erefore, we must estimate P (w|c) for every possible term w in the vocabulary V and class c in the ontology C. It turns out that this is a much easier task than estimating P (d|c) since there is a  nite number of terms in the vocabulary and a  nite number of classes, but an in nite number of possible documents.  e inde- pendence assumption allows us to write the probability P (c|d) as:
P (c|d) = ∑ P (d|c)P (c) c∈C P(d|c)P(c)
∏Vi=1 P(wi|c)P(c)
= ∑ ∏V P(wi|c)P(c)
P(d|c) =
P(wi|c)
  c∈C i=1
 e only thing le  to describe is how to estimate P (w|c). Before we can esti- mate the probability, we must  rst decide on what the probability actually means. For example, P (w|c) could be interpreted as “the probability that term w is re- lated to class c,” “the probability that w has nothing to do with class c,” or any number of other things. In order to make the meaning concrete, we must explic- itly de ne the event space that the probability is de ned over. An event space is the
2  is is the same assumption that lies at the heart of most of the retrieval models de- scribed in Chapter 7. It is also equivalent to the bag of words assumption discussed in Chapter 11.
 
346 9 Classi cationandClustering
set of possible events (or outcomes) from some process. A probability is assigned to each event in the event space, and the sum of the probabilities over all of the events in the event space must equal one.
 e probability estimates and the resulting classi cation will vary depending on the choice of event space. We will now brie y describe two of the more popular event spaces and show how P (w|c) is estimated in each.
Multiple-Bernoulli model
 e  rst event space that we describe is very simple. Given a class c, we de ne a binary random variable wi for every term in the vocabulary.  e outcome for the binaryeventiseither0or1. eprobabilityP(wi =1|c)canthenbeinterpreted as “the probability that term wi is generated by class c.” Conversely, P (wi = 0|c) can be interpreted as “the probability that term wi is not generated by class c.”  is is exactly the event space used by the binary independence model (see Chapter 7), and is known as the multiple-Bernoulli event space.
Under this event space, for each term in some class c, we estimate the prob- ability that the term is generated by the class. For example, in a spam classi er, P (cheap = 1|spam) is likely to have a high probability, whereas P (dinner = 1|spam) is going to have a much lower probability.
  document id
 cheap
  buy
banking
 dinner
  the
 class
 1 2 3 4 5 6 7 8 9 10
          0 1 0 1 1 0 0 0 0 1
           0 0 0 0 1 0 1 0 0 1
                  0 1 0 1 0 1 1 0 0 0
 0 0 0 0 0 0 0 0 0 1
           1 1 1 1 1 1 1 1 1 1
                   not spam spam not spam spam spam not spam not spam not spam not spam not spam
           Fig. 9.1. Illustration of how documents are represented in the multiple-Bernoulli event space. In this example, there are 10 documents (each with a unique id), two classes (spam and not spam), and a vocabulary that consists of the terms “cheap”, “buy”, “banking”, “din- ner”, and “the”.
9.1 Classi cationandCategorization 347
Figure 9.1 shows how a set of training documents can be represented in this
event space. In the example, there are 10 documents, two classes (spam and not
spam), and a vocabulary that consists of the terms “cheap”, “buy”, “banking”, “din-
ner”, and “the”. In this example, P (spam) = 3 and P (not spam) = 7 . Next, 10 10
we must estimate P (w|c) for every pair of terms and classes.  e most straight- forward way is to estimate the probabilities using what is called the maximum likelihood estimate, which is:
P(w|c) = dfw,c Nc
where dfw,c is the number of training documents with class label c in which term w occurs, and Nc is the total number of training documents with class label c. As we see, the maximum likelihood estimate is nothing more than the propor- tion of documents in class c that contain term w. Using the maximum likelihood estimate,wecaneasilycomputeP(the|spam) = 1,P(the|notspam) = 1, P (dinner|spam) = 0, P (dinner|not spam) = 1 , and so on.
7
Using the multiple-Bernoulli model, the document likelihood, P (d|c), can be written as:
    ∏ δ(w,d) 1−δ(w,d) P(w|c) (1 − P(w|c))
w∈V
where δ(w, D) is 1 if and only if term w occurs in document d.
In practice, it is not possible to use the maximum likelihood estimate because of the zero probability problem. In order to illustrate the zero probability problem, let us return to the spam classi cation example from Figure 9.1. Suppose that we receive a spam email that happens to contain the term “dinner”. No matter what other terms the email does or does not contain, the probability P (d|c) will always be zero because P(dinner|spam) = 0 and the term occurs in the document (i.e., δdinner,d = 1).  erefore, any document that contains the term “dinner” will automatically have zero probability of being spam.  is problem is more gen- eral, since a zero probability will result whenever a document contains a term that never occurs in one or more classes.  e problem here is that the maximum likeli- hood estimate is based on counting occurrences in the training set. However, the training set is  nite, so not every possible event is observed.  is is known as data sparseness. Sparseness is o en a problem with small training sets, but it can also happen with relatively large data sets.  erefore, we must alter the estimates in such a way that all terms, including those that have not been observed for a given
P(d|c) =
348 9 Classi cationandClustering
class, are given some probability mass.  at is, we must ensure that P (w|c) is non- zero for all terms in V. By doing so, we will avoid all of the problems associated with the zero probability problem.
As was described in Chapter 7, smoothing is a useful technique for overcoming the zero probability problem. One popular smoothing technique is o en called Bayesian smoothing, which assumes some prior probability over models and uses a maximum a posteriori estimate.  e resulting smoothed estimate for the multiple- Bernoulli model has the form:
P(w|c)= dfw,c+αw Nc +αw +βw
where αw and βw are parameters that depend on w. Different settings of these parametersresultindifferentestimates.Onepopularchoiceistosetαw =1and βw =0forallw,whichresultsinthefollowingestimate:
P (w|c) = dfw,c + 1 Nc + 1
Anotherchoiceistosetαw = μNw andβw = μ(1− Nw )forallw,whereNw is NN
the total number of training documents in which term w occurs, and μ is a single tunable parameter.  is results in the following estimate:
    dfw,c +μNw N
Nc + μ
 is event space only captures whether or not the term is generated; it fails to capture how many times the term occurs, which can be an important piece of information. We will now describe an event space that takes term frequency into account.
Multinomial model
 e binary event space of the multiple-Bernoulli model is overly simplistic, as it does not model the number of times that a term occurs in a document. Term fre- quency has been shown to be an important feature for retrieval and classi ca- tion, especially when used on long documents. When documents are very short, it is unlikely that many terms will occur more than one time, and therefore the multiple-Bernoulli model will be an accurate model. However, more o en than
 P(w|c) =
 
9.1 Classi cationandCategorization 349
not, real collections contain documents that are both short and long, and there- fore it is important to take term frequency and, subsequently, document length into account.
 e multinomial event space is very similar to the multiple-Bernoulli event space, except rather than assuming that term occurrences are binary (“term oc- curs” or “term does not occur”), it assumes that terms occur zero or more times (“term occurs zero times”, “term occurs one time”, etc.).
  document id
 cheap
  buy
banking
 dinner
  the
 class
 1 2 3 4 5 6 7 8 9 10
          0 3 0 2 5 0 0 0 0 1
           0 0 0 0 2 0 1 0 0 1
                  0 1 0 3 0 1 1 0 0 0
 0 0 0 0 0 0 0 0 0 1
           2 1 1 2 1 1 1 1 1 2
                   not spam spam not spam spam spam not spam not spam not spam not spam not spam
           Fig. 9.2. Illustration of how documents are represented in the multinomial event space. In this example, there are 10 documents (each with a unique id), two classes (spam and not spam), and a vocabulary that consists of the terms “cheap”, “buy”, “banking”, “dinner”, and “the”.
Figure 9.2 shows how the documents from our spam classi cation example are represented in the multinomial event space.  e only difference between this representation and the multiple-Bernoulli representation is that the events are no longer binary.  e maximum likelihood estimate for the multinomial model is very similar to the multiple-Bernoulli model. It is computed as:
P (w|c) = tfw,c |c|
where tfw,c is the number of times that term w occurs in class c in the training set,
and |c| is the total number of terms that occur in training documents with class la-
bel c. In the spam classi cation example, P (the|spam) = 4 , P (the|not spam) = 20
9 , P (dinner|spam) = 0, and P (dinner|not spam) = 1 . 15 15
    
350 9 Classi cationandClustering
Since terms are now distributed according to a multinomial distribution, the
likelihood of a document d given a class c is computed according to: ( )∏tf
P (d|c) = P (|d|) tfw1,d, tfw2,d, . . . , tfwV,d !
P (w|c) w,d
∝
∏ tf P(w|c) w,d
w∈V
w∈V
where tfw,d is the number of times that term w occurs in document d, |d| is the total number of terms that occur in d, P(|d|) is the probability of generating
a document of length |d|, and (tf , tf w1,d
, . . . , tf )! is the multinomial wV,d
w2,d
coefficient.3 Notice that P (|d|) and the multinomial coefficient are document-
dependent and, for the purposes of classi cation, can be ignored.
 e Bayesian smoothed estimates of the term likelihoods are computed ac-
cording to:
P (w|c) = |c| + ∑
w∈V
αw
tfw,c + αw
 where αw is a parameter that depends on w. As with the multiple-Bernoulli model, different settings of the smoothing parameters result in different types of estimates. Setting αw = 1 for all w is one possible option.  is results in the following estimate:
P (w|c) = tfw,c + 1 |c| + |V|
Another popular choice is to set αw = μcfw , where cfw is the total number of |C|
times that term w occurs in any training document, |C| is the total number of terms in all training documents, and μ, as before, is a tunable parameter. Under this setting, we obtain the following estimate:
|c| + μ
 is estimate may look familiar, as it is exactly the Dirichlet smoothed language
modeling estimate that was described in Chapter 7.
3  e multinomial coefficient is a generalization of the binomial coefficient. It is com-
puted as (N1, N2, . . . , Nk)! = N! . It counts the total number of unique ∑ N1 !N2 !···Nk !
ways that i Ni items (terms) can be arranged given that item i occurs Ni times.
  tfw,c +μcfw P(w|c) = |C|
    
9.1 Classi cationandCategorization 351
In practice, the multinomial model has been shown to consistently outper- form the multiple-Bernoulli model. Implementing a classi er based on either of these models is straightforward. Training consists of computing simple term oc- currence statistics. In most cases, these statistics can be stored in memory, which means that classi cation can be done efficiently.  e simplicity of the model, com- bined with good accuracy, makes the Naïve Bayes classi er a popular and attrac- tive choice as a general-purpose classi cation algorithm.
9.1.2 Support Vector Machines
Unlike the Naïve Bayes classi er, which is based purely on probabilistic princi- ples, the next classi er we describe is based on geometric principles. Support Vec- tor Machines, o en called SVMs, treat inputs such as documents as points in some geometric space. For simplicity, we  rst describe how SVMs are applied to classi-  cation problems with binary class labels, which we will refer to as the “positive” and “negative” classes. In this setting, the goal of SVMs is to  nd a hyperplane4 that separates the positive examples from the negative examples.
In the Naïve Bayes model, documents were treated as binary vectors in the multiple-Bernoulli model and as term frequency vectors in the multinomial case. SVMs provide more  exibility in terms of how documents can be represented. With SVMs, rather than de ning some underlying event space, we must instead de ne a set of feature functions f1(·), . . . , fN (·) that take a document as input and produce what is known as a feature value. Given a document d, the document is representedinanN-dimensionalspacebythevectorxd =[f1(d),...,fN(d)]. Given a set of training data, we can use the feature functions to embed the training documents in this N-dimensional space. Notice that different feature functions will result in different embeddings. Since SVMs  nd a hyperplane that separates the data according to classes, it is important to choose feature functions that will help discriminate between the different classes.
Two common feature functions are fw(d) = δ(w, d) and fw(d) = tfw,d.  e  rst feature function is 1 if term w occurs in d, which is analogous to the multiple- Bernoulli model.  e second feature function counts the number of times that w occurs in d, which is analogous to the multinomial model. Notice that these fea- ture functions are indexed by w, which means that there is a total of |V | such func- tions.  is results in documents being embedded in a |V|-dimensional space. It is also possible to de ne similar feature functions over bigrams or trigrams, which
4 A hyperplane generalizes the notion of a plane to N -dimensional space.
 
352 9 Classi cationandClustering
would cause the dimensionality of the feature space to explode. Furthermore, other information can be encoded in the feature functions, such as the document length, the number of sentences in the document, the last time the document was updated, and so on.
Fig. 9.3. Data set that consists of two classes (pluses and minuses).  e data set on the le  is linearly separable, whereas the one on the right is not.
Now that we have a mechanism for representing documents in an N -dimen- sional space, we describe how SVMs actually classify the points in this space. As described before, the goal of SVMs is to  nd a hyperplane that separates the neg- ative and positive examples.  e hyperplane is learned from the training data. An unseen test point is classi ed according to which side of the hyperplane the point falls on. For example, if the point falls on the negative side, then we classify it as negative. Similarly, if it falls on the positive side, then it is classi ed as positive. It is not always possible to draw a hyperplane that perfectly separates the negative training data from the positive training data, however, since no such hyperplane may exist for some embedding of the training data. For example, in Figure 9.3, it is possible to draw a line (hyperplane) that separates the positive class (denoted by “+”) from the negative class (denoted by “–”) in the le  panel. However, it is impossible to do so in the right panel.  e points in the le  panel are said to be linearly separable, since we can draw a linear hyperplane that separates the points.
It is much easier to de ne and  nd a good hyperplane when the data is linearly separable.  erefore, we begin our explanation of how SVMs work by focusing on this special case. We will then extend our discussion to the more general and common case where the data points are not linearly separable.
  
Case 1: Linearly separable data
w¢x>0 +–
–
9.1 Classi cationandCategorization 353
Suppose that you were given a linearly separable training set, such as the one in Figure 9.3, and were asked to  nd the optimal hyperplane that separates the data points. How would you proceed? You would very likely  rst ask what exactly is meant by optimal. One might  rst postulate that optimal means any hyperplane that separates the positive training data from the negative training data. However, we must also consider the ultimate goal of any classi cation algorithm, which is to generalize well to unseen data. If a classi er can perfectly classify the training data but completely fails at classifying the test set data, then it is of little value.  is scenario is known as over tting.
 +
+
+ +–
+ +
+
– –
– w¢x<0 ––
+
+
–
–
Fig. 9.4. Graphical illustration of Support Vector Machines for the linearly separable case. Here, the hyperplane de ned by w is shown, as well as the margin, the decision regions, and the support vectors, which are indicated by circles.
In order to avoid over tting, SVMs choose the hyperplane that maximizes the separation between the positive and negative data points.  is selection criteria makes sense intuitively, and is backed up by strong theoretical results as well. As- suming that our hyperplane is de ned by the vector w, we want to  nd the w that separates the positive and negative training data and maximizes the separation be-
w¢x=0
w¢x=¡1
w¢x=1
354 9 Classi cationandClustering
tween the data points.  e maximal separation is de ned as follows. Suppose that x− is the closest negative training point to the hyperplane and that x+ is the clos- est positive training point to the hyperplane.5  en, we de ne the margin as the distance from x− to the hyperplane plus the distance from x+ to the hyperplane. Figure 9.4 shows a graphical illustration of the margin, with respect to the hy- perplane and the support vectors (i.e., x+ and x−).  e margin can be computed using simple vector mathematics as follows:
Margin(w)= |w·x−|+|w·x+| ||w||
where · is the dot product (inner product) between two vectors, and ||w|| =
(w · w)1/2 is the length of the vector w.  e SVM algorithm’s notion of an opti-
mal hyperplane, therefore, is the hyperplane w that maximizes the margin while
still separating the data. In order to simplify things, it is typically assumed that
w · x− = −1 and w · x+ = 1.  ese assumptions, which do not change the
solution to the problem, result in the margin being equal to 2 . An alternative ||w||
yet equivalent formulation is to  nd the hyperplane w that solves the following optimization problem:
  minimize: subject to:
1 ||w||2 2
w·xi ≥1 ∀is.t.Class(i)=+ w·xi ≤−1∀is.t.Class(i)=−
  is formulation is o en used because it is easier to solve. In fact, this optimization problem can be solved using a technique called quadratic programming, the details of which are beyond the scope of this book. However, there many excellent open source SVM packages available. In the “References and Further Reading” section at the end of this chapter we provide pointers to several such so ware packages.
Once the best w has been found, an unseen document d can be classi ed using
the following rule:
{+ifw·xd >0 Class(d) = − otherwise
 5  e vectors x− and x+ are known as support vectors.  e optimal hyperplane w is a linear combination of these vectors.  erefore, they provide the support for the decision boundary.  is is the origin of the name “Support Vector Machine.”
9.1 Classi cationandCategorization 355
 erefore, the rule classi es documents based on which side of the hyperplane the document’s feature vector is on. Referring back to Figure 9.4, we see that in this example, those points to the le  of the hyperplane are classi ed as positive exam- ples and those to the right of the hyperplane are classi ed as negative examples.
Case 2: Non-linearly separable data
Very few real-world data sets are actually linearly separable.  erefore, the SVM formulation just described must be modi ed in order to account for this.  is can be achieved by adding a penalty factor to the problem that accounts for training instances that do not satisfy the constraints of the linearly separable formulation.
Suppose that, for some training point x in the positive class, w · x = −0.5.  is violates the constraint w · x ≥ 1. In fact, x falls on the entirely wrong side of the hyperplane. Since the target for w · x is (at least) 1, we can apply a linear penalty based on the difference between the target and actual value.  at is, the penalty given to x is 1 − (−0.5) = 1.5. If w · x = 1.25, then no penalty would be assigned, since the constraint would not be violated.  is type of penalty is known as the hinge loss function. It is formally de ned as:
{ max(1 − w · x, 0) if Class(i) = + L(x)= max(1+w·x,0)ifClass(i)=−
 is loss function is incorporated into the SVM optimization as follows: minimize: 1 ||w||2 + C ∑N ξi
 2 i=1 subject to:
w·xi ≥1−ξi w·xi ≤−1+ξi ξi ≥ 0
∀is.t.Class(i)=+ ∀is.t.Class(i)=− ∀i
where ξi is known as a slack variable that allows the target values to be violated.  e slack variables enforce the hinge loss function. Notice that if all of the con- straints are satis ed, all of the slack variables would be equal to 0, and therefore the loss function would reduce to the linearly separable case. In addition, if any constraint is violated, then the amount by which it is violated is added into the objective function and multiplied by C, which is a free parameter that controls how much to penalize constraint violations. It is standard to set C equal to 1.  is
356 9 Classi cationandClustering
optimization problem  nds a hyperplane that maximizes the margin while allow- ing for some slack. As in the linearly separable case, this optimization problem can be solved using quadratic programming. In addition, classi cation is performed in the same way as the linearly separable case.
The kernel trick
 e example in Figure 9.3 illustrates the fact that certain embeddings of the train- ing data are not linearly separable. It may be possible, however, that a transforma- tion or mapping of the data into a higher dimensional space results in a set of linearly separable points.  is may result in improved classi cation effectiveness, although it is not guaranteed.
 ere are many ways to map an N-dimensional vector into a higher dimen- sional space. For example, given the vector [f1(d), . . . , fN (d)], one could aug- ment the vector by including squared feature values.  at is, the data items would now be represented by the 2N -dimensional vector:
[f1(d),...,fN(d),f1(d)2,...,fN(d)2]
 e higher the dimensionality of the feature vectors, however, the less efficient the algorithm becomes, both in terms of space requirements and computation time.
One important thing to notice is that the key mathematical operation involved in training and testing SVMs is the dot product. If there was an efficient way to compute the dot product between two very high-dimensional vectors without having to store them, then it would be feasible to perform such a mapping. In fact, this is possible for certain classes of high-dimensional mappings.  is can be achieved by using a kernel function. A kernel function takes two N -dimensional vectors and computes a dot product between them in a higher dimensional space.  is higher dimensional space is implicit, in that the higher dimensional vectors are never actually constructed.
Let us now consider an example. Suppose that we have two 2-dimensional vec- tors w = [w1w2] and x = [x1x2]. Furthermore, we de ne Φ(·) as follows:
 x2  √1
 Φ(x) = 2x1x2 x2
Here, Φ(·) maps 2-dimensional vectors into 3-dimensional vectors. As we de- scribed before, this may be useful because the original inputs may actually be lin- early separable in the 3-dimensional space to which Φ(·) maps the points. One can
9.1 Classi cationandCategorization 357
imagine many, many ways of mapping the original inputs into higher dimensional spaces. However, as we will now show, certain mappings have very nice properties that allow us to efficiently compute dot products in the higher dimensional space.
Given this mapping, the naïve way to compute Φ(w) · Φ(x) would be to  rst explicitly construct Φ(w) and Φ(x) and then perform the dot product in the 3- dimensional space. However, surprisingly, it turns out that this is not necessary, since:
Φ(w) · Φ(x) = w12x21 + 2w1w2x1x2 + w2x2 = (w · x)2
where w · x is computed in the original, 2-dimensional space.  erefore, rather than explicitly computing the dot product in the higher 3-dimensional space, we only need to compute the dot product in the original 2-dimensional space and then square the value.  is “trick,” which is o en referred to as the kernel trick, allows us to efficiently compute dot products in some higher dimensional space.
Of course, the example given here is rather trivial.  e true power of the ker- nel trick becomes more apparent when dealing with mappings that project into much higher dimensional spaces. In fact, some kernels perform a dot product in an in nite dimensional space!
Table 9.1. A list of kernels that are typically used with SVMs. For each kernel, the name, value, and implicit dimensionality are given.
A list of the most widely used kernels is given in Table 9.1. Note that the Gaus- sian kernel is also o en called a radial basis function (RBF) kernel.  e best choice of kernel depends on the geometry of the embedded data. Each of these kernels has been shown to be effective on textual features, although the Gaussian kernel tends to work well across a wide range of data sets, as long as the variance (σ2) is properly set. Most standard SVM so ware packages have these kernels built in, so using them is typically as easy as specifying a command-line argument.  erefore,
   Kernel Type
Value
  Implicit Dimension
  Linear
 K(x1, x2) = x1 · x2
 (N) N+p−1
N
    Polynomial
K(x1, x2) = (x1 · x2)p
  Gaussian
K(x1, x2) = exp −||x1 − x2||2/2σ2
  In nite
    
358 9 Classi cationandClustering
given their potential power and their ease of use, it is o en valuable to experiment with each of the kernels to determine the best one to use for a speci c data set and task.
 e availability of these so ware packages, together with the SVM’s  exibil- ity in representation and, most importantly, their demonstrated effectiveness in many applications, has resulted in SVMs being very widely used in classi cation applications.
Non-binary classification
Up until this point, our discussion has focused solely on how support vector ma- chines can be used for binary classi cation tasks. We will now describe two of the most popular ways to turn a binary classi er, such as a support vector machine, into a multi-class classi er.  ese approaches are relatively simple to implement and have been shown to be work effectively.
 e  rst technique is called the one versus all (OVA) approach. Suppose that we have a K ≥ 2 class classi cation problem.  e OVA approach works by train- ing K classi ers. When training the kth classi er, the kth class is treated as the positive class and all of the other classes are treated as the negative class.  at is, each classi er treats the instances of a single class as the positive class, and the remaining instances are the negative class. Given a test instance x, it is classi ed using all K classi ers.  e class for x is the (positive) class associated with the classi er that yields the largest value of w · x.  at is, if wc is the “class c versus not class c” classi er, then items are classi ed according to:
Class(x) = arg max wc · x c
 e other technique is called the one versus one (OVO) approach. In the OVO approach, a binary classi er is trained for every unique pair of classes. For example, for a ternary classi cation problem with the labels “excellent”, “fair”, and “bad”, it would be necessary to train the following classi ers: “excellent versus fair”, “ex- cellent versus bad”, and “fair versus bad”. In general, the OVO approach requires
K(K−1) classi erstobetrained,whichcanbecomputationallyexpensiveforlarge 2
data sets and large values of K. To classify a test instance x, it is run through each of the classi ers. Each time x is classi ed as c, a vote for c is recorded.  e class that has the most votes at the end is then assigned to x.
Both the OVA and OVO approaches work well in practice.  ere is no con- crete evidence that suggests that either should be preferred over the other. Instead,
 
9.1 Classi cationandCategorization 359
the effectiveness of the approaches largely depends on the underlying character- istics of the data set.
9.1.3 Evaluation
Most classi cation tasks are evaluated using standard information retrieval met- rics, such as accuracy,6 precision, recall, the F measure, and ROC curve analysis. Each of these metrics were described in detail in Chapter 8. Of these metrics, the most commonly used are accuracy and the F measure.
 ere are two major differences between evaluating classi cation tasks and other retrieval tasks.  e  rst difference is that the notion of “relevant” is replaced with “is classi ed correctly.”  e other major difference is that microaveraging, which is not commonly used to evaluate retrieval tasks, is widely used in classi-  cation evaluations. Macroaveraging for classi cation tasks involves computing some metric for each class and then computing the average of the per-class met- rics. On the other hand, microaveraging computes a metric for every test instance (document) and then averages over all such instances. It is o en valuable to com- pute and analyze both the microaverage and the macroaverage, especially when the class distribution P (c) is highly skewed.
9.1.4 Classifier and Feature Selection
Up until this point we have covered the basics of two popular classi ers. We have described the principles the classi ers are built upon, their underlying assump- tions, the pros and cons, and how they can be used in practice. As classi cation is a deeply complex and rich subject, we cover advanced classi cation topics in this section that may be of interest to those who would like a deeper or more complete understanding of the topic.
Generative, discriminative, and non-parametric models
 e Naïve Bayes classi er was based on probabilistic modeling.  e model re- quires us to assume that documents are generated from class labels according to a probabilistic model that corresponds to some underlying event space.  e Naïve Bayes classi er is an example of a wider class of probabilistic models called gener- ative models.  ese models assume that some underlying probability distribution
6 Accuracy is another name for precision at rank 1.
 
360 9 Classi cationandClustering
generates both documents and classes. In the Naïve Bayes case, the classes and documents are generated as follows. First, a class is generated according to P (c).  en, a document is generated according to P (d|c).  is process is summarized in Figure 9.5. Generative models tend to appeal to intuition by mimicking how people may actually generate (write) documents.
Generate class according to P(c)
Class 2 Generate document
according to P(d|c)
Fig. 9.5. Generative process used by the Naïve Bayes model. First, a class is chosen accord- ing to P (c), and then a document is chosen according to P (d|c).
Of course, the accuracy of generative models largely depends on how accu- rately the probabilistic model captures this generation process. If the model is a reasonable re ection of the actual generation process, then generative models can be very powerful, especially when there are very few training examples.
As the number of training examples grows, however, the power of the gener- ative model can be limited by simplifying distributional assumptions, such as the independence assumption in the Naïve Bayes classi er. In such cases, discrimina- tive models o en outperform generative models. Discriminative models are those that do not model the generative process of documents and classes. Instead, they directly model the class assignment problem given a document as input. In this way, they discriminate between class labels. Since these models do not need to model the generation of documents, they o en have fewer distributional assump- tions, which is one reason why they are o en preferred to generative models when
                         Class 1 Class 2 Class 3
   
9.1 Classi cationandCategorization 361
there are many training examples. Support vector machines are an example of a discriminative model. Notice that no assumptions about the document genera- tion process are made anywhere in the SVM formulation. Instead, SVMs directly learn a hyperplane that effectively discriminates between the classes.
  –
–
–+ +
– –––+–
–
+
+ +
–
+
– +–––+–+
++–
– +
 +
–– +
+
––+
–
––
++ –
 – –
++ +
–
–
++ +
+
––
+– +++
+––
Fig. 9.6. Example data set where non-parametric learning algorithms, such as a nearest neighbor classi er, may outperform parametric algorithms.  e pluses and minuses indi- cate positive and negative training examples, respectively.  e solid gray line shows the actual decision boundary, which is highly non-linear.
Non-parametric classi ers are another option when there is a large number of training examples. Non-parametric classi ers let the data “speak for itself ” by eliminating all distributional assumptions. One simple example of a non- parametric classi er is the nearest neighbor classi er. Given an unseen example, the nearest neighbor classi er  nds the training example that is nearest (accord- ing to some distance metric) to it.  e unseen example is then assigned the label of this nearest neighbor. Figure 9.6 shows an example output of a nearest neigh- bor classi er. Notice the irregular, highly non-linear decision boundary induced by the classi er. Generative and discriminative models, even SVMs with a non-
+
362 9 Classi cationandClustering
linear kernel, would have a difficult time  tting a model to this data. For this rea- son, the nearest neighbor classi er is optimal as the number of training examples approaches in nity. However, the classi er tends to have a very high variance for smaller data sets, which o en limits its applicability.
Feature selection
 e SVM classi er embeds inputs, such as documents, into some feature space that is de ned by a set of feature functions. As we described, it is common to de-  ne one (or more) feature functions for every word in the vocabulary.  is |V|- dimensional feature space can be extremely large, especially for very large vocabu- laries. Since the feature set size affects both the efficiency and effectiveness of the classi er, researchers have devised techniques for pruning the feature space.  ese are known as feature selection techniques.
 e goal of feature selection is to  nd a small subset of the original features that can be used in place of the original feature set with the aim of signi cantly improving efficiency (in terms of storage and time) while not hurting effective- ness much. In practice, it turns out that feature selection techniques o en improve effectiveness instead of reducing it.  e reason for this is that some of the features eliminated during feature selection may be noisy or inaccurate, and therefore hin- der the ability of the classi cation model to learn a good model.
Information gain is one of the most widely used feature selection criteria for text classi cation applications. Information gain is based on information theory principles. As its name implies, it measures how much information about the class labels is gained when we observe the value of some feature. Let us return to the spam classi cation example in Figure 9.1. Observing the value of the fea- ture “cheap” provides us quite a bit of information with regard to the class labels. If “cheap” occurs, then it is very likely that the label is “spam”, and if “cheap” does not occur, then it is very likely that the label is “not spam”. In information theory, entropy is the expected information contained in some distribution, such as the class distribution P (c).  erefore, the information gain of some feature f mea- sures how the entropy of P (c) changes a er we observe f . Assuming a multiple- Bernoulli event space, it is computed as follows:
IG(w) = H(C) − H(C|w) ∑∑∑
= − P(c)logP(c) + c∈C
w∈{0,1}
P(w) P(c|w)logP(c|w) c∈C
9.1 Classi cationandCategorization 363
where H(C) is the entropy of P(c) and H(C|w) is known as the conditional entropy. As an illustrative example, we compute the information gain for the term “cheap” from our spam classi cation example:
I G(cheap) = −P (spam) log P (spam) − P (spam) log P (spam) + P (cheap)P (spam|cheap) log P (spam|cheap) +
P (cheap)P (spam|cheap) log P (spam|cheap) +
P (cheap)P (spam|cheap) log P (spam|cheap) +
P (cheap)P (spam|cheap) log P (spam|cheap) = − 3 log 3 − 7 log 7 + 4 · 3 log 3
+4 ·1log1+ 6 ·0log0+ 6 ·6log6 10 4 4 10 6 6 10 6 6
= 0.2749
whereP(cheap)isshorthandforP(cheap = 0),P(spam)meansP(C = not spam), and it is assumed that 0 log 0 = 0.  e corresponding information gainsfor“buy”,“banking”,“dinner”,and“the”are0.0008,0.0434,0.3612,and0.0, respectively.  erefore, according to the information gain, “dinner” is the most informative word, since it is a perfect predictor of “not spam” according to the training set. On the opposite side of the spectrum, “the” is the worst predictor, since it appears in every document and therefore has no discriminative power.
Similar information gain measures can be derived for other event spaces, such as the multinomial event space.  ere are many different ways to use the infor- mation gain to actually select features. However, the most common thing to do is to select the K features with the largest information gain and train a model using only those features. It is also possible to select a percentage of all features or use a threshold.
Although many other feature selection criteria exist, information gain tends to be a good general-purpose feature selection criteria, especially for text-based clas- si cation problems. We provide pointers to several other feature selection tech- niques in the “References and Further Reading” section at the end of this chapter.
                   10 10 10 10 10 4 4
           
364 9 Classi cationandClustering
9.1.5 Spam, Sentiment, and Online Advertising
Although ranking functions are a very critical part of any search engine, classi ca- tion and categorization techniques also play an important role in various search- related tasks. In this section, we describe several real-world text classi cation ap- plications.  ese applications are spam detection, sentiment classi cation, and on- line advertisement classi cation.
Spam, spam, spam
Classi cation techniques can be used to help detect and eliminate various types of spam. Spam is broadly de ned to be any content that is generated for malevolent purposes,7 such as unsolicited advertisements, deceptively increasing the ranking of a web page, or spreading a virus. One important characteristic of spam is that it tends to have little, if any, useful content.  is de nition of spam is very subjective, because what may be useful to one person may not be useful to another. For this reason, it is o en difficult to come up with an objective de nition of spam.
 ere are many types of spam, including email spam, advertisement spam, blog spam, and web page spam. Spammers use different techniques for different types of spam.  erefore, there is no one single spam classi cation technique that works for all types of spam. Instead, very specialized spam classi ers are built for the different types of spam, each taking into account domain-speci c information. Much has been written about email spam, and  ltering programs such as Spam- Assassin8 are in common use. Figure 9.7 shows the SpamAssassin output for an example email. SpamAssassin computes a score for the email that is compared to a threshold (default value 5.0) to determine whether it is spam.  e score is based on a combination of features, one of the most important of which is the output of a Bayes classi er. In this case, the URL contained in the body of the email was on a blacklist, the timestamp on the email is later than the time it was received, and the Bayes classi er gives the email a 40–60% chance of being in the class “spam” based on the words in the message.  ese three features did not, however, give the email a score over 5, so it was not classi ed as spam (which is a mistake).
7  e etymology of the word spam, with respect to computer abuse, is quite interesting.  e meaning is believed to have been derived from a 1970 Monty Python skit set in a restaurant where everything on the menu has spam (the meat product) in it. A chorus of Vikings begins singing a song that goes, “Spam, spam, spam, spam, ...” on and on, therefore tying the word spam to repetitive, annoying behavior.
8 http://spamassassin.apache.org/
 
9.1 Classi cationandCategorization 365
To:  ... 
From:  ... 
Subject: non profit debt  X Spam Checked: This message probably not SPAM  X Spam Score: 3.853, Required: 5 
X Spam Level: *** (3.853)  X Spam Tests: BAYES_50,DATE_IN_FUTURE_06_12,URIBL_BLACK  X Spam Report rig:      Start SpamAssassin (v2.6xx cscf) results 
   2.0 URIBL_BLACK            Contains an URL listed in the URIBL blacklist 
                              [URIs: bad debtyh.net.cn] 
   1.9 DATE_IN_FUTURE_06_12   Date: is 6 to 12 hours after Received: date 
   0.0 BAYES_50               BODY: Bayesian spam probability is 40 to 60% 
                              [score: 0.4857] 
 
Say good bye to debt  Acceptable Unsecured Debt includes All Major Credit Cards, No collateral  Bank Loans, Personal Loans,  
Medical Bills etc. 
http://www.bad debtyh.net.cn 
 
Fig. 9.7. Example output of SpamAssassin email spam  lter
Since this book focuses on search engines, we will devote our attention to web page spam, which is one of the most difficult and widespread types of spam. De- tecting web page spam is a difficult task, because spammers are becoming increas- ingly sophisticated. It seems sometimes that the spammers themselves have ad- vanced degrees in information retrieval!  ere are many different ways that spam- mers target web pages. Gyöngyi and Garcia-Molina (2005) proposed a web spam taxonomy that attempts to categorize the different web page spam techniques that are o en used to arti cially increase the ranking of a page.  e two top-level cat- egories of the taxonomy are link spam and term spam.
With link spam, spammers use various techniques to arti cially increase the link-based scores of their web pages. In particular, search engines o en use mea- sures such as inlink count and PageRank, which are based entirely on the link structure of the Web, for measuring the importance of a web page. However, these techniques are susceptible to spam. One popular and easy link spam technique involves posting links to the target web page on blogs or unmoderated message boards. Another way for a website to arti cially increase its link-based score is to join a link exchange network. Link exchange networks are large networks of web-
 
366 9 Classi cationandClustering
sites that all connect to each other, thereby increasing the number of links coming into the site. Another link spam technique is called link farming. Link farms are similar to exchange networks, except the spammer himself buys a large number of domains, creates a large number of sites, and then links them all together.  ere are various other approaches, but these account for a large fraction of link spam. A number of alternatives to PageRank have been proposed recently that attempt to dampen the potential effect of link spam, including HostTrust (Gyöngyi et al., 2004) and SpamRank (Benczúr et al., 2005).
 e other top-level category of spam is term spam. Term spam attempts to modify the textual representation of the document in order to make it more likely to be retrieved for certain queries or keywords. As with link-based scores, term- based scores are also susceptible to spam. Most of the widely used retrieval mod- els, including BM25 and language modeling, make use of some formulation that involves term frequency and document frequency.  erefore, by increasing the term frequency of target terms, these models can easily be tricked into retrieving non-relevant documents. Furthermore, most web ranking functions match text in the incoming anchor text and the URL. Modifying the URL to match a given term or phrase is easy. However, modifying the incoming anchor text requires more effort, but can easily be done using link exchanges and link farms. Another technique, called dumping,  lls documents with many unrelated words (o en an entire dictionary).  is results in the document being retrieved for just about any query, since it contains almost every combination of query terms.  erefore, this acts as a recall enhancing measure.  is can be combined with the other spamming techniques, such as repetition, in order to have high precision as well as high re- call. Phrase stitching (combining words and sentences from various sources) and weaving (adding spam terms into a valid source such as a news story) are other techniques for generating arti cial content. All of these types of term spam should be considered when developing a ranking function designed to prevent spam.
Figure 9.8 shows an example of a web page containing spam.  e page contains both term spam with repetition of important words and link spam where related spam sites are mentioned.
As should be apparent by now, there is an overwhelming number of types of spam. Here, we simply focused on web page spam and did not even start to con- sider the other types of spam. Indeed, it would be easy to write an entire book on subjects related to spam. However, before we end our spam discussion, we will describe just one of the many different ways that classi cation has been used to tackle the problem of detecting web page spam.
Website:
9.1 Classi cationandCategorization 367
      BETTING NFL FOOTBALL PRO FOOTBALL SPORTSBOOKS NFL FOOTBALL LINE ONLINE NFL SPORTSBOOKS NFL
     Players Super Book
When It Comes To Secure NFL Betting And Finding The Best Football Lines Players Super Book Is The Best Option! Sign Up And Ask For 30 % In Bonuses.
MVP Sportsbook
Football Betting Has Never been so easy and secure! MVP Sportsbook has all the NFL odds you are looking for. Sign Up Now and ask for up to
30 % in Cash bonuses.
Term spam:
pro football sportsbooks nfl football line online nfl sportsbooks nfl football  gambling odds online pro nfl betting pro nfl gambling online nfl football  spreads offshore football gambling online nfl gamblibg spreads online  football gambling line online nfl betting nfl sportsbook online online nfl betting spreads betting nfl football online online football wagering online  gambling online gambling football online nfl football betting odds offshore  football sportsbook online nfl football gambling ...
Link spam:
MVP Sportsbook Football Gambling  Beverly Hills Football Sportsbook Players SB Football Wagering  Popular Poker Football Odds  Virtual Bookmaker Football Lines  V Wager Football Spreads 
Bogarts Casino Football Point Spreads  Gecko Casino Online Football Betting  Jackpot Hour Online Football Gambling  MVP Casino Online Football Wagering  Toucan Casino NFL Betting  Popular Poker NFL Gambling  All Tracks NFL Wagering  Bet Jockey NFL Odds 
Live Horse Betting NFL Lines  MVP RacebookNFL Point Spreads Popular Poker NFL Spreads  Bogarts Poker NFL Sportsbook ...
Fig. 9.8. Example of web page spam, showing the main page and some of the associated term and link spam
                  
368 9 Classi cationandClustering
Ntoulas et al. (2006) propose a method for detecting web page spam using content (textual) analysis.  e method extracts a large number of features from each web page and uses these features in a classi er. Some of the features include the number of words on the page, number of words in the title, average length of the words, amount of incoming anchor text, and the fraction of visible text.  ese features attempt to capture very basic characteristics of a web page’s text. Another feature used is the compressibility of the page, which measures how much the page can be reduced in size using a compression algorithm. It turns out that pages that can be compressed more are much more likely to be spam, since pages that contain many repeated terms and phrases are easier to compress.  is has also been shown to be effective for detecting email spam.  e authors also use as features the fraction of terms drawn from globally popular words9 and the fraction of globally popular words appearing on the page.  ese features attempt to capture whether or not the page has been  lled with popular terms that are highly likely to match a query term.  e last two features are based on n-gram likelihoods. Experiments show that pages that contain very rare and very common n-grams are more likely to be spam than those pages that contain n-grams of average likelihood. All of these features were used with a decision tree learning algorithm, which is another type of supervised classi cation algorithm, and shown to achieve classi cation accuracy well above 90%.  e same features could easily be used in a Naïve Bayes or Support Vector Machine classi er.
Sentiment
As we described in Chapter 6, there are three primary types of web queries.  e models described in Chapter 7 focus primarily on informational and navigational queries. Transactional queries, the third type, present many different challenges. If a user queries for a product name, then the search engine should display a vari- ety of information that goes beyond the standard ranked list of topically relevant results. For example, if the user is interested in purchasing the product, then links to online shopping sites can be provided to help the user complete her purchase. It may also be possible that the user already owns the product and is searching for accessories or enhancements.  e search engine could then derive revenue from the query by displaying advertisements for related accessories and services.
9  e list of “globally popular” words in this experiment was simply the N most frequent words in the test corpus.
 
9.1 Classi cationandCategorization 369
Another possible scenario, and the one that we focus on in detail here, is that the user is researching the product in order to determine whether he should pur- chase it. In this case, it would be valuable to retrieve information such as product speci cations, product reviews, and blog postings about the product. In order to reduce the amount of information that the user needs to read through, it would be preferable to have the system automatically aggregate all of the reviews and blog posts in order to present a condensed, summarized view.
 ere are a number of steps involved with building such a system, each of which involves some form of classi cation. First, when crawling and indexing sites, the system has to automatically classify whether or not a web page contains a review or if it is a blog posting expressing an opinion about a product.  e task of identifying opinionated text, as opposed to factual text, is called opinion de- tection. A er a collection of reviews and blog postings has been populated, an- other classi er must be used to extract product names and their corresponding reviews.  is is the information extraction task. For each review identi ed for a given product, yet another classi er must be used to determine the sentiment of the page. Typically, the sentiment of a page is either “negative” or “positive”, al- though the classi er may choose to assign a numeric score as well, such as “two stars” or “four stars”. Finally, all of the data, including the sentiment, must be ag- gregated and presented to the user in some meaningful way. Figure 9.9 shows part of an automatically generated product review from a web service.  is sentiment- based summary of various aspects of the product, such as “ease of use”, “size”, and “so ware”, is generated from individual user reviews.
Rather than go into the details of all of these different classi ers, we will focus our attention on how sentiment classi ers work. As with our previous examples, let us consider how a person would identify the sentiment of some piece of text. For a majority of cases, we use vocabulary clues in order to determine the senti- ment. For example, a positive digital camera review would likely contain words such as “great”, “nice”, and “amazing”. On the other hand, negative reviews would contain words such as “awful”, “terrible”, and “bad”.  is suggests one possible so- lution to the problem, where we build two lists.  e  rst list will contain words that are indicative of positive sentiment, and another list will contain words in- dicative of negative sentiment.  en, given a piece of text, we could simply count the number of positive words and the number of negative words. If there are more positive words, then assign the text a positive sentiment label. Otherwise, label it as having negative sentiment. Even though this approach is perfectly reasonable, it turns out that people are not very good at creating lists of words that indicate pos-
370 9 Classi cationandClustering
All user reviews 
General Comments (148 comments)      82% positive 
Ease of Use (108 comments)                                                                  78% positive  Screen (92 comments)                                                                  97% positive  Software (78 comments)                                                                  35% positive  Sound Quality (59 comments)                                                                  89% positive  Size (59 comments)                                                                 76% positive 
Fig. 9.9. Example product review incorporating sentiment
itive and negative sentiment.  is is largely due to the fact that human language is ambiguous and largely dependent on context. For example, the text “the digital camera lacks the amazing picture quality promised” would likely be classi ed as having positive sentiment because it contains two positive words (“amazing” and “quality”) and only one negative word (“lacks”).
Pang et al. (2002) proposed using machine learning techniques for sentiment classi cation. Various classi ers were explored, including Naïve Bayes, Support Vector Machines, and maximum entropy, which is another popular classi cation technique.  e features used in the classi ers were unigrams, bigrams, part-of- speech tags, adjectives, and the position of a term within a piece of text.  e au- thors report that an SVM classi er using only unigram features exhibited the best performance, resulting in more accurate results than a classi er trained using all of the features. In addition, it was observed that the multiple-Bernoulli event space outperformed the multinomial event space for this particular task.  is is likely caused by the fact that most sentiment-related terms occur only once in any piece of text, and therefore term frequency adds very little to the model. Interestingly, the machine learning models were signi cantly more accurate than the baseline model that used human-generated word lists.  e SVM classi er with unigrams
                                                    
           
     
9.1 Classi cationandCategorization 371
had an accuracy of over 80%, whereas the baseline model had an accuracy of only around 60%.
Classifying advertisements
As described in Chapter 6, sponsored search and content match are two differ- ent advertising models widely used by commercial search engines.  e former matches advertisements to queries, whereas the latter matches advertisements to web pages. Both sponsored search and content match use a pay per click pricing model, which means that advertisers must pay the search engine only if a user clicks on the advertisement. A user may click on an advertisement for a number of reasons. Clearly, if the advertisement is “topically relevant,” which is the stan- dard notion of relevance discussed in the rest of this book, then the user may click on it. However, this is not the only reason why a user may click. If a user searches for “tropical  sh”, she may click on advertisements for pet stores, local aquariums, or even scuba diving lessons. It is less likely, however, that she would click on ad- vertisements for  shing,  sh restaurants, or mercury poisoning.  e reason for this is that the concept “tropical  sh” has a certain semantic scope that limits the type of advertisements a user may  nd interesting.
Although it is possible to use standard information retrieval techniques such as query expansion or query reformulation analysis to  nd these semantic matches for advertising, it is also possible to use a classi er that maps queries (and web pages) into semantic classes. Broder et al. (2007) propose a simple yet effective technique for classifying textual items, such as queries and web pages, into a se- mantic hierarchy.  e hierarchy was manually constructed and consists of over 6,000 nodes, where each node represents a single semantic class. As one moves deeper down the hierarchy, the classes become more speci c. Human judges man- ually placed thousands of queries with commercial intent into the hierarchy based on each query’s intended semantic meaning.
Given such a hierarchy and thousands of labeled instances, there are many pos- sible ways to classify unseen queries or web pages. For example, one could learn a Naïve Bayes model or use SVMs. Since there are over 6,000 classes, however, there could be data sparsity issues, with certain classes having very few labeled instances associated with them. A bigger problem, however, would be the efficiency of this approach. Both Naïve Bayes and SVMs would be very slow to classify an item into one of 6,000 possible classes. Since queries must be classi ed in real time, this is not an option. Instead, Broder et al. propose using cosine similarity with tf.idf
372 9 Classi cationandClustering
weighting to match queries (or web pages) to semantic classes.  at is, they frame the classi cation problem as a retrieval problem, where the query is the query (or web page) to be classi ed and the document set consists of 6,000 “documents”, one for each semantic class. For example, for the semantic class “Sports”, the “doc- ument” for it would consists of all of the queries labeled as “Sports”.  ese “doc- uments” are stored in an inverted index, which allows for efficient retrieval for an incoming query (or web page).  is can be viewed as an example of a nearest neighbor classi er.
        Aquariums
                   Fish
Rainbow Fish Resources
Web Page
Supplies
Ad
                        Fig. 9.10. Example semantic class match between a web page about rainbow  sh (a type of tropical  sh) and an advertisement for tropical  sh food.  e nodes “Aquariums”, “Fish”, and “Supplies” are example nodes within a semantic hierarchy.  e web page is classi ed as “Aquariums - Fish” and the ad is classi ed as “Supplies - Fish”. Here, “Aquariums” is the least common ancestor. Although the web page and ad do not share any terms in common, they can be matched because of their semantic similarity.
Discount Tropical Fish Food
Feed your tropical fish a gourmet diet for just pennies a day! www.cheapfishfood.com
 
9.2 Clustering 373
To use such a classi er in practice, one would have to preclassify every adver- tisement in the advertising inventory.  en, when a new query (or web page) ar- rives, it is classi ed.  ere are a number of ways to use the semantic classes to improve matching. Obviously, if the semantic class of a query exactly matches the semantic class of an advertisement, it should be given a high score. However, there are other cases where two things may be very closely related, even though they do not have exactly the same semantic class.  erefore, Broder et al. propose a way of measuring the distance between two semantic classes within the hierarchy based on the inverse of the least common ancestor of the two nodes in the hierarchy. A common ancestor is a node in the hierarchy that you must pass through in order to reach both nodes.  e least common ancestor is the one with the maximum depth in the hierarchy.  e distance is 0 if the two nodes are the same and very large if the the least common ancestor is the root node. Figure 9.10 shows an ex- ample of how a web page can be semantically matched to an advertisement using the hierarchy. In the  gure, the least common ancestor of the web page and ad classes is “Aquariums”, which is one node up the hierarchy.  erefore, this match would be given a lower score than if both the web page and ad were classi ed into the same node in the hierarchy.  e full advertisement score can be computed by combining this distance based on the hierarchy with the standard cosine similar- ity score. In this way, advertisements are ranked in terms of both topical relevance and semantic relevance.
9.2 Clustering
Clustering algorithms provide a different approach to organizing data. Unlike the classi cation algorithms covered in this chapter, clustering algorithms are based on unsupervised learning, which means that they do not require any training data. Clustering algorithms take a set of unlabeled instances and group (cluster) them together. One problem with clustering is that it is o en an ill-de ned problem. Classi cation has very clear objectives. However, the notion of a good clustering is o en de ned very subjectively.
In order to gain more perspective on the issues involved with clustering, let us examine how we, as humans, cluster items. Suppose, once again, that you are at a grocery store and are asked to cluster all of the fresh produce (fruits and vegeta- bles). How would you proceed? Before you began, you would have to decide what criteria you would use for clustering. For example, you could group the items by their color, their shape, their vitamin C content, their price, or some meaningful
374 9 Classi cationandClustering
combination of these factors. As with classi cation, the clustering criteria largely depend on how the items are represented. Input instances are assumed to be a fea- ture vector that represents some object, such as a document (or a fruit). If you are interested in clustering according to some property, it is important to make sure that property is represented in the feature vector.
A er the clustering criteria have been determined, you would have to deter- mine how you would assign items to clusters. Suppose that you decided to cluster the produce according to color and you have created a red cluster (red grapes, red apples) and a yellow cluster (bananas, butternut squash). What do you do if you come across an orange? Do you create a new orange cluster, or do you assign it to the red or yellow cluster?  ese are important questions that clustering algorithms must address as well.  ese questions come in the form of how many clusters to use and how to assign items to clusters.
Finally, a er you have assigned all of the produce to clusters, how do you quan- tify how well you did?  at is, you must evaluate the clustering.  is is o en very difficult, although there have been several automatic techniques proposed.
In this example, we have described clusters as being de ned by some  xed set of properties, such as the “red” cluster.  is is, in fact, a very speci c form of clus- ter, called monothetic. We discussed monothetic classes or clusters in Chapter 6, and mentioned that most clustering algorithms instead produce polythetic clus- ters, where members of a cluster share many properties, but there is no single de n- ing property. In other words, membership in a cluster is typically based on the similarity of the feature vectors that represent the objects.  is means that a cru- cial part of de ning the clustering algorithm is specifying the similarity measure that is used.  e classi cation and clustering literature o en refers to a distance measure, rather than a similarity measure, and we use that terminology in the fol- lowing discussion. Any similarity measure, which typically has a value S from 0 to 1, can be converted into a distance measure by using 1 − S. Many similarity and distance measures have been studied by information retrieval and machine learn- ing researchers, from very simple measures such as Dice’s coefficient (mentioned in Chapter 6) to more complex probabilistic measures.
 e reader should keep these factors in mind while reading this section, as they will be recurring themes throughout.  e remainder of this section describes three clustering algorithms based on different approaches, discusses evaluation issues, and brie y describes clustering applications.
9.2.1 Hierarchical and K-Means Clustering
We will now describe two different clustering algorithms that start with some ini- tial clustering of the data and then iteratively try to improve the clustering by op- timizing some objective function.  e main difference between the algorithms is the objective function. As we will show, different objective functions lead to dif- ferent types of clusters.  erefore, there is no one “best” clustering algorithm.  e choice of algorithm largely depends on properties of the data set and task.
 roughout the remainder of this section, we assume that our goal is to cluster some set of N instances (which could be web pages, for example), represented as feature vectors, into K clusters, where K is a constant that is  xed a priori.
Hierarchical clustering
Hierarchical clustering is a clustering methodology that builds clusters in a hier- archical fashion.  is methodology gives rise to a number of different clustering algorithms.  ese algorithms are o en “clustered” into two groups, depending on how the algorithm proceeds.
Divisive clustering algorithms begin with a single cluster that consists of all of the instances. During each iteration it chooses an existing cluster and divides it into two (or possibly more) clusters.  is process is repeated until there are a total of K clusters.  e output of the algorithm largely depends on how clusters are chosen and split.
Divisive clustering is a top-down approach.  e other general type of hierar- chical clustering algorithm is called agglomerative clustering, which is a bottom-up approach. Figures 9.11 and 9.12 illustrate the difference between the two types of algorithms. An agglomerative algorithm starts with each input as a separate cluster.  at is, it begins with N clusters, each of which contains a single input.  e algorithm then proceeds by joining two (or possibly more) existing clusters to form a new cluster.  erefore, the number of clusters decreases a er each itera- tion.  e algorithm terminates when there are K clusters. As with divisive cluster- ing, the output of the algorithm is largely dependent on how clusters are chosen and joined.
 e hierarchy generated by an agglomerative or divisive clustering algorithm can be conveniently visualized using a dendrogram.10 A dendrogram graphically represents how a hierarchical clustering algorithm progresses. Figure 9.13 shows
10 From the Greek word dendron, meaning “tree.”
9.2 Clustering 375
 
376 9 Classi cationandClustering
Fig. 9.11. Example of divisive clustering with K = 4.  e clustering proceeds from le  to right and top to bottom, resulting in four clusters.
the dendrogram that corresponds to generating the entire agglomerative cluster- ing hierarchy for the points in Figure 9.12. In the dendrogram, points D and E are  rst combined to form a new cluster called H.  en, B and C are combined to form cluster I.  is process is continued until a single cluster M is created, which consists of A, B, C, D, E, and F. In a dendrogram, the height at which instances combine is signi cant and represents the similarity (or distance) value at which the combination occurs. For example, the dendrogram shows that D and E are the most similar pair.
Algorithm 1 is a simple implementation of hierarchical agglomerative clus- tering.11  e algorithm takes N vectors X1, . . . , XN , representing the instances, and the desired number of clusters K as input.  e array (vector) A is the assign- ment vector. It is used to keep track of which cluster each input is associated with. If A[i] = j, then it means that input Xi is in cluster j.  e algorithm considers joiningeverypairofclusters.Foreachpairofclusters(Ci,Cj),acostC(Ci,Cj) is computed.  e cost is some measure of how expensive it would be to merge
11 O en called HAC in the literature.
   A
E
B
C
  D
      A
G F
E
D
G F
      B
C
     A
E
B
C
   D
      A
E
G F
G F
   D
    B
C
    
9.2 Clustering 377
Fig. 9.12. Example of agglomerative clustering with K = 4.  e clustering proceeds from le  to right and top to bottom, resulting in four clusters.
M
I
    A
    E
D
B
C
G F
                 A
    E
D
B
C
G F
                A
    E
D
B
C
G F
                A
    E
D
B
C
G F
                                     L
H
K
                                  J
                                                                         ABCDEFG
Fig. 9.13. Dendrogram that illustrates the agglomerative clustering of the points from Figure 9.12
     
378 9 Classi cationandClustering Algorithm 1 Agglomerative Clustering
  1: 2: 3: 4: 5: 6: 7: 8: 9:
10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25:
procedureAC(X1,...,XN,K) A[1],...,A[N] ← 1,...,N
ids ← {1,...,N}
for c = N to K do
bestcost ← ∞ bestclusterA ← unde ned bestclusterB ← unde ned fori ∈ idsdo
forj ∈ ids−{i}do
ci,j ← COST(Ci,Cj) if ci,j < bestcost then
bestcost ← ci,j bestclusterA ← i bestclusterB ← j
end if end for
end for
ids ← ids − {bestClusterA} fori = 1toN do
if A[i] is equal to bestClusterA then A[i] ← bestClusterB
end if end for
end for endprocedure
 clusters Ci and Cj . We will return to how the cost is computed shortly. A er all pairwise costs have been computed, the pair of clusters with the lowest cost are then merged.  e algorithm proceeds until there are K clusters.
As shown by Algorithm 1, agglomerative clustering largely depends on the cost function.  ere are many different ways to de ne the cost function, each of which results in the  nal clusters having different characteristics. We now describe a few of the more popular choices and the intuition behind them.
Single linkage measures the cost between clusters Ci and Cj by computing the distance between every instance in cluster Ci and every one in Cj .  e cost is then the minimum of these distances, which can be stated mathematically as:
9.2 Clustering 379
COST(Ci,Cj) = min{dist(Xi,Xj)|Xi ∈ Ci,Xj ∈ Cj}
where dist is the distance between input Xi and Xj . It is typically computed us- ing the Euclidean distance12 between Xi and Xj , but many other distance mea- sures have been used. Single linkage relies only on the minimum distance between the two clusters. It does not consider how far apart the remainder of the instances in the clusters are. For this reason, single linkage could result in very “long” or spread-out clusters, depending on the structure of the two clusters being com- bined.
Complete linkage is similar to single linkage. It begins by computing the dis- tance between every instance in cluster Ci and Cj. However, rather than using the minimum distance as the cost, it uses the maximum distance.  at is, the cost is:
COST(Ci,Cj) = max{dist(Xi,Xj)|Xi ∈ Ci,Xj ∈ Cj}
Since the maximum distance is used as the cost, clusters tend to be more compact
and less spread out than in single linkage.
  Cluster A
Cluster C
Cluster B
            Fig. 9.14. Examples of clusters in a graph formed by connecting nodes representing in- stances. A link represents a distance between the two instances that is less than some threshold value.
To illustrate the difference between single-link and complete-link clusters, consider the graph shown in Figure 9.14.  is graph is formed by representing in-
 12  e Euclidean distance between two vectors x and y is computed according to
√∑ (xi − yi)2 where the subscript i denotes the ith component of the vector. i
 
380 9 Classi cationandClustering
stances (i.e., the Xis) as nodes and connecting nodes where dist(Xi, Xj ) < T , where T is some threshold value. In this graph, clusters A, B, and C would all be single-link clusters.  e single-link clusters are, in fact, the connected compo- nents of the graph, where every member of the cluster is connected to at least one other member.  e complete-link clusters would be cluster A, the singleton clus- ter C, and the upper and lower pairs of instances from cluster B.  e complete- link clusters are the cliques or maximal complete subgraphs of the graph, where every member of the cluster is connected to every other member.
Average linkage uses a cost that is a compromise between single linkage and complete linkage. As before, the distance between every pair of instances in Ci and Cj is computed. As the name implies, average linkage uses the average of all of the pairwise costs.  erefore, the cost is:
∑
COST(Ci,Cj) =
∑ dist(Xi , Xj ) Xi∈Ci,Xj∈Cj
|Ci ||Cj |
 where |Ci| and |Cj| are the number of instances in clusters Ci and Cj, respec- tively.  e types of the clusters formed using average linkage depends largely on the structure of the clusters, since the cost is based on the average of the distances between every pair of instances in the two clusters.
Average group linkage is closely related to average linkage.  e cost is computed according to:
COST(Ci,Cj) = dist(μCi,μCj )
X∈C X isthecentroidofclusterCi. ecentroidofaclusterissim-
|C|
whereμC =
ply the average of all of the instances in the cluster. Notice that the centroid is also a vector with the same number of dimensions as the input instances.  erefore, average group linkage represents each cluster according to its centroid and mea- sures the cost by the distance between the centroids.  e clusters formed using average group linkage are similar to those formed using average linkage.
 Figure 9.15 provides a visual summary of the four cost functions described up to this point. Speci cally, it shows which pairs of instances (or centroids) are involved in computing the cost functions for the set of points used in Figures 9.11 and 9.12.
Ward’s method is the  nal method that we describe. Unlike the previous costs, which are based on various notions of the distance between two clusters, Ward’s method is based on the statistical property of variance.  e variance of a set of numbers measures how spread out the numbers are. Ward’s method attempts to
9.2 Clustering 381
Fig. 9.15. Illustration of how various clustering cost functions are computed
minimize the sum of the variances of the clusters.  is results in compact clusters with a minimal amount of spread around the cluster centroids.  e cost, which is slightly more complicated to compute than the previous methods, is computed according to:
∑∑
(X −μCk)·(X −μCk)+ (X −μCi∪Cj)·(X −μCi∪Cj)
X ∈Ci ∪Cj
whereCi∪Cj istheunionoftheinstancesinclustersCi andCj,andμCi∪Cj isthe centroid of the cluster consisting of the instances in Ci ∪ Cj .  is cost measures what the intracluster variance would be if clusters i and j were joined.
So, which of the  ve agglomerative clustering techniques is the best? Once again the answer depends on the data set and task the algorithm is being applied
  A
D
B
C
Single Linkage E
G F
                         A
D
B
C
Complete Linkage E
G F
                      A
D
B
C
Average Linkage E
G F
                    μ
Average Group Linkage
     μ
μ
    μ
      COST(Ci,Cj)=
k̸=i,j X∈Ck ∑
382 9 Classi cationandClustering
to. If the underlying structure of the data is known, then this knowledge may be used to make a more informed decision about the best algorithm to use. Typically, however, determining the best method to use requires experimentation and eval- uation. In the information retrieval experiments that have involved hierarchical clustering, for example, average-link clustering has generally had the best effec- tiveness. Even though clustering is an unsupervised method, in the end there is still no such thing as a free lunch, and some form of manual evaluation will likely be required.
Efficiency is a problem with all hierarchical clustering methods. Because the computation involves the comparison of every instance to all other instances, even the most efficient implementations are O(N2) for N instances.  is limits the number of instances that can be clustered in an application.  e next cluster- ing algorithm we describe, K-means, is more efficient because it produces a  at clustering, or partition, of the instances, rather than a hierarchy.
K -means
 e K-means algorithm is fundamentally different than the class of hierarchical clustering algorithms just described. For example, with agglomerative clustering, the algorithm begins with N clusters and iteratively combines two (or more) clus- ters together based on how costly it is to do so. As the algorithm proceeds, the number of clusters decreases. Furthermore, the algorithm has the property that once instances Xi and Xj are in the same cluster as each other, there is no way for them to end up in different clusters as the algorithm proceeds.
With the K-means algorithm, the number of clusters never changes.  e al- gorithm starts with K clusters and ends with K clusters. During each iteration of the K-means algorithm, each instance is either kept in the same cluster or as- signed to a different cluster.  is process is repeated until some stopping criteria is met.
 e goal of the K-means algorithm is to  nd the cluster assignments, repre- sented by the assignment vector A[1], . . . , A[N ], that minimize the following cost function:
∑K ∑ k=1 i:A[i]=k
where dist(Xi, Ck) is the distance between instance Xi and class Ck. As with the various hierarchical clustering costs, this distance measure can be any reasonable
COST(A[1],...,A[N]) =
dist(Xi,Ck)
9.2 Clustering 383 measure, although it is typically assumed to be the following:
dist(Xi, Ck) = ||Xi − μCk ||2
=(Xi −μCk)·(Xi −μCk)
whichistheEuclideandistancebetweenXiandμCk squared.Here,asbefore,μCk isthecentroidofclusterCk.Noticethatthisdistancemeasureisverysimilartothe cost associated with Ward’s method for agglomerative clustering.  erefore, the method attempts to  nd the clustering that minimizes the intracluster variance of the instances.
Alternatively,thecosinesimilaritybetweenXiandμCk canbeusedasthedis- tance measure. As described in Chapter 7, the cosine similarity measures the angle between two vectors. For some text applications, the cosine similarity measure has been shown to be more effective than the Euclidean distance.  is speci c form of K-means is o en called spherical K-means.
One of the most naïve ways to solve this optimization problem is to try every possible combination of cluster assignments. However, for large data sets this is computationally intractable, because it requires computing an exponential num- ber of costs. Rather than  nding the globally optimal solution, the K-means al- gorithm  nds an approximate, heuristic solution that iteratively tries to minimize the cost.  is solution returned by the algorithm is not guaranteed to be the global optimal. In fact, it is not even guaranteed to be locally optimal. Despite its heuris- tic nature, the algorithm tends to work very well in practice.
Algorithm 2 lists the pseudocode for one possible K-means implementation.  e algorithm begins by initializing the assignment of instances to clusters.  is can be done either randomly or by using some knowledge of the data to make a more informed decision. An iteration of the algorithm then proceeds as follows. Each instance is assigned to the cluster that it is closest to, in terms of the distance measure dist(Xi, Ck).  e variable change keeps track of whether any of the in- stances changed clusters during the current iteration. If some have changed, then the algorithm proceeds. If none have changed, then the algorithm ends. Another reasonable stopping criterion is to run the algorithm for some  xed number of iterations.
In practice, K-means clustering tends to converge very quickly to a solution. Even though it is not guaranteed to  nd the optimal solution, the solutions re- turned are o en optimal or close to optimal. When compared to hierarchical clus- tering, K-means is more efficient. Speci cally, since KN distance computations
384 9 Classi cationandClustering Algorithm 2 K-Means Clustering
  1: 2: 3: 4: 5: 6: 7: 8: 9:
10: 11: 12: 13:
procedure KMC(X1, . . . , XN , K) A[1],...,A[N]← initialclusterassignment repeat
change ← false fori = 1toN do
kˆ ← argmink dist(Xi,Ck) if A[i] is not equal kˆ then
A[i] ← kˆ
change ← true end if
end for
until change is equal to f alse return A[1], . . . , A[N ]
endprocedure
 are done in every iteration and the number of iterations is small, implementa- tions of K-means are O(KN) rather than the O(N2) complexity of hierarchi- cal methods. Although the clusters produced by K-means depend on the starting points chosen (the initial clusters) and the ordering of the input data, K-means generally produces clusters of similar quality to hierarchical methods.  erefore, K-means is a good choice for an all-purpose clustering algorithm for a wide range of search engine–related tasks, especially for large data sets.
9.2.2 K Nearest Neighbor Clustering
Even though hierarchical and K-means clustering are different from an algorith- mic point of view, one thing that they have in common is the fact that both algo- rithms place every input into exactly one cluster, which means that clusters do not overlap.13  erefore, these algorithms partition the input instances into K parti- tions (clusters). However, for certain tasks, it may be useful to allow clusters to overlap. One very simple way of producing overlapping clusters is called K near- est neighbor clustering. It is important to note that the K here is very different from the K in K-means clustering, as will soon become very apparent.
13 Note that this is true for hierarchical clusters at a given level of the dendrogram (i.e., at a given similarity or distance value). Clusters from different levels of the dendrogram do overlap.
 
9.2 Clustering 385
Fig. 9.16. Example of overlapping clustering using nearest neighbor clustering with K = 5.  e overlapping clusters for the black points (A, B, C, and D) are shown.  e  ve nearest neighbors for each black point are shaded gray and labeled accordingly.
In K nearest neighbor clustering, a cluster is formed around every input in- stance. For input instance x, the K points that are nearest to x according to some distance metric and x itself form a cluster. Figure 9.16 shows several examples of nearest neighbor clusters with K = 5 formed for the points A, B, C, and D. Al- though the  gure only shows clusters around four input instances, in reality there would be one cluster per input instance, resulting in N clusters.
As Figure 9.16 illustrates, the algorithm o en fails to  nd meaningful clusters. In sparse areas of the input space, such as around D, the points assigned to cluster D are rather far away and probably should not be placed in the same cluster as D. However, in denser areas of the input space, such as around B, the clusters are better de ned, even though some related inputs may be missed because K is not large enough. Applications that use K nearest neighbor clustering tend to emphasize  nding a small number of closely related instances in the K nearest neighbors (i.e., precision) over  nding all the closely related instances (recall).
K nearest neighbor clustering can be rather expensive, since it requires com- puting distances between every pair of input instances. If we assume that com- puting the distance between two input instances takes constant time with respect
     A
A
A
                     A
A
D D
D
D
D
D
A
                                                                                   C
                                     BB CC
                                                         B
        B
B
B
C
C
                                         C
       
386 9 Classi cationandClustering
to K and N, then this computation takes O(N2) time. A er all of the distances have been computed, it takes at most O(N2) time to  nd the K nearest neigh- bors for each point.  erefore, the total time complexity for K nearest neighbor clustering is O(N2), which is the same as hierarchical clustering.
For certain applications, K nearest neighbor clustering is the best choice of clustering algorithm.  e method is especially useful for tasks with very dense in- put spaces where it is useful or important to  nd a number of related items for ev- ery input. Examples of these tasks include language model smoothing, document score smoothing, and pseudo-relevance feedback. We describe how clustering can be applied to smoothing shortly.
9.2.3 Evaluation
Evaluating the output of a clustering algorithm can be challenging. Since cluster- ing is an unsupervised learning algorithm, there is o en little or no labeled data to use for the purpose of evaluation. When there is no labeled training data, it is sometimes possible to use an objective function, such as the objective function being minimized by the clustering algorithm, in order to evaluate the quality of the clusters produced.  is is a chicken and egg problem, however, since the eval- uation metric is de ned by the algorithm and vice versa.
If some labeled data exists, then it is possible to use slightly modi ed versions of standard information retrieval metrics, such as precision and recall, to evaluate the quality of the clustering. Clustering algorithms assign each input instance to a cluster identi er.  e cluster identi ers are arbitrary and have no explicit mean- ing. For example, if we were to cluster a set of emails into two clusters, some of the emails would be assigned to cluster identi er 1, while the rest would be assigned to cluster 2. Not only do the cluster identi ers have no meaning, but the clusters may not have a meaningful interpretation. For example, one would hope that one of the clusters would correspond to “spam” emails and the other to “non-spam” emails, but this will not necessarily be the case.  erefore, care must be taken when de ning measures of precision and recall.
One common procedure of measuring precision is as follows. First, the algo- rithm clusters the input instances into K = |C| clusters.  en, for each cluster Ci, we de ne MaxClass(Ci) to be the (human-assigned) class label associated with the most instances in Ci. Since MaxClass(Ci) is associated with more of the in- stances in Ci than any other class label, it is assumed that it is the true label for cluster Ci.  erefore, the precision for cluster Ci is the fraction of instances in
9.2 Clustering 387
the cluster with label MaxClass(Ci).  is measure is o en microaveraged across instances, which results in the following measure of precision:
∑Ki=1 |MaxClass(Ci)| ClusterP recision = N
where |MaxClass(Ci)| is the total number of instances in cluster Ci with the label MaxClass(Ci).  is measure has the intuitive property that if each cluster corre- sponds to exactly one class label and every member of a cluster has the same label, then the measure is 1.
In many search applications, clustering is only one of the technologies that are being used. Typically, the output of a clustering algorithm is used as part of some complex end-to-end system. In these cases, it is important to analyze and evaluate how the clustering algorithm affects the entire end-to-end system. For example, if clustering is used as a subcomponent of a web search engine in order to improve ranking, then the clustering algorithm can be evaluated and tuned by measuring the impact on the effectiveness of the ranking.  is can be difficult, as end-to-end systems are o en complex and challenging to understand, and many factors will impact the ranking.
9.2.4 How to Choose K
 us far, we have largely ignored how to choose K. In hierarchical and K-means clustering, K represents the number of clusters. In K nearest neighbors smooth- ing, K represents the number of nearest neighbors used. Although these two things are fundamentally different, it turns out that both are equally challenging to set properly in a fully automated way.  e problem of choosing K is one of the most challenging issues involved with clustering, since there is really no good so- lution. No magical formula exists that will predict the optimal number of clusters to use in every possible situation. Instead, the best choice of K largely depends on the task and data set being considered.  erefore, K is most o en chosen ex- perimentally.
In some cases, the application will dictate the number of clusters to use.  is, however, is rare. Most of the time, the application offers no clues as to the best choice of K. In fact, even the range of values for K to try might not be obvious. Should 2 clusters be used? 10? 100? 1,000?  ere is no better way of getting an understanding of the best setting for K than running experiments that evaluate the quality of the resulting clusters for various values of K.
 
                                                                                   388 9 Classi cationandClustering
With hierarchical clustering, it is possible to create the entire hierarchy of clus- ters and then use some decision mechanism to decide what level of the hiearchy to use for the clustering. In most situations, however, the number of clusters has to be manually chosen, even with hierarchical clustering.
Fig. 9.17. Example of overlapping clustering using Parzen windows.  e clusters for the black points (A, B, C, and D) are shown.  e shaded circles indicate the windows used to determine cluster membership.  e neighbors for each black point are shaded gray and labeled accordingly.
When forming K nearest neighbor clusters, it is possible to use an adaptive value for K.  at is, for instances in very dense regions, it may be useful to choose a large K, since the neighbors are likely to be related. Similarly, in very sparse ar- eas, it may be best to choose only a very small number of nearest neighbors, since moving too far away may result in unrelated neighbors being included.  is idea is closely related to Parzen windows,14 which are a variant of K nearest neighbors used for classi cation. With Parzen windows, the number of nearest neighbors is not  xed. Instead, all of the neighbors within a  xed distance (“window”) of an instance are considered its nearest neighbors. In this way, instances in dense
14 Named a er Emanuel Parzen, an American statistician.
       A
A
BB CC
D
                                                                                                                                                           B
                  B
B
B
C
C
                                                 C
        
9.2 Clustering 389
areas will have many nearest neighbors, and those in sparse areas will have few. Figure 9.17 shows the same set of points from Figure 9.16, but clustered using a Parzen window approach. We see that fewer outliers get assigned to incorrect clusters for points in sparse areas of the space (e.g., point C), whereas points in denser regions have more neighbors assigned to them (e.g., B). However, the clus- ters formed are not perfect.  e quality of the clusters now depends on the size of the window.  erefore, although this technique eliminates the need to choose K, it introduces the need to choose the window size, which can be an equally challenging problem.
9.2.5 Clustering and Search
A number of issues with clustering algorithms have resulted in them being less widely used in practice than classi cation algorithms.  ese issues include the computational costs, as well as the difficulty of interpreting and evaluating the clusters. Clustering has been used in a number of search engines for organizing the results, as we discussed in section 6.3.3.  ere are very few results for a search compared to the size of the document collection, so the efficiency of clustering is less of a problem. Clustering is also able to discover structure in the result set for arbitrary queries that would not be possible with a classi cation algorithm.
Topic modeling, which we discussed in section 7.6.2, can also be viewed as an application of clustering with the goal of improving the ranking effectiveness of the search engine. In fact, most of the information retrieval research involving clustering has focused on this goal.  e basis for this research is the well-known cluster hypothesis. As originally stated by van Rijsbergen (1979), the cluster hy- pothesis is:
Closely associated documents tend to be relevant to the same requests.
Note that this hypothesis doesn’t actually mention clusters. However, “closely as- sociated” or similar documents will generally be in the same cluster. So the hy- pothesis is usually interpreted as saying that documents in the same cluster tend to be relevant to the same queries.
Two different tests have been used to verify whether the cluster hypothesis holds for a given collection of documents.  e  rst compares the distribution of similarity scores for pairs of relevant documents (for a set of queries) to the dis- tribution for pairs consisting of a non-relevant and a relevant document. If the cluster hypothesis holds, we might expect to see a separation between these two distributions. On some smaller corpora, such as the CACM corpus mentioned
390 9 Classi cationandClustering
in Chapter 8, this is indeed the case. If there were a number of clusters of relevant documents, however, which were not similar to each other, then this test may fail to show any separation. To address this potential problem, Voorhees (1985) proposed a test based on the assumption that if the cluster hypothesis holds, rel- evant documents would have high local precision, even if they were scattered in many clusters. Local precision simply measures the number of relevant documents found in the top  ve nearest neighbors for each relevant document.
trec12
robust
      0.0 0.2
0.4 0.6
trec12
0.8
1.0
0.0 0.2
0.4 0.6 0.8 1.0
robust
         4000
3000
2000
1000
1500
1000
500
        00
0 0.2 0.4 0.6 0.8 1
Local Precision
0 0.2 0.4 0.6 0.8 1
Local Precision
 Fig. 9.18. Cluster hypothesis tests on two TREC collections.  e top two compare the distributions of similarity values between relevant-relevant and relevant-nonrelevant pairs (light gray) of documents.  e bottom two show the local precision of the relevant documents.
Frequency
Frequency
9.2 Clustering 391
Figure 9.18 shows the results of these tests used on two TREC collections.  ese collections have similar types of documents, including large numbers of news stories.  e 250 queries for the robust collection are known to be harder in terms of the typical MAP values obtained than the 150 queries used for trec12.  e tests on the top row of the  gure show that for both collections there is poor separation between the distributions of similarity values.  e tests on the lower row, however, show that relevant documents in the trec12 collection have high local precision.  e local precision is lower in the robust collection, which means that relevant documents tend to be more isolated and, consequently, harder to retrieve.
Given that the cluster hypothesis holds, at least for some collections and queries, the next question is how to exploit this in a retrieval model.  ere are, in fact, a number of ways of doing this.  e  rst approach, known as cluster-based retrieval, ranks clusters instead of individual documents in response to a query. If there were K clusters C1 . . . CK , for example, we could rank clusters using the query likelihood retrieval model.  is means that we rank clusters by P (Q|Cj ), where Q is the query, and:
∏n i=1
 e probabilities P(qi|Cj) are estimated using a smoothed unigram language model based on the frequencies of words in the cluster, as described for docu- ments in Chapter 7. A er the clusters have been ranked, documents within each cluster could be individually ranked for presentation in a result list.  e intuition behind this ranking method is that a cluster language model should provide bet- ter estimates of the important word probabilities than document-based estimates. In fact, a relevant document with no terms in common with the query could po- tentially be retrieved if it were a member of a highly ranked cluster with other relevant documents.
Rather than using this two-stage process, the cluster language model can be directly incorporated into the estimation of the document language model as fol-
lows:
P(w|D) = (1 − λ − δ)fw,D + δfw,Cj + λfw,Coll |D| |Cj| |Coll|
P(Q|Cj) =
P(qi|Cj)
   where λ and δ are parameters, fw,D is the word frequency in the document D, fw,Cj is the word frequency in the cluster Cj that contains D, and fw,Coll is the
392 9 Classi cationandClustering
word frequency in the collection Coll.  e second term, which comes from the cluster language model, increases the probability estimates for words that occur frequently in the cluster and are likely to be related to the topic of the document. In other words, the cluster language model makes the document more similar to other members of the cluster.  is document language model with cluster-based smoothing can be used directly by the query likelihood retrieval model to rank documents as described in section 7.3.1.
 e document language model can be further generalized to the case where the document D is a member of multiple overlapping clusters, as follows:
Cj
In this case, the second term in the document language model probability estimate for a word w is the weighted sum of the probabilities from the cluster language models for all clusters.  e weight (P (D|Cj )) is the probability of the document being a member of cluster Cj . We can also make the simplifying assumption that P (D|Cj ) is uniform for those clusters that contain D, and zero otherwise.
Retrieval experiments have shown that retrieving clusters can yield small but variable improvements in effectiveness. Smoothing the document language model with cluster-based estimates, on the other hand, provides signi cant and consis- tent bene ts. In practice, however, the expense of generating clusters has meant that cluster-based techniques have not been deployed as part of the ranking al- gorithm in operational search engines. However, promising results have recently been obtained using query-speci c clustering, where clusters are constructed only from the top-ranked (e.g., 50) documents (Liu & Cro , 2008; Kurland, 2008).  ese clusters, which can be used for either cluster-based retrieval or document smoothing, can obviously be generated much more efficiently.
References and Further Reading
Classi cation and clustering have been thoroughly investigated in the research ar- eas of statistics, pattern recognition, and machine learning.  e books by Duda et al. (2000) and Hastie et al. (2001) describe a wide range of classi cation and clus- tering techniques and provide more details about the mathematical foundations the techniques are built upon.  ese books also provide good overviews of other
P(w|D)=(1−λ−δ)|D| +δ
|Cj|P(D|Cj)+λ|Coll|
fw,D ∑ fw,Cj fw,Coll
   
9.2 Clustering 393
useful machine learning techniques that can be applied to various search engine tasks.
For a more detailed treatment of Naïve Bayes classi cation for text classi - cation, see McCallum and Nigam (1998). C. J. C. Burges (1998) gives a very de- tailed tutorial on SVMs that covers all of the basic concepts and theory. However, the subject matter is not light and requires a certain level of mathematical sophis- tication to fully understand. In addition, Joachims (2002a) is an entire book de- scribing various uses of SVMs for text classi cation.
Van Rijsbergen (1979) provides a review of earlier research on clustering in information retrieval, and describes the cluster hypothesis and cluster-based re- trieval. Diaz (2005) proposed an alternative interpretation of the cluster hypothe- sis by assuming that closely related documents should have similar scores, given the same query. Using this assumption, Diaz developed a framework for smoothing retrieval scores using properties of K nearest neighbor clusters. Language model- ing smoothing using K-means clustering was examined in Liu and Cro  (2004). Another language modeling smoothing technique based on overlapping K near- est neighbor clusters was proposed in Kurland and Lee (2004).
 ere are various useful so ware packages available for text classi cation.  e Mallet so ware toolkit15 provides implementations of various machine learning algorithms, including Naïve Bayes, maximum entropy, boosting, Winnow, and conditional random  elds. It also provides support for parsing and tokenizing text into features. Another popular so ware package is SVMLight,16 which is an SVM implementation that supports all of the kernels described in this chapter. Clustering methods are included in a number of packages available on the Web.
Exercises
9.1. Provide an example of how people use clustering in their everyday lives. What are the features that they use to represent the objects? What is the similarity mea- sure? How do they evaluate the outcome?
9.2. Assume we want to do classi cation using a very  ne-grained ontology, such as one describing all the families of human languages. Suppose that, before train- ing, we decide to collapse all of the labels corresponding to Asian languages into a single “Asian languages” label. Discuss the negative consequences of this decision.
15 http://mallet.cs.umass.edu/ 16 http://svmlight.joachims.org/
 
394 9 Classi cationandClustering
9.3. Suppose that we were to estimate P (d|c) according to:
P (d|c) = Nd,c Nc
where Nd,c is the number of times document d is assigned to class c in the training set, and Nc is the number of instances assigned class label c in the training set.  is is analogous to the way that P (c) is estimated. Why can this estimate not be used in practice?
9.4. For some classi cation data set, compute estimates for P (w|c) for all words
w using both the multiple-Bernoulli and multinomial models. Compare the multiple- Bernoulli estimates with the multinomial estimates. How do they differ? Do the estimates diverge more for certain types of terms?
9.5. Explain why the solution to the original SVM formulation w = arg maxw 2 ||w||
isequivalenttothealternativeformulationw=argminw 1||w||2. 2
9.6. Compare the accuracy of a one versus all SVM classi er and a one versus one SVM classi er on a multiclass classi cation data set. Discuss any differences observed in terms of the efficiency and effectiveness of the two approaches.
9.7. Under what conditions will the microaverage equal the macroaverage?
9.8. Cluster the following set of two-dimensional instances into three clusters us-
ing each of the  ve agglomerative clustering methods:
(–4, –2), (–3, –2), (–2, –2), (–1, –2), (1, –1), (1, 1), (2, 3), (3, 2), (3, 4), (4, 3)
Discuss the differences in the clusters across methods. Which methods pro- duce the same clusters? How do these clusters compare to how you would manu- ally cluster the points?
9.9. Use K-means and spherical K-means to cluster the data points in Exercise 9.8. How do the clusterings differ?
9.10. Nearest neighbor clusters are not symmetric, in the sense that if instance A is one of instance B’s nearest neighbors, the reverse is not necessarily true. Explain how this can happen with a diagram.
   
9.2 Clustering 395
9.11.  e K nearest neighbors of a document could be represented by links to those documents. Describe two ways this representation could be used in a search application.
9.12. Can the ClusterPrecision evaluation metric ever be equal to zero? If so, pro- vide an example. If not, explain why.
9.13. Test the cluster hypothesis on the CACM collection using both methods shown in Figure 9.18. What do you conclude from these tests?
10
Social Search
10.1 What Is Social Search?
“You will be assimilated.”
Borg Collective, Star Trek: First Contact
 In this chapter we will describe social search, which is rapidly emerging as a key search paradigm on the Web. As its name implies, social search deals with search within a social environment.  is can be de ned as an environment where a com- munity of users actively participate in the search process. We interpret this de - nition of social search very broadly to include any application involving activities such as de ning individual user pro les and interests, interacting with other users, and modifying the representations of the objects being searched.  e active role of users in social search applications is in stark contrast to the standard search paradigms and models, which typically treat every user the same way and restrict interactions to query formulation.
Users may interact with each other online in a variety of ways. For example, users may visit a social media site,1 which have recently gained a great deal of popularity. Examples of these sites include Digg (websites), Twitter (status mes- sages), Flickr (pictures), YouTube (videos), Del.icio.us (bookmarks), and CiteU- Like (research papers). Social networking sites, such as MySpace, Facebook, and LinkedIn, allow friends, colleagues, and people with similar interests to interact with each other in various ways. More traditional examples of online social in- teractions include email, instant messenger, massively multiplayer online games (MMOGs), forums, and blogs.
1 Social media sites are o en collectively referred to as Web 2.0, as opposed to the clas- sical notion of the Web (“Web 1.0”), which consists of non-interactive HTML docu- ments.
 
398 10 SocialSearch
As we see, the online world is a very social environment that is rich with users interacting with each other in various forms.  ese social interactions provide new and unique data resources for search systems to exploit, as well as a myriad of privacy issues. Most of the web search approaches we described in Chapter 7 only consider features of the documents or the link structure of the Web. In so- cially rich environments, however, we also have a plethora of user interaction data available that can help enhance the overall user experience in new and interesting ways.
It would be possible to write an entire book on online social interactions and search within such environments.  e focus of this chapter is to highlight and describe a few aspects of social search that are particularly interesting from a search engine and information retrieval perspective.
 e  rst topic we cover is user tags. Many social media websites allow users to assign tags to items. For example, a video-sharing website may allow users to not only assign tags to their own videos, but also to videos created by other people. An underwater video, for example, may have the tags “swimming”, “underwater”, “tropic”, and “ sh”. Some sites allow multi-term tags, such as “tropical  sh”, but others allow only single-term tags. As we will describe, user tags are a form of man- ual indexing, where the content of an object is represented by manually assigned terms.  ere are many interesting search tasks related to user tags, such as search- ing for items using tags, automatically suggesting tags, and visualizing clusters of tags.
 e second topic covered here is searching within communities, which de- scribes online communities and how users search within such environments. On- line communities are virtual groups of users that share common interests and in- teract socially in various ways in an online environment. For example, a sports fan who enjoys the outdoors and photography may be a member of baseball, hiking, and digital camera communities. Interactions in these communities range from passive activities (reading web pages) to those that are more active (writing in blogs and forums).  ese communities are virtual and ad hoc, meaning that there is typically no formal mechanism for joining one, and consequently people are im- plicitly rather than explicitly members of a community.  erefore, being able to automatically determine which communities exist in an online environment, and which users are members of each, can be valuable for a number of search-related tasks. One such task that we will describe is community-based question answer- ing, whereby a user posts a question to an online system and members of his own community, or the community most related to his question, provide answers to
10.1 WhatIsSocialSearch? 399
the question. Such a search task is much more social, interactive, and focused than standard web search.
 e next topics we describe are  ltering and recommender systems. It may be considered somewhat unusual to include these in a chapter on social search, be- cause they are not typical “Web 2.0” applications. Both types of systems, however, rely on representations of individual users called pro les, and for that reason  t into our broad de nition. Both systems also combine elements of document re- trieval and classi cation. In standard search tasks, systems return documents in response to many different queries.  ese queries typically correspond to short- term information needs. In  ltering, there is a  xed query (the pro le) that rep- resents some long-term information need.  e search system monitors incoming documents and retrieves only those documents that are relevant to the informa- tion need. Many online news websites provide document  ltering functionality. For example, CNN provides an alerts service, which allows users to specify various topics of interest, such as “tropical storms”, or more general topics, such as sports or politics. When a new story matches a user’s pro le, the system alerts the user, typically via email. In this way, the user does not need to continually search for articles of interest. Instead, the search system is tasked with  nding relevant docu- ments that match the user’s long-term information needs. Recommender systems are similar to document  ltering systems, except the goal is to predict how a user would rate some item, rather than retrieving relevant documents. For example, Amazon.com employs a recommender system that attempts to predict how much a user would like certain items, such as books, movies, or music. Recommender systems are social search algorithms because predictions are estimated based on ratings given by similar users, thereby implicitly linking people to a community of users with related interests.
 e  nal two topics covered in this chapter, peer-to-peer (P2P) search and metasearch, deal with architectures for social search. Peer-to-peer search is the task of querying a community of “nodes” for a given information need. Nodes can be individuals, organizations, or search engines. When a user issues a query, it is passed through the P2P network and run on one or more nodes, and then results are returned.  is type of search can be fully distributed across a large network of nodes. Metasearch is a special case of P2P search where all of the nodes are search engines. Metasearch engines run queries against a number of search engines, col- lect the results, and then merge the results.  e goal of metasearch engines is to provide better coverage and accuracy than a single search engine.
400 10 SocialSearch
Finally, we note that personalization is another area that could be regarded as part of social search, because it covers a range of techniques for improving search by representing individual user preferences and interests. Since most of these tech- niques provide context for the query, however, they were discussed as part of query re nement in section 6.2.5.
10.2 User Tags and Manual Indexing
Before electronic search systems became available at libraries, patrons had to rely on card catalogs for  nding books. As their name implies, card catalogs are large collections (catalogs) of cards. Each card contains information about a particular author, title, or subject. A person interested in a speci c author, title, or subject would go to the appropriate catalog and attempt to  nd cards describing relevant books.  e card catalogs, therefore, act as indexes to the information in a library.
Card catalogs existed long before computers did, which means that these cards were constructed manually. Given a book, a person had to extract the author, ti- tle, and subject headings of the book so that the various catalogs could be built.  is process is known as manual indexing. Given that it is impractical to manu- ally index the huge collections of digital media available today, search engines use automatic indexing techniques to assign identi ers (terms, phrases, features) to documents during index construction. Since this process is automatic, the quality and accuracy of the indexing can be much lower than that of manual indexing.  e advantages of automatic indexing, however, are that it is exhaustive, in the sense that every word in the document is indexed and nothing is le  out, and consistent, whereas people can make mistakes indexing or have certain biases in how they in- dex. Search evaluations that have compared manual to automatic indexing have found that automatic indexing is at least as effective and o en much better than manual indexing.  ese studies have also shown, however, that the two forms of indexing complement each other, and that the most effective searches use both.
As a compromise between manually indexing every item (library catalogs) and automatically indexing every item (search engines), social media sites pro- vide users with the opportunity to manually tag items. Each tag is typically a sin- gle word that describes the item. For example, an image of a tiger may be assigned the tags “tiger”, “zoo”, “big”, and “cat”. By allowing users to assign tags, some items end up with tags, and others do not. Of course, to make every item searchable, it is likely that every item is automatically indexed.  erefore, some items will contain
10.2 UserTagsandManualIndexing 401
both automatic and manual identi ers. As we will show later in this section, this results in unique challenges for retrieval models and ranking functions.
Social media tagging, like card catalog generation, is called manual tagging.  is clash in naming is rather unfortunate, because the two types of indexing are actually quite different. Card catalogs are manually generated by experts who choose keywords, categories, and other descriptors from a controlled vocabulary ( xed ontology).  is ensures that the descriptors are more or less standardized. On the other hand, social media tagging is done by end users who may or may not be experts.  ere is little-to-no quality control done on the user tags. Fur- thermore, there is no  xed vocabulary from which users choose tags. Instead, user tags form their own descriptions of the important concepts and relationships in a domain. User-generated ontologies (or taxonomies) are known as folksonomies.  erefore, a folksonomy can be interpreted as a dynamic, community-in uenced ontolog y.
 ere has been a great deal of research and interest invested in developing a semantically tagged version of the Web, which is o en called the semantic web.  e goal of the semantic web is to semantically tag web content in such a way that it becomes possible to  nd, organize, and share information more easily. For example, a news article could be tagged with metadata, such as the title, subject, description, publisher, date, and language. However, in order for the semantic web to materialize and yield signi cant improvements in relevance, a standard- ized,  xed ontology of metadata tags must be developed and used consistently across a large number of web pages. Given the growing popularity of social media sites that are based on  exible, user-driven folksonomies, compared to the small number of semantic web sites that are based on rigid, prede ned ontologies, it seems as though users, in general, are more open to tagging data with a relatively unrestricted set of tags that are meaningful to them and that re ect the speci c context of the application.
Given that users are typically allowed to tag items in any way that they wish, there are many different types of tags. For example, Golder and Huberman (2006) described seven different categories of tags. Z. Xu et al. (2006) proposed a sim- pli ed set of  ve tag categories, which consists of the following:
1. Content-based tags. Tags describing the content of an item. Examples: “car”, “woman”, and “sky”.
2. Context-based tags. Tags that describe the context of an item. Examples: “New York City” or “Empire State Building”.
402 10 SocialSearch
3. Attribute tags. Tags that describe implicit attributes of the item. Examples: “Nikon” (type of camera), “black and white” (type of movie), or “homepage” (type of web page).
4. Subjective tags. Tags that subjectively describe an item. Examples: “pretty”, “amazing”, and “awesome”.
5. Organizationaltags.Tagsthathelporganizeitems.Examples:“todo”,“mypic- tures”, and “readme”.
As we see, tags can be applied to many different types of items, ranging from web pages to videos, and used for many different purposes beyond just tagging the con- tent.  erefore, tags and online collaborative tagging environments can be very useful tools for users in terms of searching, organizing, sharing, and discovering new information. It is likely that tags are here to stay and will become even more widely used in the future.  erefore, it is important to understand the various issues surrounding them and how they are used within search engines today.
In the remainder of this section, we will describe how tags can be used for search, how new tags for an item can be inferred from existing tags, and how sets of tags can be visualized and presented to the user.
10.2.1 Searching Tags
Since this is a book about search engines, the  rst tag-related task that we discuss is searching a collection of collaboratively tagged items. One unique property of tags is that they are almost exclusively textual keywords that are used to describe textual or non-textual items.  erefore, tags can provide a textual dimension to items that do not explicitly have a simple textual representation, such as images or videos.  ese textual representations of non-textual items can be very useful for searching. We can apply many of the retrieval strategies described in Chap- ter 7 to the problem. Despite the fact that searching within tagged collections can be mapped to a text search problem, tags present certain challenges that are not present when dealing with standard document or web retrieval.
 e  rst challenge, and by far the most pervasive, is the fact that tags are very sparse representations of very complex items. Perhaps the simplest way to search a set of tagged items is to use a Boolean retrieval model. For example, given the query “ sh bowl”, one could run the query “ sh AND bowl”, which would only return items that are tagged with both “ sh” and “bowl”, or “ sh OR bowl”, which would return items that are tagged with either “ sh” or “bowl”. Conjunc- tive (AND) queries are likely to produce high-quality results, but may miss many
10.2 UserTagsandManualIndexing 403
relevant items.  us, the approach would have high precision but low recall. At the opposite end of the spectrum, the disjunctive (OR) queries will match many more relevant items, but at the cost of precision.
 Age of Aquariums - Tropical Fish
Huge educational aquarium site for tropical fish hobbyists, promoting responsible fish keeping internationally since 1997.
The Krib (Aquaria and Tropical Fish)
This site contains information about tropical fish aquariums, including archived usenet postings and e-mail discussions, along with new ...
...
Keeping Tropical Fish and Goldfish in Aquariums, Fish Bowls, and ... Keeping Tropical Fish and Goldfish in Aquariums, Fish Bowls, and Ponds at AquariumFish.net.
                                        P(w | “tropical fish” )
Fig. 10.1. Search results used to enrich a tag representation. In this example, the tag being expanded is “tropical  sh”.  e query “tropical  sh” is run against a search engine, and the snippets returned are then used to generate a distribution over related terms.
Of course, it is highly desirable to achieve both high precision and high re- call. However, doing so is very challenging. Consider the query “aquariums” and a picture of a  sh bowl that is tagged with “tropical  sh” and “gold sh”. Most re- trieval models, including Boolean retrieval, will not be able to  nd this item, be- cause there is no overlap between the query terms and the tag terms.  is problem, which was described in Chapter 6 in the context of advertising, is known as the vocabulary mismatch problem.  ere are various ways to overcome this problem, including simple things such as stemming. Other approaches attempt to enrich the sparse tag (or query) representation by performing a form of pseudo-relevance
fish tropical
aquariums goldfish
bowls
404 10 SocialSearch
feedback. Figure 10.1 illustrates how web search results may be used to enrich a tag representation. In the example, the tag “tropical  sh” is run as a query against a search engine.  e snippets returned are then processed using any of the stan- dard pseudo-relevance feedback techniques described in section 7.3.2, such as rel- evance modeling, which forms a distribution over related terms. In this example, the terms “ sh”, “tropical”, “aquariums”, “gold sh”, and “bowls” are the terms with the highest probability according to the model.  e query can also, optionally, be expanded in the same way. Search can then be done using the enriched query and/or tag representations in order to maintain high levels of precision as well as recall.
 e second challenge is that tags are inherently noisy. As we have shown, tags can provide useful information about items and help improve the quality of search. However, like anything that users create, the tags can also be off topic, inappropriate, misspelled, or spam.  erefore it is important to provide proper in- centives to users to enter many high-quality tags. For example, it may be possible to allow users to report inappropriate or spam tags, thereby reducing the incentive to produce junk tags. Furthermore, users may be given upgraded or privileged ac- cess if they enter some number of (non-spam) tags over a given time period.  is incentive promotes more tagging, which can help improve tag quality and cover- age.
 e  nal challenge is that many items in a given collection may not be tagged, which makes them virtually invisible to any text-based search system. For such items, it would be valuable to automatically infer the missing tags and use the tags for improving search recall. We devote the next section to diving deeper into the details of this problem.
10.2.2 Inferring Missing Tags
As we just described, items that have no tags pose a challenge to a search sys- tem. Although precision is obviously a very important metric for many tag-related search tasks, recall may also be important in some cases. In such cases, it is impor- tant to automatically infer a set of tags for items that have no manual tags assigned to them.
Let us  rst consider the case when the items in our collection are textual, such as books, news articles, research papers, or web pages. In these cases, it is possible to infer tags based solely on the textual representation of the item. One simple approach would involve computing some weight for every term that occurs in the
10.2 UserTagsandManualIndexing 405
text and then choosing the K terms with the highest weight as the inferred tags.  ere are various measures of term importance, including a tf.idf-based weight,
such as:
wt(w) = log(fw,D + 1) log( N ) dfw
 where fw,d is the number of times term w occurs in item D, N is the total num- ber of items, and dfw is the number of items that term w occurs in. Other term importance measures may take advantage of document structure. For example, terms that occur in the title of an item may be given more weight.
It is also possible to treat the problem of inferring tags as a classi cation prob- lem, as was recently proposed by Heymann, Ramage, and Garcia-Molina (2008). Given a  xed ontology or folksonomy of tags, the goal is to train a binary classi er for each tag. Each of these classi ers takes an item as input and predicts whether the associated tag should be applied to the item.  is approach requires training one classi er for every tag, which can be a cumbersome task and requires a large amount of training data. Fortunately, however, training data for this task is virtu- ally free since users are continuously tagging (manually labeling ) items!  erefore, it is possible to use all of the existing tag/item pairs as training data to train the classi ers. Heymann et al. use an SVM classi er to predict web page tags.  ey compute a number of features, including tf.idf weights for terms in the page text and anchor text, as well as a number of link-based features. Results show that high precision and recall can be achieved by using the textual features alone, especially for tags that occur many times in the collection. A similar classi cation approach can be applied to other types of items, such as images or videos.  e challenge when dealing with non-textual items is extracting useful features from the items.
 e two approaches we described for inferring missing tags choose tags for items independently of the other tags that were assigned.  is may result in very relevant, yet very redundant, tags being assigned to some item. For example, a picture of children may have the tags “child”, “children”, “kid”, “kids”, “boy”, “boys”, “girl”, “girls”—all of which would be relevant, but as you see, are rather redundant.  erefore, it is important to choose a set of tags that are both relevant and non- redundant.  is is known as the novelty problem.
Carbonell and Goldstein (1998) describe the Maximal Marginal Relevance (MMR) technique, which addresses the problem of selecting a diverse set of items. Rather than choosing tags independently of each other, MMR chooses tags iter- atively, adding one tag to the item at a time. Given an item i and the current set of tags for the item Ti, the MMR technique chooses the next tag according to the
406 10 SocialSearch tag t that maximizes:
()
MMR(t;Ti)= λSimitem(t,i)−(1−λ)maxSimtag(ti,t) t∈Ti
where Simitem is a function that measures that similarity between a tag t and item i (such as those measures described in this section), Simtag measures the similarity between two tags (such as the measures described in section 10.2.1), and λ is a tunable parameter that can be used to trade off between relevance (λ = 1) and novelty (λ = 0).  erefore, a tag that is very relevant to the item and not very similar to any of the other tags will have a large MMR score. Iteratively choosing tags in this way helps eliminate the production of largely redundant sets of tags, which is useful not only when presenting the inferred tags to the user, but also from the perspective of using the inferred tags for search, since a diverse set of tags should help further improve recall.
10.2.3 Browsing and Tag Clouds
As we have shown, tags can be used for searching a set of collaboratively tagged items. However, tags can also be used to help users browse, explore, and discover new items in a large collection of items.  ere are several different ways that tags can be used to aid browsing. For example, when a user is viewing a given item, all of the item’s tags may be displayed.  e user may then click on one of the tags and be shown a list of results of items that also have that tag.  e user may then repeat this process, repeatedly choosing an item and then clicking on one of the tags.  is allows users to browse through the collection of items by following a chain of related tags.
Such browsing behavior is very focused and does not really allow the user to explore a large range of the items in the collection. For example, if a user starts on a picture of a tropical  sh, it would likely take many clicks for the user to end up viewing an image of an information retrieval textbook. Of course, this may be desirable, especially if the user is only interested in things closely related to tropical  sh.
One way of providing the user with a more global view of the collection is to allow the user to view the most popular tags.  ese may be the most popular tags for the entire site or for a particular group or category of items. Tag popularity may be measured in various ways, but is commonly computed as the number of times the tag has been applied to some item. Displaying the popular tags allows
10.2 UserTagsandManualIndexing 407
the user to begin her browsing and exploration of the collection using one of these tags as a starting point.
 animals architecture art australia autumn baby band barcelona beach berlin birthday black blackandwhite blue california cameraphone canada canon car cat chicago china christmas church city clouds color concert day dog england europe family festival film florida flower flowers food france friends fun garden germany girl graffiti green halloween hawaii holiday home house india ireland italy japan july kids lake landscape light live londonmacro me mexico music nature new newyork night
nikon nyc ocean paris park party people portrait red river rock sanfrancisco scotland sea seattle show sky snow spain spring street summer sunset taiwan texas thailand tokyo toronto travel
tree trees trip uk usa vacation washington water wedding
Fig. 10.2. Example of a tag cloud in the form of a weighted list.  e tags are in alphabetical order and weighted according to some criteria, such as popularity.
 us far we have described ways that tags may aid users with browsing. One of the most important aspects of browsing is displaying a set of tags to a user in a vi- sually appealing and meaningful manner. For example, consider displaying the 50 most popular tags to the user.  e simplest way to do so is to just display the tags in a list or table, possibly in alphabetical or sorted order according to popularity. Besides not being very visually appealing, this display also does not allow the user to quickly observe all of the pertinent information. When visualizing tags, it is useful to show the list of tags in alphabetical order, so that users may quickly scan through the list or  nd the tags they are looking for. It is also bene cial to por- tray the popularity or importance of a given tag.  ere are many ways to visualize this information, but one of the most widely used techniques is called tag clouds. In a tag cloud, the display size of a given tag is proportional to its popularity or importance. Tags may be arranged in a random order within a “cloud” or alpha- betically. Figure 10.2 shows an example of a tag cloud where the tags are listed alphabetically. Such a representation is also called a weighted list. Based on this
408 10 SocialSearch
tag cloud, the user can easily see that the tags “wedding”, “party”, and “birthday” are all very popular.  erefore, tag clouds provide a convenient, visually appealing way of representing a set of tags.
10.3 Searching with Communities 10.3.1 What Is a Community?
 e collaborative tagging environments that we just described are  lled with im- plicit social interactions. By analyzing the tags that users submit or search for, it is possible to discover groups of users with related interests. For example, ice hockey fans are likely to tag pictures of their favorite ice hockey players, tag their favorite ice hockey web pages, search for ice hockey–related tags, and so on. Tagging is just one example of how interactions in an online environment can be used to infer relationships between entities (e.g., people). Groups of entities that interact in an online environment and that share common goals, traits, or interests are an online community.  is de nition is not all that different from the traditional de nition of community. In fact, online communities are actually very similar to traditional communities and share many of the same social dynamics.  e primary difference between our de nition and that of a traditional community is that an online com- munity can be made up of users, organizations, web pages, or just about any other meaningful online entity.
Let us return to our example of users who tag and search for ice hockey–related items. It is easy to see that ice hockey fans form an online community. Members of the community do many other things other than tag and search. For example, they also post to blogs, newsgroups, and other forums.  ey may also send instant messages and emails to other members of the community about their ice hockey experiences. Furthermore, they may buy and sell ice hockey–related items online, through sites such as eBay. Hence, there are many ways that a user may partici- pate in a community. It is important to note, however, that his membership in the community is implicit. Another important thing to notice is that users are very likely to have a number of hobbies or interests, and may be members of more than one online community.  erefore, in order to improve the overall user expe- rience, it can be useful for search engines and other online sites to automatically determine the communities associated with a given user.
Some online communities consist of non-human entities. For example, a set of web pages that are all on the same topic form an online community that is o en
10.3 SearchingwithCommunities 409
called a web community.  ese pages form a community since they share similar traits (i.e., they are all about the same topic). Since web pages are created by users, web communities share many of the same characteristics as communities of users. Automatically identifying web communities can be useful for improving search.
 e remainder of this section covers several aspects of online communities that are useful from a search engine perspective. We  rst describe several ef- fective methods for automatically  nding online communities. We then discuss community-based question answering, where people ask questions and receive answers from other members of the community. Finally, we cover collaborative searching, which is a search paradigm that involves a group of users searching to- gether.
10.3.2 Finding Communities
 e  rst task that we describe is how to automatically  nd online communities. As we mentioned before, online communities are implicitly de ned by interac- tions among a set of entities with common traits.  is de nition is rather vague and makes it difficult to design general-purpose algorithms for  nding every pos- sible type of online community. Instead, several algorithms have been developed that can effectively  nd special types of communities that have certain assumed characteristics. We will describe several such algorithms now.
Most of the algorithms used for  nding communities take as input a set of en- tities, such as users or web pages, information about each entity, and details about how the entities interact or are related to each other.  is can be conveniently rep- resented as a graph, where each entity is a node in the graph, and interactions (or relationships) between the entities are denoted by edges. Graphs can be either di- rected or undirected.  e edges in directed graphs have directional arrows that indicate the source node and destination node of the edge. Edges in undirected graphs do not have directional arrows and therefore have no notion of source and destination. Directed edges are useful for representing non-symmetric or causal relationships between two entities. Undirected edges are useful for representing symmetric relationships or for simply indicating that two entities are related in some way.
Using this representation, it is easy to de ne two criteria for  nding communi- ties within the graph. First, the set of entities (nodes) must be similar to each other according to some similarity measure. Second, the set of entities should interact with each other more than they interact with other entities.  e  rst requirement
410 10 SocialSearch
makes sure that the entities actually share the same traits, whereas the second en- sures that the entities interact in a meaningful way with each other, thereby mak- ing them a community rather than a set of users with the same traits who never interact with each other.
 e  rst algorithm that we describe is the HITS algorithm, which was brie y discussed in Chapter 4 in the context of PageRank.  e HITS algorithm is sim- ilar to PageRank, except that it is query-dependent, whereas PageRank is usually query-independent. You may be wondering what HITS has to do with  nding communities, since it was originally proposed as a method for improving web search. Both PageRank and HITS, however, are part of a family of general, power- ful algorithms known as link analysis algorithms.  ese algorithms can be applied to many different types of data sets that can be represented as directed graphs.
Given a graph of entities, we must  rst identify a subset of the entities that may possibly be members of the community. We call these entities the candidate entities. For example, if we wish to  nd the ice hockey online community, then we must query each node in the graph and  nd all of the nodes (users) that are interested in ice hockey.  is can be accomplished by, for example,  nding all users of a system who have searched for anything hockey-related.  is ensures that the  rst criteria, which states that entities should be similar to each other, is satis ed. Another example is the task of  nding the “fractal art” web community. Here, we could search the Web for the query “fractal art” and consider only those entities (web pages) that match the query. Again, this ensures that all of the pages are topically similar to each other.  is  rst step  nds sets of similar items, but fails to identify the sets of entities that actively participate, via various interactions, within the community, which is the second criteria that we identi ed as being important.
Given the candidate entities, the HITS algorithm can be used to  nd the “core” of the community.  e HITS algorithm takes a graph G with node set V and edge set E as input. For  nding communities, the vertex set V consists of the candidate entities, and the edge set E consists of all of the edges between can- didate entities. For each of the candidate entities (nodes) p in the graph, HITS computes an authority score (A(p)) and a hub score (H(p)). It is assumed that good hubs are those that point to good authorities and that good authorities are those that are pointed to by good hubs. Notice the circularity in these de nitions.  is means that the authority score depends on the hub score, which in turn de- pends on the authority score. Given a set of authority and hub scores, the HITS algorithm updates the scores according to the following equations:
end for end for
for p ∈ V do Ai(p) ← Ai(p)
end for end for
return AK , HK endprocedure
10.3 SearchingwithCommunities 411
 Algorithm 3 HITS
 1: 2: 3: 4: 5: 6: 7: 8: 9:
10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22:
23: 24: 25: 26: 27:
procedureHITS(G=(V,E),K) A0(p)←1∀p∈V H0(p)←1∀p∈V
fori = 1toK do
Ai(p) ← 0 ∀p ∈ V Hi(p) ← 0 ∀p ∈ V ZA ← 0
ZH ← 0
for p ∈ V do
for q ∈ V do
if (p, q) ∈ E then
Hi(p) ← Hi(p) + Ai−1(q) ZH ←ZH +Ai−1(q)
end if
if (q, p) ∈ E then
Ai(p) ← Ai(p) + Hi−1(q)
ZA ← ZA + Hi−1(q) end if
 ZA Hi(p) ← Hi(p)
 ZH
 A(p) = H(p) =
∑
q→p ∑
p→q
H(q) A(q)
where p → q indicates that an edge exists between entity p (source) and entity q (destination). As the equations indicate, A(p) is the sum of the hub scores of the entities that point at p, and H(p) is the sum of the authority scores pointed at by p.  us, to be a strong authority, an entity must have many incoming edges, all
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               412 10 SocialSearch
with relatively moderate hub scores, or have very few incoming links that have very large hub scores. Similarly, to be a good hub, an entity must have many outgoing edges to less authoritative pages, or few outgoing edges to very highly authorita- tive pages.
Fig. 10.3. Illustration of the HITS algorithm. Each row corresponds to a single iteration of the algorithm and each column corresponds to a speci c step of the algorithm.
An iterative version of HITS is given in Algorithm 3.  e algorithm begins by initializing all hub and authority scores to 1.  e algorithm then updates the hub and authority scores according to the equations we just showed.  en, the hub scores are normalized so that the sum of the scores is 1.  e same is also done for the authority scores.  e entire process is then repeated on the normalized scores for a  xed number of iterations, denoted by K.  e algorithm is guaranteed to converge and typically does so a er a small number of iterations. Figure 10.3 shows an example of the HITS algorithm applied to a graph with seven nodes and six edges.  e algorithm is carried out for three iterations. Notice that the nodes with many incoming edges tend to have higher authority scores, and those with more outgoing edges tend to have larger hub scores. Another characteristic
 Iteration 1: Input
1, 1                   1, 1
1, 1
1, 1 1, 1
                1, 1
                                                       1, 1
           Iteration 1: Update Scores 2, 0             0, 1
0, 1
0, 0 3, 0
          0, 3
                                                 1, 1
           Iteration 1: Normalize Scores
 .33, 0
0, .17 0, .17
     0, .50
       .17, .17
0,0 .50, 0
   .33, ,
.17, .17
Iteration 2: Input
0, .17
0
     0, .17
0,0 .50, 0
0, .50
         .67, 0
0, .50 0, .50
Iteration 2: Update Scores
             0, 1
                                   .50, .33
0,0 .83, 0
        Iteration 2: Normalize Scores
 .33, 0
0, .21 0, .21
     0, .43
       .25, .14
0,0 .42, 0
   .33, ,
.25, .14
Iteration 3: Input
0, .21
0
     0, .21
0,0 .42, 0
0, .43
         Iteration 3: Update Scores
 .57, 0
0, .42 0, .42
     0,1
       .43, .33
0,0 .86, 0
  Iteration 3: Normalize Scores
 .31, 0
0, .19 0, .19
     0, .46
       .23, .16
0,0 .46, 0
 
10.3 SearchingwithCommunities 413
of the HITS algorithm is that nodes that are not connected to any other nodes will always have hub and authority scores of 0.
Once the hub and authority scores have been computed, the entities can be ranked according to their authority score.  is list will contain the most authori- tative entities within the community. Such entities are likely to be the “leaders” or form the “core” of the community, based on their interactions with other mem- bers of the community. For example, if this algorithm were applied to the com- puter science researcher citation graph to  nd the information retrieval research community, the most authoritative authors would be those that are cited many times by proli c authors.  ese would arguably be the luminaries in the  eld and those authors that form the core of the research community. When  nding web communities on the web graph, the algorithm will return pages that are linked to by a large number of reputable web pages.
Node: 1 2 3 4 5 6 7
       1 1
5 5
2 2
                                              3 3
4 4
7 7
                                                           6 6
                                 Vector:
0010100 0000000 0000000 0 0 0 0 0 0 0 0010000 0111000 0000000
 Fig. 10.4. Example of how nodes within a directed graph can be represented as vectors. For a given node p, its vector representation has component q set to 1 if p → q.
Clustering algorithms, such as the ones described in Chapter 9, may also be used for  nding online communities.  ese algorithms easily adapt to the prob-
414 10 SocialSearch
lem, since community  nding is an inherently unsupervised learning problem. Both agglomerative clustering and K-means can be used for  nding communi- ties. Both of these clustering algorithms require a function that measures the dis- tance between two clusters. As we discussed in Chapter 9, the Euclidean distance is o en used. However, it is not immediately clear how to apply the Euclidean distance to nodes in a (directed or undirected) graph. One simple way is to rep- resent each node (entity) in the graph as a vector that has |V | components—one for every node in the graph. For some node p, component q of its vector repre- sentation is set to 1 if p → q, and 0 otherwise.  is results in each node being represented by the nodes it points to. Figure 10.4 shows how the nodes of a graph are represented this way. Each vector may optionally be normalized. Using this representation, it is possible to use the Euclidean distance and directly apply ag- glomerative clustering or K-means to the problem. A high similarity according to the Euclidean distance will occur when two entities have edges directed toward many of the same entities. For example, returning to the problem of  nding the information retrieval research community, two authors would be considered very similar if they tended to cite the same set of authors. It is realistic to assume that most members of the information retrieval research community tend to cite many of the same authors, especially those that would be given a large authority score by HITS.
Evaluating the effectiveness of community- nding algorithms is typically more difficult than evaluating traditional clustering tasks, since it is unclear how to de- termine whether some entity should be part of a given community. In fact, it is likely that if a group of people were asked to manually identify online communi- ties, there would be many disagreements due to the vague de nition of a commu- nity.  erefore, it is impossible to say whether HITS or agglomerative clustering is better at  nding communities.  e best choice largely depends on the task and data set for the application.
Now that you have seen several ways of automatically  nding online commu- nities, you may be wondering how this information can be put to practical use.  ere are many different things that can be done a er a set of communities has been identi ed. For example, if a user has been identi ed as part of the informa- tion retrieval research community, then when the user visits a web page, targeted content may be displayed to the user that caters to her speci c interests. Search engines could use the community information as additional contextual informa- tion for improving the relevance of search results, by retrieving results that are also topically related to the community or communities associated with the user. On-
10.3 SearchingwithCommunities 415
line community information can also be used in other ways, including enhanced browsing, identifying experts, website recommendation, and possibly even sug- gesting who may be a compatible date for you!
10.3.3 Community-Based Question Answering
In the last section, we described how to automatically  nd online communities. In this section, we describe how such communities can be used effectively to help an- swer complex information needs that would be difficult to answer using conven- tional search engines. For example, consider a person who is interested in learning about potential interactions between a medicine he was just prescribed and an herbal tea he o en drinks. He could use a search engine and spend hours entering various queries, looking at search results, and trying to  nd useful information on the subject.  e difficulty with using this approach is that no single page may exist that completely satis es the person’s information need. Now, suppose that the person could ask his question directly to a large group of other people, sev- eral of whom are pharmacists or herbal experts. He would be much more likely to get an answer.  is search scenario, where a person submits a question to a community consisting of both experts and non-experts in a wide range of topics, each of whom can opt to answer the question, is called community-based ques- tion answering (CQA).  ese systems harness the power of human knowledge in order to satisfy a broad range of information needs. Several popular commercial systems of this type exist today, including Yahoo! Answers and Naver, a Korean search portal.
 ere are both pros and cons to CQA systems.  e pros include users being able to get answers to complex or obscure information needs; the chance to see multiple, possibly differing opinions about a topic; and the chance to interact with other users who may share common interests, problems, and goals.  e cons include the possibility of receiving no answer at all to a question, having to wait (possibly days) for an answer, and receiving answers that are incorrect, misleading, offensive, or spam.
As we just mentioned, many of the answers that people submit are of low qual- ity. It turns out, however, that the old computer programming adage of “garbage in, garbage out” also applies to questions. Studies have shown that low-quality an- swers are o en given to low-quality questions. Indeed, there are a wide range of questions that users can, and do, ask. Table 10.1 shows a small sample of questions submitted to Yahoo! Answers. Some of the questions in the table are well-formed
416
10 SocialSearch
What part of Mexico gets the most tropical storms?
How do you pronounce the french words, coeur and miel?
GED test?
Why do I have to pay this  ne?
What is Schrödinger’s cat?
What’s this song?
Hi...can u ppl tell me sumthing abt death dreams??
What are the engagement and wedding traditions in Egypt?
Fun things to do in LA?
What lessons from the Tao Te Ching do you apply to your everyday life? Foci of a hyperbola?
What should I do today?
Why was iTunes deleted from my computer?
Heather Locklear?
Do people in the Australian Defense Force (RAAF) pay less tax than civilians? Whats a psp xmb?
If C(-3, y) and D(1, 7) lie upon a line whose slope is 2,  nd the value of y.? Why does love make us so irrational?
Am I in love?
What are some technologies that are revolutionizing business?
Table 10.1. Example questions submitted to Yahoo! Answers
  and grammatically correct, whereas others are not. In addition, some of the ques- tions have simple, straightforward answers, but many others do not.
Besides allowing users to ask and answer questions, CQA services also provide users with the ability to search the archive of previously asked questions and the corresponding answers.  is search functionality serves two purposes. First, if a user  nds that a similar question has been asked in the past, then they may not need to ask the question and wait for responses. Second, search engines may aug- ment traditional search results with hits from the question and answer database. For example, if a user enters the query “schrödingers cat”, the search engine may choose to return answers to “What is Schrödinger’s cat?” (which appears in Ta- ble 10.1) in addition to the other, more standard set of ranked web pages.
 erefore, given a query,2 it is important to be able to automatically  nd po- tential answers in the question and answer database.  ere are several possible
2 For the remainder of our discussion, we use the term “query” to refer to a question or a search query.
 
10.3 SearchingwithCommunities 417
ways to search this database. New queries can be matched against the archived questions alone, the archived answers alone, or questions and answers combined. Studies have shown that it is better to match queries against the archived ques- tions rather than answers, since generally it is easier to  nd related questions (which are likely to have relevant answers) than it is to match queries directly to answers.
Matching queries to questions can be achieved using any of the retrieval mod- els described in Chapter 7, such as language modeling or BM25. However, tradi- tional retrieval models are likely to miss many relevant questions because of the vocabulary mismatch problem. Here, vocabulary mismatch is caused by the fact that there are many different ways to ask the same question. For example, suppose we had the query “who is the leader of india?”. Related questions are “who is the prime minister of india?”, “who is the current head of the indian government?”, and so on. Notice that the only terms in common among any two of these ques- tions are “who”, “is”, “the”, “of ”, and “india”. Blindly applying any standard retrieval model would retrieve non-related questions such as “who is the  nance minister ofindia?”and“whoisthetallestpersoninallofindia?”.Stopwordremovalinthis case does not help much. Instead, better matches can be achieved by generalizing the notion of “leader” to include other concepts, such as “prime minister”, “head”, and “government”.
In section 6.4, we described cross-language retrieval, where a user queries in a source language (e.g., English) and documents are retrieved in a target language (e.g., French). Most of the retrieval methods developed for cross-language re- trieval are based on machine translation techniques, which require learning trans- lation probabilities of the form P (s|t), where s is a word in the source language and t is a word in the target language. Translation models can also be used to help overcome the vocabulary mismatch problem within a single language.  is is achieved by estimating P(t|t′), where t and t′ are both words in the same language.  is probability can be interpreted as the probability that word t is used in place of t′. Returning to our example, it is likely that P(leader|minister) and P (leader|government) would have non-zero values, and therefore result in more relevant questions being retrieved. We will now describe two translation- based models that have been used for  nding related questions and answers in an archive.
 e  rst model was proposed by Berger and Lafferty (1999). It is similar to the query likelihood model described in section 7.3.1, except it allows query terms to
418 10 SocialSearch
be “translated” from other terms. Given a query, related questions3 are ranked
according to:
∏∑
w∈Q t∈V
P (Q|A) =
P (w|t)P (t|A)
where Q is the query, A is a related question in the archive, V is the vocabulary, P (w|t) are the translation probabilities, and P (t|D) is the smoothed probability of generating t given document D (see section 7.3.1 for more details).  erefore, we see that the model allows query term w to be translated from other terms t that may occur in the question. One of the primary issues with this model is that there is no guarantee the question will be related to the query; since every term is translated independently, the question with the highest score may be a good term-for-term translation of the query, but not a good overall translation.
 e second model, developed by Xue et al. (2008), is an extension of Berger’s model that attempts to overcome this issue by allowing matches of the original query terms to be given more weight than matches of translated terms. Under this model, questions (or answers) are ranked using the following formula:
|C|
where β is a parameter between 0 and 1 that controls the in uence of the transla- tion probabilities, and μ is the Dirichlet smoothing parameter. Notice that when β = 0, this model is equivalent to the original query likelihood model, with no in uence from the translation model. As β approaches 1, the translation model begins to have more impact on the ranking and becomes more similar to Berger’s model.
Ranking using these models can be computationally expensive, since each in- volves a sum over the entire vocabulary, which can be very large. Query processing speeds can be signi cantly improved by considering only a small number of trans- lations per query term. For example, if the  ve most likely translations of each query term are used, the number of terms in the summation will be reduced from V to 5.
 e one major issue that has been ignored thus far is how to compute the trans- lation probabilities. In cross-language retrieval, translation probabilities can be
3  e discussion focuses on question retrieval, but the same models can be used to re- trieve archived answers. As we said, question retrieval is generally more effective.
∏ (1 − β)fw,A + β ∑ P(w|t)ft,A + μcw
 P(Q|A) = t∈V w∈Q |A| + μ
  
   xp
  xp window install drive computer version click pc program microso 
                  everest
everest mountain tallest 29,035 highest mt
  measure feet mount
10.3 SearchingwithCommunities 419
search search google information internet website web
list free info page
  Table 10.2. Translations automatically learned from a set of question and answer pairs.  e 10 most likely translations for the terms “everest”, “xp”, and “search” are given.
automatically learned using a parallel corpus. Translation probabilities are esti- matedfrompairsofdocumentsoftheform{(D1s,D1t),...,(DNs ,DNt )},where Dis is document i written in the source language and Dit is document i written in the target language. However, the notion of a parallel corpus becomes hazy when dealing with intralanguage translations. A variety of approaches have been used for estimating translation probabilities within the same language. For  nding related questions, one of the most successful approaches makes the assumption that question/answer pairs form a parallel corpus from which translation prob- abilities can be estimated.  at is, translation probabilities are estimated from archived pairs of the form {(Q1, A1), . . . , (QN , AN )}, where Qi is question i and Ai is answer i. Example translations estimated from a real question and an- swer database using this approach are shown in Table 10.2. Pointers to algorithms for estimating translation probabilities given a parallel corpus are given in the “References and Further Reading” section at the end of this chapter.
In this section, we assumed that people in a community would provide an- swers to questions, and an archive of questions and answers would be created by this process. As we mentioned in Chapter 1, it is also possible to design question answering systems that  nd answers for a more limited range of questions in the textoflargedocumentcorpora.WedescribethesesystemsinmoredetailinChap- ter 11.
420 10 SocialSearch
10.3.4 Collaborative Searching
 e  nal community-based search task that we consider is collaborative search- ing. As the name suggests, collaborative searching involves a group of users with a common goal searching together in a collaborative setting.  ere are many situa- tions where collaborative searching can be useful. For example, consider a group of students working together on a world history report. In order to complete the report, the students must do background research on the report topic. Tradition- ally, the students would split the topic into various subtopics, assign each group member one of the subtopics, and then each student would search the Web or an online library catalog, independently of the other students, for information and resources on their speci c subtopic. In the end, the students would have to com- bine all of the information from the various subtopics to form a coherent report. Each student would learn a great deal about his or her particular subtopic, and no single student would have a thorough understanding of all of the material in the report. Clearly, every student would end up learning a great deal more if the research process were more collaborative. A collaborative search system would al- low the students to search the Web and other resources together, so that every member of the group could contribute and understand every subtopic of the re- port. Collaborative search can also be useful within companies, where colleagues must collect information about various aspects of a particular project. Last, but certainly not least, recreational searchers may  nd collaborative search systems particularly useful. Suppose you and your friends are planning a party. A collabo- rative search system would help everyone coordinate information-gathering tasks, such as  nding recipes, choosing decorations, selecting music, deciding on invita- tions, etc.
 ere are two common types of collaborative search scenarios, depending on where the search participants are physically located with respect to each other.  e  rst scenario, known as co-located collaborative search, occurs when all of the search participants are in the same location, such as the same office or same li- brary, sitting in front of the same computer.  e other scenario, known as remote collaborative searching, occurs when the search participants are physically located in different locations.  e participants may be in different offices within the same building, different buildings within the same city, or even in completely different countries across the globe. Figure 10.5 provides a schematic for these scenarios. Both situations present different challenges, and the systems developed for each havedifferentrequirementsintermsofhowtheysupportsearch.Toillustratethis, we brie y describe two examples of collaborative search systems.
            10.3 SearchingwithCommunities 421
Fig. 10.5. Overview of the two common collaborative search scenarios. On the le  is co- located collaborative search, which involves multiple participants in the same location at the same time. On the right is remote collaborative search, where participants are in different locations and not necessarily all online and searching at the same time.
 e CoSearch system developed by Amershi and Morris (2008) is a co-located collaborative search system.  e system has a primary display, keyboard, and mouse that is controlled by the person called the “driver”, who leads the search task. Additional participants, called “observers”, each have a mouse or a Bluetooth- enabled4 mobile phone.  e driver begins the session by submitting a query to a search engine.  e search results are displayed on the primary display and on the display of any user with a mobile phone. Observers may click on search results, which adds the corresponding page into a shared “page queue.”  is allows every participant to recommend which page should be navigated to next in a conve- nient, centralized manner, rather than giving total control to the driver. In addi- tion to the page queue, there is also a “query queue,” where participants submit new queries.  e query queue provides everyone with a list of potentially useful queries to explore next, and provides the driver with a set of options generated
4 Bluetooth is the name of a short-range wireless technology that allows for communi- cation between devices, such as laptops, printers, PDAs, and mobile phones.
                                                                                                                                                               Co-located Collaborative Searching
                                                                                                                                                                                     Remote Collaborative Searching
 
422 10 SocialSearch
collectively by the group.  e CoSearch system provides many useful ways for a group of people to collaboratively search together, since it allows everyone to work toward a common task, while at the same time preserving the important di- vision of labor that is part of collaboration, via the use of multiple input devices.
An example of a remote collaborative search system is SearchTogether, devel- oped by Morris and Horvitz (2007b). In this system, it is assumed that every participant in the session is in a different location and has his own computer. Furthermore, unlike co-located search, which assumes that all of the participants are present during the entire search session, remote search makes no assumptions about whether everyone is online at the same time.  erefore, whereas co-located search sessions tend to be transient, remote search sessions can be persistent. Users of the system may submit queries, which are logged and shared with all of the other search participants.  is allows all participants to see what others are search- ing for, and allows them to resubmit or re ne the queries. Users can add ratings (e.g., “thumbs up” or “thumbs down”) and comments to pages that are viewed during the search process, which will be aggregated and made available to other participants. In addition, a participant may explicitly recommend a given page to another participant, which will then show up in her recommended pages list.  erefore, the SearchTogether system provides most of the functionality of the CoSearch system, except it is adapted to the speci c needs of remote collabora- tion. One particular advantage of a persistent search session is that new partici- pants, who were not previously part of the search process, can quickly be brought up to speed by browsing the query history, page ratings, comments, and recom- mendations.
As we have outlined, collaborative search systems provide users with a unique set of tools to effectively collaborate with each other during a co-located or re- mote search session. Despite the promise of such systems, very few commercial collaborative search systems exist today. However, such systems are beginning to gain considerable attention in the research community. Given this, and the in- creasingly collaborative nature of the online experience, it may be only a matter of time before collaborative search systems become more widely available.
10.4 FilteringandRecommending 423
10.4 Filtering and Recommending
10.4.1 Document Filtering
As we mentioned previously, one part of social search applications is representing individual users’ interests and preferences. One of the earliest applications that fo- cused on user pro les was document  ltering. Document  ltering, o en simply referred to as  ltering, is an alternative to the standard ad hoc search paradigm. In ad hoc search, users typically enter many different queries over time, while the document collection stays relatively static. In  ltering, the user’s information need stays the same, but the document collection itself is dynamic, with new docu- ments arriving periodically.  e goal of  ltering, then, is to identify ( lter) the rel- evant new documents and send them to the user. Filtering, as described in Chapter 3, is a push application.
Filtering is also an example of a supervised learning task, where the pro le plays the part of the training data and the incoming documents are the test items that need to be classi ed as “relevant” or “not relevant.” However, unlike a spam de- tection model, which would take thousands of labeled emails as input, a  ltering system pro le may only consist of a single query, making the learning task even more challenging. For this reason,  ltering systems typically use more specialized techniques than general classi cation techniques in order to overcome the lack of training data.
Although they are not as widely used as standard web search engines, there are many examples of real-world document  ltering systems. For example, many news sites offer  ltering services.  ese services include alerting users when there is breaking news, when an article is published in a certain new category (e.g., sports or politics), or when an article is published about a certain topic, which is typically speci ed using one or more keywords (e.g., “terrorism” or “global warming”).  e alerts come in the form of emails, SMS (text messages), or even personalized news feeds, thereby allowing the user to keep up with topics of interest without having to continually check the news site for updates or enter numerous queries to the site’s search engine.  erefore,  ltering provides a way of personalizing the search experience by maintaining a number of long-term information needs.
Document  ltering systems have two key components. First, the user’s long- term information needs must be accurately represented.  is is done by construct- ing a pro le for every information need. Second, given a document that has just ar- rived in the system, a decision mechanism must be devised for identifying which are the relevant pro les for that document.  is decision mechanism must not
424 10 SocialSearch
only be efficient, especially since there are likely to be thousands of pro les, but it must also be highly accurate.  e  ltering system should not miss relevant doc- uments and, perhaps even more importantly, should not be continually alerting users about non-relevant documents. In the remainder of this section, we describe the details of these two components.
Profiles
In web search, users typically enter a very short query.  e search engine then faces the daunting challenge of determining the user’s underlying information need from this very sparse piece of information.  ere are numerous reasons why most search engines today expect information needs to be speci ed as short key- word queries. However, one of the primary reasons is that users do not want to (or do not have the time to) type in long, complex queries for each and every one of their information needs. Many simple, non-persistent information needs can o en be satis ed using a short query to a search engine. Filtering systems, on the other hand, cater to long-term information needs.  erefore, users may be more willing to spend more time specifying their information need in greater detail in order to ensure highly relevant results over an extended period of time.  e rep- resentation of a user’s long-term information need is o en called a  ltering pro le or just a pro le.
What actually makes up a  ltering pro le is quite general and depends on the particular domain of interest. Pro les may be as simple as a Boolean or key- word query. Pro les may also contain documents that are known to be relevant or non-relevant to the user’s information need. Furthermore, they may contain other items, such as social tags and related named entities. Finally, pro les may also have one or more relational constraints, such as “published before 1990”, “price in the $10–$25 range”, and so on. Whereas the other constraints described act as so   lters, relational constraints of this form act as hard  lters that must be satis ed in order for a document to be retrieved.
Although there are many different ways to represent a pro le, the underlying  ltering model typically dictates the actual representation. Filtering models are very similar to the retrieval models described in Chapter 7. In fact, many of the widely used  ltering models are simply retrieval models where the pro le takes the place of the query.  ere are two common types of  ltering models.  e  rst are static models. Here, static refers to the fact that the user’s pro le does not change over time, and therefore the same model can always be applied.  e second are
10.4 FilteringandRecommending 425
adaptive models, where the user’s pro le is constantly changing over time.  is scenario requires the  ltering model to be dynamic over time as new information is incorporated into the pro le.
Static filtering models
As we just described, static  ltering models work under the assumption that the  ltering pro le remains static over time. In some ways, this makes the  ltering process easier, but in other ways it makes it less robust. All of the popular static  ltering models are derived from the standard retrieval models described in Chap- ter 7. However, unlike web search,  ltering systems do not return a ranked list of documents for each pro le. Instead, when a new document enters the system, the  ltering system must decide whether or not it is relevant with respect to each pro-  le. Figure 10.6 illustrates how a static  ltering system works. As new documents arrive, they are compared to each pro le. Arrows from a document to a pro le indicate that the document was deemed relevant to the pro le and returned to the user.
t = 2 t = 3 t = 5 t = 8 Document Stream
Fig. 10.6. Example of a static  ltering system. Documents arrive over time and are com- pared against each pro le. Arrows from documents to pro les indicate the document matches the pro le and is retrieved.
    Profile 1
          Profile 2
           Profile 3
                                  
426 10 SocialSearch
In the most simple case, a Boolean retrieval model can be used. Here, the  lter- ing pro le would simply consist of a Boolean query, and a new document would be retrieved for the pro le only if it satis ed the query.  e Boolean model, de- spite its simplicity, can be used effectively for document  ltering, especially where precision is important. In fact, many web-based  ltering systems make use of a Boolean retrieval model.
One of the biggest drawbacks of the Boolean model is the low level of recall. Depending on the  ltering domain, users may prefer to have good coverage over very high precision results.  ere are various possible solutions to this problem, including using the vector space model, the probabilistic model, BM25, or lan- guage modeling. All of these models can be extended for use with  ltering by specifying a pro le using a keyword query or a set of documents. Directly apply- ing these models to  ltering, however, is not trivial, since each of them returns a score, not a “retrieve” or “do not retrieve” answer as in the case of the Boolean model. One of the most widely used techniques for overcoming this problem is to use a score threshold to determine whether to retrieve a document.  at is, only documents with a similarity score above the threshold will be retrieved. Such a threshold would have to be tuned in order to achieve good effectiveness. Many complex issues arise when applying a global score threshold, such as ensuring that scores are comparable across pro les and over time.
As a concrete example, we describe how static  ltering can be done within the language modeling framework for retrieval. Given a static pro le, which may consist of a single keyword query, multiple queries, a set of documents, or some combination of these, we must  rst estimate a pro le language model denoted by P .  ere are many ways to do this. One possibility is:
(1 − λ) ∑K fw,Ti cw P(w|P)=∑K α αi |Ti| +λ|C|
i=1 i i=1
where T1, . . . , Tk are the pieces of text (e.g., queries, documents) that make up the pro le, and αi is the weight (importance) associated with text Ti.  e other variables and parameters are de ned in detail in Chapter 7.
 en, given an incoming document, a document language model (D) must be estimated. We again follow the discussion in Chapter 7 and estimate D using the following :
P(w|D) = (1 − λ)fw,D + λ cw |D| |C|
     
10.4 FilteringandRecommending 427
Documents can then be ranked according to the negative KL-divergence be- tween the pro le language model (P ) and the document language model (D) as follows:
∑∑
−KL(P||D) = P(w|P)logP(w|D) − P(w|P)logP(w|P) w∈V w∈V
Document D is then delivered to pro le P if −KL(P ||D) ≥ t, where t is some relevance threshold.
Document  ltering can also be treated as a machine learning problem. At its core,  ltering is a classi cation task that o en has a very small amount of training data (i.e., the pro le).  e task is then to build a binary classi er that determines whether an incoming document is relevant to the pro le. However, training data would be necessary in order to properly learn such a model. For this task, the train- ing data comes in the form of binary relevance judgments over pro le/document pairs. Any of the classi cation techniques described in Chapter 9 can be used. Suppose that a Support Vector Machine with a linear kernel is used; the scoring function would then have the following form:
s(P;D) = w · f(P,D) = w1f1(P,D) + w2f2(P,D) + ... + wdfd(P,D)
where w1, . . . , wd are the set of weights learned during the SVM training pro- cess, and f1(P, D), . . . , fd(P, D) are the set of features extracted from the pro-  le/document pair. Many of the features that have been successfully applied to text classi cation, such as unigrams and bigrams, can also be applied to  ltering. Given a large amount of training data, it is likely that a machine learning approach will outperform the simple language modeling approach just described. However, when there is little or no training data, the language modeling approach is a good choice.
Adaptive filtering models
Static  ltering pro les are assumed not to change over time. In such a setting, a user would be able to create a pro le, but could not update it to better re ect his information need.  e only option would be to delete the pro le and create a new one that would hopefully produce better results.  is type of system is rigid and not very robust.
Adaptive  ltering is an alternative  ltering technique that allows for dynamic pro les.  is technique provides a mechanism for updating the pro le over time.
428 10 SocialSearch
      Profile 1
Profile 1.1
              Profile 2
Profile 2.1
                           Profile 3
Profile 3.1
                            t = 2 t = 3 t = 5 t = 8 Document Stream
        Fig. 10.7. Example of an adaptive  ltering system. Documents arrive over time and are compared against each pro le. Arrows from documents to pro les indicate the document matches the pro le and is retrieved. Unlike static  ltering, where pro les are static over time, pro les are updated dynamically (e.g., when a new match occurs).
Pro les may be either updated using input from the user or done automatically based on user behavior, such as click or browsing patterns.  ere are various rea- sons why it may be useful to update a pro le as time goes on. For example, users may want to  ne-tune their information need in order to  nd more speci c types of information.  erefore, adaptive  ltering techniques are more robust than static  ltering techniques and are designed to adapt to  nd more relevant docu- ments over the life span of a pro le. Figure 10.7 shows an example adaptive  lter- ing system for the same set of pro les and incoming documents from Figure 10.6. Unlike the static  ltering case, when a document is delivered to a pro le, the user provides feedback about the document, and the pro le is then updated and used for matching future incoming documents.
As Figure 10.7 suggests, one of the most common ways to adapt a pro le is in response to user feedback. User feedback may come in various forms, each of which can be used in different ways to update the user pro le. In order to provide a concrete example of how pro les can be adapted in response to user feedback, we consider the case where users provide relevance feedback (see Chapter 6) on
10.4 FilteringandRecommending 429
documents.  at is, for some set of documents, such as the set of documents re- trieved for a given pro le, the user explicitly states whether or not the document is relevant to the pro le. Given the relevance feedback information, there are a number of ways to update the pro le. As before, how the pro le is represented and subsequently updated largely depends on the underlying retrieval model that is being used.
As described in Chapter 7, the Rocchio algorithm can be used to perform rel- evance feedback in the vector space model.  erefore, if pro les are represented as vectors in a vector space model, Rocchio’s algorithm can be applied to update the pro les when the user provides relevance feedback information. Given a pro-  le P , a set of non-relevant feedback documents (denoted N onrel), and a set of relevant feedback documents (denoted Rel), the adapted pro le P ′ is computed as follows:
′1∑1∑
P = α.P + β.|Rel| Di − γ.|Nonrel| Di
Di ∈Rel Di ∈N onrel
where Di is the vector representing document i, and α, β, and γ are parameters that control how to trade off the weighting between the initial pro le, the relevant documents, and the non-relevant documents.
Chapter 7 also described how relevance models can be used with language mod- eling for pseudo-relevance feedback. However, relevance models can also be used for true relevance feedback as follows:
1∑∑
  P (w|P ) = |Rel| 1∑
≈ |Rel|
where C is the set of documents in the collection, Rel is the set of documents that have been judged relevant, Di is document i, and P (Di|D) is the probability that document Di is generated from document D’s language model.  e approxima- tion (≈) can be made because Di is a document, and P (Di|D) is going to be 1 or verycloseto1whenDi =Dandnearly0formostotherdocuments. erefore, the probability of w in the pro le is simply the average probability of w in the language models of the relevant documents. Unlike the Rocchio algorithm, the non-relevant documents are not considered.
Di∈Rel D∈C
P (w|D)P (Di|D) P(w|Di)
  Di ∈Rel
430 10 SocialSearch
If a classi cation technique, such as one of those described in Chapter 9, is used for  ltering, then an online learning algorithm can be used to adapt the clas- si cation model as new user feedback arrives. Online learning algorithms update model parameters, such as the hyperplane w in SVMs, by considering only one new item or a batch of new items.  ese algorithms are different from standard supervised learning algorithms because they do not have a “memory.”  at is, once an input has been used for training, it is discarded and cannot be explicitly used in the future to update the model parameters. Only the new training inputs are used for training.  e details of online learning methods are beyond the scope of this book. However, several references are given in the “References and Further Reading” section at the end of this chapter.
 Model
Classi cation Model Parameters Online Learning
Table 10.3. Summary of static and adaptive  ltering models. For each, the pro le repre- sentation and pro le updating algorithm are given.
Both static and adaptive  ltering, therefore, can be considered special cases of many of the retrieval models and techniques described in Chapters 6, 7, and 9. Table 10.3 summarizes the various  ltering models, including how pro les are represented and updated. In practice, the vector space model and language mod- eling have been shown to be effective and easy to implement, both for static and adaptive  ltering.  e classi cation models are likely to be more robust in highly dynamic environments. However, as with all classi cation techniques, the model requires training data to learn an effective model.
Fast filtering with millions of profiles
In a full-scale production system, there may be thousands or possibly even mil- lions of pro les that must be matched against incoming documents. Fortunately, standard information retrieval indexing and query evaluation strategies can be applied to perform this matching efficiently. In most situations, pro les are rep- resented as a set of keywords or a set of feature values, which allows each pro le to
Pro le Representation
Boolean Expression
Vector
Language Modeling Probability Distribution Relevance Modeling
Boolean Vector Space
N/A Rocchio
Pro le Updating
  
10.4 FilteringandRecommending 431
be indexed using the strategies discussed in Chapter 5. Scalable indexing infras- tructures can easily handle millions, or possibly even billions, of pro les.  en, once the pro les are indexed, an incoming document can be transformed into a “query”, which again is represented as either a set of terms or a set of features.  e “query” is then run against the index of pro les, retrieving a ranked list of pro les.  e document is then delivered to only those pro les whose score, with respect to the “query”, is above the relevance threshold previously discussed.
Evaluation
Many of the evaluation metrics described in Chapter 8 can be used to evaluate  ltering systems. However, it is important to choose appropriate metrics, because  ltering differs in a number of ways from standard search tasks, such as news or web search. One of the most important differences is the fact that  ltering sys- tems do not produce a ranking of documents for each pro le. Instead, relevant documents are simply delivered to the pro le as they arrive.  erefore, measures such as precision at rank k and mean average precision are not appropriate for the task. Instead, set-based measures are typically used.
Table 10.4. Contingency table for the possible outcomes of a  ltering system. Here, TP (true positive) is the number of relevant documents retrieved, FN (false negative) is the number of relevant documents not retrieved, FP (false positive) is the number of non- relevant documents retrieved, and TN (true negative) is the number of non-relevant doc- uments not retrieved.
Table 10.4, which is similar to Table 8.3 in Chapter 8, shows all of the possi- bilities for an incoming document with respect to some pro le. A document may be either relevant or non-relevant, as indicated by the column headings. Further- more, a document may be either retrieved or not retrieved by the  ltering system, as indicated by the row headings. All of the  ltering evaluation metrics described here can be formulated in terms of the cells in this table.
    Relevant
 Non-Relevant
 Retrieved
  TP
 FP
 Not retrieved
  FN
 TN
    e simplest way to evaluate a  ltering system is using the classical evaluation measures of precision and recall, which correspond to T P and T P , re-
  TP+FP TP+FN spectively.  e F measure, which is the harmonic mean of precision and recall, is
432 10 SocialSearch
also commonly used. Typically, these measures are computed for each pro le in the system and then averaged together.5
It is possible to de ne a more general evaluation metric that combines each of the four cells in Table 10.4 in the following way:
U=α·TP+β·TN+δ·FP+γ·FN
where the coefficients α, β, δ, and γ can be set in different ways to achieve differ- ent trade-offs between the various components of the measure. One setting of the coefficients that has been widely used in  ltering experiments is α = 2, β = 0, δ = −1, and γ = 0.  is results in true positives (relevant retrieved documents) being given weight 2 and false negatives (relevant documents that were not re- trieved) being penalized by a factor of 1. Of course, different coefficients can be chosen based on the actual costs of the underlying task.
10.4.2 Collaborative Filtering
Static and adaptive  ltering are not social tasks, in that pro les are assumed to be independent of each other. If we now consider the complex relationships that ex- ist between pro les, additional useful information can be obtained. For example, suppose that we have an adaptive  ltering system with two pro les, which we call pro le A (corresponding to user A) and pro le B (corresponding to user B). If both user A and B judged a large number of the same documents to be relevant and/or non-relevant to their respective pro les, then we can infer that the two pro les are similar to each other. We can then use this information to improve the relevance of the matches to both user A and B. For example, if user A judged a document to be relevant to pro le A, then it is likely that the document will also be relevant to pro le B, and so it should probably be retrieved, even if the score assigned to it by the adaptive  ltering system is below the predetermined thresh- old. Such a system is social, in the sense that a document is returned to the user based on both the document’s topical relevance to the pro le and any judgments or feedback that users with similar pro les have given about the document.
Filtering that considers the relationship between pro les (or between users) and uses this information to improve how incoming items are matched to pro les (or users) is called collaborative  ltering. Collaborative  ltering is o en used as a component of recommender systems. Recommender systems use collaborative
5 Recall that this type of averaging is known as macroaveraging.
 
10.4 FilteringandRecommending 433
 ltering algorithms to recommend items (such as books or movies) to users. Many major commercial websites, such as Amazon.com and Net ix, make heavy use of recommender systems to provide users with a list of recommended products in the hopes that the user will see something she may like but may not have known about, and consequently make a purchase.  erefore, such systems can be valuable both to the end users, who are likely to see relevant products, including some that they may not have considered before, and to search engine companies, who can use such systems to increase revenue.
In the remainder of this section, we focus on collaborative  ltering algorithms for recommender systems. It is important to note that these algorithms differ from static and adaptive  ltering algorithms in a number of ways. First, when collabora- tive  ltering algorithms are used for making recommendations, they typically as- sociate a single pro le with each user.  at is, the user is the pro le. Second, static and adaptive  ltering systems would make a binary decision (retrieve or do not retrieve) for each incoming document, but collaborative  ltering algorithms for recommender systems provide ratings for items.  ese ratings may be 0 (relevant) and 1 (non-relevant) or more complex, such as ratings on a scale of 1 through 5. Finally, collaborative  ltering algorithms for recommender systems provide a rat- ing for every incoming item, as well as every item in the database for which the current user has not explicitly provided a judgment. On the other hand, static and adaptive  ltering algorithms only decide whether or not to send incoming docu- ments to users and never retrospectively examine older documents to determine whether they should be retrieved.
Figure 10.8 represents a virtual space of users, where users with similar prefer- ences and tastes are close to each other.  e dialog boxes drawn above each user’s head denote their preference for some item, such as a movie about tropical  sh.  ose users who have not rated the movie have question marks in their dialog boxes. It is the job of the collaborative  ltering algorithm to predict as accurately as possible what rating these users would give to the movie.
Collaborative  ltering is conceptually simple, but the details can be difficult to get correct. For example, one must decide how to represent users and how to mea- sure the similarity between them. A er similar users have been identi ed, the user ratings must be combined in some way. Another important issue concerns how collaborative  ltering and, in particular, recommender systems should be evalu- ated. We will address these issues in the remainder of this section while describing the details of two collaborative  ltering algorithms that have been used success- fully in recommender systems.
                                                                                                                                                                                                                                                               434 10 SocialSearch
Fig. 10.8. A set of users within a recommender system. Users and their ratings for some item are given. Users with question marks above their heads have not yet rated the item. It is the goal of the recommender system to  ll in these question marks.
Rating with user clusters
In both of the algorithms that follow, we assume that we have a set of users U and a set of items I. Furthermore, ru(i) is user u’s rating of item i, and rˆu(i) is our system’s prediction for user u’s rating for item i. Note that ru(i) is typically unde ned when user u has not provided a rating for item i, although, as we will describe later, this does not need to be the case.  erefore, the general collabora- tive  ltering task is to compute rˆu(i) for every user/item pair that does not have anexplicitrating.Weassumethattheonlyinputwearegivenistheexplicitratings ru(i), which will be used for making predictions. Furthermore, for simplicity, we will assume that ratings are integers in the range of 1 through M , although most of the algorithms described will work equally well for continuous ratings.
One simple approach is to  rst apply one of the clustering algorithms de- scribed in Chapter 9 to the set of users. Typically, users are represented by their rating vectors ru = [ru(i1) . . . ru(i|U|)]. However, since not all users judge all items, not every entry of the vector ru may be de ned, which makes it challenging to compute distance measures, such as cosine similarity.  erefore, the distance measures must be modi ed to account for the missing values.  e simplest way to do this is to  ll in all of the missing ratings with some value, such as 0. Another
  1
?
5
4
1
         1
2
?
?
1
1
                                                                                                                                              3
?
5
5
                                                                                                                                                                             
                                                                                                                                                                                                                                                               10.4 FilteringandRecommending 435
possibility is to  ll in the missing values with the user’s average rating, denoted by ru, or the item’s average rating.
One of the common similarity measures used for clustering users is the corre- lation measure, which is computed as follows:
 ∑ (ru(i) − ru) · (ru′(i) − ru′)
  √∑ i∈Iu∩Iu′
i∈Iu∩Iu′ (ru(i) − ru)2
∑
i∈Iu∩Iu′ (ru′ (i) − ru′ )2
    where Iu and Iu′ are the sets of items that users u and u′ judged, respectively, which means that the summations are only over the set of items that both user u and u′ judged. Correlation takes on values in the range –1 to 1, with 1 being achieved when two users have identical ratings for the same set of items, and –1 being achieved when two users rate items exactly the opposite of each other.
Fig. 10.9. Illustration of collaborative  ltering using clustering. Groups of similar users are outlined with dashed lines. Users and their ratings for some item are given. In each group, there is a single user who has not judged the item. For these users, the unjudged item is assigned an automatic rating based on the ratings of similar users.
In Figure 10.9, we provide a hypothetical clustering of the users, denoted by dashed boundaries. A er users have been clustered, any user within a cluster that
   1
?
5
4
1
           A
2
D
1
?
1
B
3
?
5
                                                                                         ?1C
                                                                                                                                                 5
                                                                                  
436 10 SocialSearch
has not judged some item could be assigned the average rating for the item among other users in the cluster. For example, the user who has not judged the tropical  sh movie in cluster A would be assigned a rating of 1.25, which is the average of the ratings given to the movie by the four other users in cluster A.  is can be stated mathematically as:
rˆu(i) = |Cluster(u)|
u′∈Cluster(u)
1∑
ru′(i)
 where Cluster(u) represents the cluster that user u belongs to.
Averaging the ratings of a group of users is one simple way of aggregating the ratings within a cluster. Another possible approach is to use the expected rating of the item, given the ratings of the other users within the cluster, which is calculated
as:
rˆu(i) = =
x · P (ru(i) = x|C = Cluster(u)) ∑M |u′:ru′(i)=x|
∑M x=1
x · |Cluster(u)|
where P (ru(i) = x|C = Cluster(u)) is the probability that user u will rate the
 x=1
item with rating m, given that they are in cluster Cluster(u).  is probability is estimated as |u′:ru′ (i)=x| , which is the proportion of users in Cluster(u) who
 |Cluster(u)|
have rated item i with value x. For example, if all of the users in Cluster(u) rate
the item 5, then rˆu(i) will equal 5. However, if  ve users in Cluster(u) rate the item1and veusersrateit5,thenrˆu(i) = 1· 5 +5· 5 = 3.
One issue that arises when relying on clustering for predicting ratings is very sparse clusters, such as cluster D in Figure 10.9. What score should be assigned to a user who does not  t nicely into a cluster and has rather unique interests and tastes?  is is a complex and challenging problem with no straightforward answer. One simple, but not very effective, solution is to assign average item ratings to ev- ery unrated item for the user. Unfortunately, this explicitly assumes that “unique” users are average, which is actually unlikely to be true.
Rating with nearest neighbors
An alternative strategy to using clusters is to use nearest neighbors for predicting user ratings.  is approach makes use of the K nearest neighbors clustering tech- nique described in Chapter 9. To predict ratings for user u, we  rst  nd the K
  10 10
10.4 FilteringandRecommending 437
users who are closest to the user according to some similarity measure. Once we have found the nearest neighbors, we will use the ratings (and similarities) of the neighbors for prediction as follows:
1∑′
rˆu(i) = ru + ∑ sim(u, u′) sim(u, u )(ru′ (i) − ru′ )
u′ ∈N (u) u′ ∈N (u)
wheresim(u,u′)isthesimilarityofuseruandu′,andN(u)isthesetofu’snear- est neighbors.  is algorithm predicts user u’s rating of item i by  rst including the user’s average item rating (ru).  en, for every user u′ in u’s nearest neighbor- hood,ru′(i)−ru′ isweightedbysim(u,u′)andaddedintothepredictedvalue. Youmaywonderwhyru′(i)−ru′ isusedinsteadofru′(i). edifferenceisused because ratings are relative. Certain users may very rarely rate any item with 1, whereas other users may never rate an item below 3.  erefore, it is best to mea- sure ratings relative to a given user’s average rating for the purpose of prediction.
Although the approaches using clusters and nearest neighbors are similar in nature, the nearest-neighbors approach tends to be more robust with respect to noise. Furthermore, when using nearest neighbors, there is no need to choose a clustering cost function, only a similarity function, thereby simplifying things. Empirical results suggest that predicting ratings using nearest neighbors and the correlation similarity measure tends to outperform the various clustering ap- proaches across a range of data sets. Based on this evidence, the nearest neighbor approach, using the correlation similarity function, is a good choice for a wide range of practical collaborative  ltering tasks.
Evaluation
Collaborative  ltering recommender systems can be evaluated in a number of ways. Standard information retrieval metrics, such as those described in Chap- ter 8, can be used, including accuracy, precision, recall, and the F measure.
However, standard information retrieval measures are very strict, since they require the system to predict exactly the correct value. Consider the case when the actual user rating is 4 and system A predicts 3 and system B predicts 1.  e accuracy of both system A and B is zero, since they both failed to get exactly the right answer. However, system A is much closer to the correct answer than system B. For this reason, a number of evaluation metrics that consider the difference between the actual and predicted ratings have been used. One such measure is absolute error, which is computed as:
      
438 10 SocialSearch
ABS = 1 ∑∑|rˆu(i)−ru(i)| |U||I| u∈U i∈I
where the sums are over the set of user/item pairs for which predictions have been made.  e other measure is called mean squared error, which can be calculated as:
 1∑∑2
(rˆu(i) − ru(i))
 e biggest difference between absolute error and mean squared error is that mean squared error penalizes incorrect predictions more heavily, since the penalty is squared.
 ese are the most commonly used evaluation metrics for collaborative  lter- ing recommender systems. So, in the end, which measure should you use? Unfor- tunately, that is not something we can easily answer. As we have repeated many times throughout the course of this book, the proper evaluation measure depends a great deal on the underlying task.
10.5 Peer-to-Peer and Metasearch
10.5.1 Distributed Search
We have described social search applications that involve networks or communi- ties of people, but a number of tools for  nding and sharing information are im- plemented using communities of “nodes,” where each node can store and search information, and communicate with other nodes.  e simplest form of this type of distributed search environment is a metasearch engine, where each node is a complete web search engine and the results from a relatively small number of dif- ferent search engines are combined with the aim of improving the effectiveness of the ranking. A peer-to-peer (P2P) search application, on the other hand, typically has a large number of nodes, each with a relatively small amount of information and only limited knowledge about other nodes.
In contrast to search applications that use only a single document collection, all distributed search6 applications must carry out three additional, important functions:
6 O en called distributed information retrieval or federated search.
MSE = |U||I|
u∈U i∈I
  
10.5 Peer-to-PeerandMetasearch 439
• Resource representation: Generating a description of the information resource (i.e., the documents) stored at a node.
• Resource selection: Selecting one or more resources to search based on their descriptions.
• Result merging: Merging the ranked result lists from the searches carried out at the nodes containing the selected information resources.
 ese functions are carried out by designated nodes that depend on the archi- tecture of the application.  e simplest assumption is that there will be one special node that provides the directory services of selection and merging, and every other node is responsible for providing its own representation. For a metasearch appli- cation with the architecture shown in Figure 10.10, the resource representation and selection functions are trivial. Rather than selecting which search engines to use for a particular query, the query will instead be broadcast by the metasearch engine to all engines being used by the application. For each search engine, this is done using the application programming interface (API) and transforming the query into the appropriate format for the engine.  is transformation is generally very simple since most search engine query languages are similar.
   Query
Web Search  Engine 1
Web Search  Engine 2
Web Search  Engine 3
Result List
                  Query
Merged Result List
Metasearch Engine
                         Fig. 10.10. Metasearch engine architecture.  e query is broadcast to multiple web search engines and result lists are merged.
More generally, in a distributed search environment, each node can be repre- sented by the probabilities of occurrence of the words in the documents stored at
440 10 SocialSearch
that node.  is is the unigram language model that was used to represent docu- ments in Chapter 7. In this case, there is only one language model representing all the documents at that node, and the probabilities are estimated using word fre- quencies summed over the documents. In other words, the documents stored at that node are treated as one large document to estimate the language model.  is representation is compact, and has been shown to perform well in distributed retrieval experiments. In some applications, nodes may not be actively cooperat- ing in the distributed search protocol (process) and, in that case, only provide a search API. A language model description of the contents of those nodes can still be generated by query-based sampling.  is involves generating a series of queries to retrieve a sample of documents from the node, which are used to estimate the language model. Different strategies have been used for selecting query terms, but even queries based on random selection from the terms in retrieved documents have been shown to generate an accurate language model.
Resource selection in a general distributed search application involves  rst ranking the nodes using their representations, and then selecting the top k ranked nodes, or all nodes that score above some threshold value. Since we are represent- ing the nodes using a language model, the natural ranking algorithm to use is query likelihood.  is is sometimes referred to in the distributed search literature as the KL-divergence resource ranking algorithm, since query likelihood is a special case of KL-divergence. Following the query likelihood score given in section 7.3.1, a node N is ranked by the following score:
logP(Q|N) =
∑n fq ,N +μP(qi|C)
log i
|N| + μ
 i=1
where there are n query terms, fqi,N is the frequency of query term qi in the doc- uments at node N, P(qi|C) is the background probability estimated using some large collection of text C, and |N| is the number of term occurrences in the doc- uments stored at node N .
A er nodes have been selected, local searches are carried out at each of those nodes.  e results of these searches must be merged to produce a single ranking. If the same retrieval model (e.g., query likelihood) and the same global statistics (e.g., the background model) are used for each local search, the merge can be based directly on the local scores. If different global statistics are used in each node, such as calculating idf weights using only the documents at that node, then the local scores can be recalculated by sharing these statistics before merging. If different retrieval models are used, or if global statistics cannot be shared, then the scores
10.5 Peer-to-PeerandMetasearch 441
must be normalized before merging. A common heuristic approach to score nor- malization is to use the score from the resource ranking of the node that returned a document d, Rd, to modify the local document score, Sd, as follows:
Sd′ =Sd(α+(1−α)Rd′)
where α is a constant, and Rd′ is the resource ranking score normalized with re- spect to other resource scores. One way of normalizing the resource score is to cal- culate the minimum and maximum possible scores for a given query, Rmin and Rmax, and then:
Rd′ = (Rd − Rmin)/(Rmax − Rmin)
It is also possible to learn a score-normalizing function by comparing scores from a sample document collection to the local scores (Si & Callan, 2003).
Result merging in a metasearch application is somewhat different than gen- eral distributed search.  e two main characteristics of metasearch are that doc- ument scores from local searches generally are not available, and local searches are o en done over collections with very similar content. A metasearch engine that uses multiple web search engines, for example, is essentially using different retrieval models on the same collection (the Web). In this case, local searches pro- duce ranked lists that have high overlap in terms of the documents that are re- trieved. Effective methods of combining ranked result lists have been developed speci cally for this situation.  e most well-studied methods can be described by the following formula, giving the modi ed score for a document, Sd′ , as a function of the scores Sd,i produced by the ith search engine:
′ γ∑k Sd = nd
i=1
where nd is the number of search engines that returned document d in the result list, γ = (−1, 0, 1), and there are k search engines that returned results. When γ = −1, the modi ed score is the average of the local search scores; when γ = 0, the modi ed score is the sum of the local scores; and when γ = 1, the modi ed score is the sum of the local scores weighted by the number of search engines that returned document d.  e last variation is known as CombMNZ (combine and multiply by the number of non-zero results) and has been shown to be effective in many search engine combination experiments.
Sd,i
442 10 SocialSearch
In a typical metasearch application, scores are not available, and document ranks are used instead. In this case, the CombMNZ formula with scores based on ranks can be used.  is means, for example, that if m documents are retrieved in a result list, the score for a document at rank r would be (m − r + 1)/m.  is rank-based CombMNZ produces a merged ranking with reasonable effective- ness, although it is worse than a score-based combination. More effective rank- based combinations can be achieved with techniques based on voting procedures (Montague & Aslam, 2002).
In general, distributed search on non-overlapping collections can be compa- rable in effectiveness to searching a single collection that is the union of all the distributed collections. Of course, in most applications it would not be possible to build such a collection, but it does serve as a useful effectiveness benchmark. Experiments with TREC collections indicate that when average precision at rank 10 or 20 is used as the effectiveness measure, a distributed search that selected only 5–10 collections out of 200 was at least as effective as a centralized search, and sometimes was more effective (Powell et al., 2000). On the other hand, in a P2P testbed where the collection was distributed between 2,500 nodes and only 1% of these nodes were selected, the average precision at rank 10 was 25% lower than a centralized search (Lu & Callan, 2006).
Metasearch, which combines different searches on the same or very similar collections, generally improves retrieval effectiveness compared to a single search. TREC experiments with metasearch have shown improvements of 5–20% in mean average precision (depending on the query set used) for combinations using four different search engines, compared to the results from the single best search engine (Montague & Aslam, 2002).
10.5.2 P2P Networks
P2P networks are used in a range of applications involving communities of users, although they were popularized through  le-sharing applications for music and video, such as KaZaA and BearShare. Search in  le-sharing applications is gener- ally restricted to  nding  les with a speci ed title or some other attribute, such as the artist for music  les. In other words, they support simple exact-match re- trieval (see Chapter 7). A number of different network architectures or overlays7
7 A network overlay describes the logical connections between nodes implemented on top of the underlying physical network, which is usually the Internet.
 
10.5 Peer-to-PeerandMetasearch 443
have been developed to support this type of search. Figure 10.11 shows three of these architectures.
                               (a)
(b)
                    Fig. 10.11. Network architectures for distributed search: (a) central hub; (b) pure P2P; and (c) hierarchical P2P. Dark circles are hub or superpeer nodes, gray circles are provider nodes, and white circles are consumer nodes.
Each node in a P2P network can act as a client, a server, or both. Clients (infor- mation consumers) issue queries to initiate search. Servers (information providers) respond to queries with  les (if they have a match) and may also route queries to other nodes. Servers that maintain information about the contents of other nodes provide a directory service and are called hubs.  e architectures in Figure 10.11 differ primarily in how they route queries to providers. In the  rst architecture, which was the basis of the pioneering Napster  le-sharing application, there is a single central hub that provides directory services. Consumers send queries to the hub, which routes them to nodes that contain the matching  les. Although this
(c)
444 10 SocialSearch
architecture is efficient, it is also susceptible to failures or attacks that affect the central hub.
In the second architecture, known as “pure” P2P (for example, Gnutella 0.48), there are no hubs. A query generated by a consumer is broadcast to the other nodes in the network by  ooding, which means a node sends the query to all the nodes connected to it, each of those nodes sends the query to all of their connected nodes, and so on. Queries have a limited horizon, which restricts the number of network “hops” that can be made before they expire.  e connections between nodes are random, and each node only knows about its neighbors.  e problem with this architecture is that it does not scale well, in that the network traffic can grow exponentially with the number of connected users.
 e third architecture is the hierarchical P2P or superpeer network, which was developed as an improvement of the pure P2P network.  e Gnutella 0.6 standard is an example. In a hierarchical network, there is a two-level hierarchy of hub nodes and leaf nodes. Leaf nodes can be either providers or consumers, and connect only to hub nodes. A hub provides directory services for the leaf nodes connected to it, and can forward queries to other hubs.
All of these network architectures could be used as the basis for full distributed search instead of just exact match for  le sharing. As we have mentioned, how- ever, a hierarchical network has advantages in terms of robustness and scalability (Lu & Callan, 2006). For a distributed search application, each provider node in the network supports search over a local document collection. A consumer node provides the interface for the user to specify queries. Hubs acquire resource de- scriptions for neighboring hubs and providers, which they use to provide resource selection and result merging services. Speci cally, resource descriptions for neigh- borhoods are used to route queries more efficiently than  ooding, and resource de- scriptions for providers are used to rank the local document collections. Instead of selecting a  xed number of the top-ranked providers, in the P2P system each hub must be able to decide how many providers to use to respond to the query.
Neighborhood resource descriptions are an important part of the query rout- ing process. A neighborhood of a hub Hi in the direction of a hub Hj is the set of hubs that a query can reach in a  xed number of hops. Figure 10.12 shows an ex- ample of a hub with three neighborhoods generated by a maximum of three hops.  e advantage of this de nition of neighborhoods is that the information about
8  is means it is version 0.4 of the Gnutella standard. See http://en.wikipedia.org/wiki/Gnutella.
 
10.5 Peer-to-PeerandMetasearch 445
the providers that can be reached by traveling several hops beyond the immediate neighbors improves the effectiveness of query routing.
H
N1
N2
Fig. 10.12. Neighborhoods (Ni) of a hub node (H) in a hierarchical P2P network
 e resource description for a hub is the aggregation of the resource descrip- tions of the providers that are connected to it. In other words, it is a language model recording probabilities of occurrences of words. A neighborhood resource description is the aggregation of the hub descriptions in the neighborhood, but a hub’s contribution is reduced based on the number of hops to the hub. In other words, the closest neighbor hubs contribute the most to the neighborhood de- scription.
Retrieval experiments with distributed search implemented on a hierarchical P2P network show that effectiveness is comparable to searching using a central- ized hub, which is the architecture we assumed in the last section. More specif- ically, using neighborhood and provider descriptions to select about 1% of the 2,500 nodes in a P2P testbed produced the same average precision as selecting 1% of the nodes using a centralized hub, with about one-third of the message traffic of a query- ooding protocol (Lu & Callan, 2006).
Another popular architecture for  le-sharing systems that we have not men- tioned is a structured network.  ese networks associate each data item with a key and distribute keys to nodes using a distributed hash table (DHT). Distributed hash tables can support only exact match searching, but since they are used in a number of applications, we describe brie y how they can be used to locate a  le.
        N3
  
446 10 SocialSearch
In a DHT, all keys and nodes are represented as m-bit numbers or identi ers.  e name of a  le is converted to a key using a hash function.  e key and the associated  le are stored at one or more nodes whose identi ers are “close” in value to the key.  e de nition of distance between keys depends on the speci c DHT algorithm. In the Chord DHT, for example, the distance is the numeric difference between the two m-bit numbers (Balakrishnan et al., 2003).
To  nd a  le, a query containing the key value of the  le name is submitted to any node. Both storing and retrieving  les relies on a node being able to forward requests to a node whose identi er is “closer” to the key.  is guarantees that the request will eventually  nd the closest node. In Chord, keys are treated as points on a circle, and if k1 and k2 are the identi ers for two “adjacent” nodes, the node with identi er k2 is responsible for all keys that fall between k1 and k2. Each node maintains a routing table containing the IP addresses of a node halfway around the key “circle” from the node’s identi er, a node a quarter of the way around, a node an eighth of the way, etc. A node forwards a request for key k to the node from this table with the highest identi er not exceeding k.  e structure of the routing table ensures that the node responsible for k can be found in O(logN) hops for N nodes.
References and Further Reading
Social search is growing in popularity amongst information retrieval researchers. In particular, there has been an increased interest in social tagging and collabora- tive online environments over the past few years. It is likely that this interest will continue to grow, given the amount of research and exploration of new applica- tions that remains to be done.
 e subject of the relative effectiveness of manual and automatic indexing has been discussed for many years in information retrieval, and dates back to the original Cran eld studies (Cleverdon, 1970). A number of papers, such as Rajashekar and Cro  (1995), have shown the advantages of automatic indexing relative to manual indexing, and also that effectiveness improves when the two representations are combined. Several researchers have looked at the usefulness of tags and other types of metadata for improving search effectiveness. Research by Heymann, Koutrika, and Garcia-Molina (2008) showed that social media tags, such as those from deli.cio.us, are not useful for improving search, mainly due to poor coverage and the fact that most of the tags are already present as anchor text.
10.5 Peer-to-PeerandMetasearch 447
Hawking and Zobel (2007) report similar results for other types of metadata, and discuss the implications for the semantic web.
Both Sahami and Heilman (2006) and Metzler et al. (2007) proposed similar techniques for matching short segments of text by expanding the representations to use web search results. Although these methods were evaluated in the context of query similarity, they can easily be applied to measuring the similarity between tags.
Tag clouds similar to Figure 10.2 can be generated by so ware available on the Web, such as Wordle.9
More details of the HITS algorithm described in this section for  nding com- munities can be found in Gibson et al. (1998). Furthermore, Hopcro  et al. (2003) describe various agglomerative clustering approaches. Other work related to  nding online communities includes that of Flake et al. (2000), which de- scribes how a variety of community- nding approaches can be implemented effi- ciently. In addition, Borgs et al. (2004) looks at the problem of identifying com- munity structure within newsgroups, while Almeida and Almeida (2004) pro- pose a community-aware search engine. Finally, Leuski and Lavrenko (2006) de- scribe how clustering and language modeling can be used to analyze the behavior and interactions of users in a virtual world.
Jeon et al. (2005) described the approach of question ranking for community- based question answering. Xue et al. (2008) extended this work with more effec- tive estimation methods for the translation probabilities and showed that com- bining the archived questions and answers produced better rankings.  e prob- lem of answer quality and its effect on CQA is addressed in Jeon et al. (2006). Agichtein et al. (2008) incorporate more features into a prediction of question and answer quality, and show that features derived from the community graph of people who ask and answer questions are very important.
 e concept of community-based question answering had its origins in digital reference services (Lankes, 2004).  ere are other search tasks, beyond community- based question answering, in which users search for human-generated answers to questions. Several of these have been explored within the information retrieval community. Finding answers to questions in FAQs10 has been the subject of a number of papers. Burke et al. (1997) and Berger et al. (2000), for example, both attempt to overcome the vocabulary mismatch problem in FAQ retrieval
9 http://www.wordle.net/
10 FAQ is an abbreviation of Frequently Asked Questions.
 
448 10 SocialSearch
by considering synonyms and translations. Jijkoun and de Rijke (2005) describe retrieval from FAQ data derived from the Web, and Riezler et al. (2007) describe a translation-based model for web FAQ retrieval. Forums are another source of questions and answers. Cong et al. (2008) describe how question-answer pairs can be extracted from forum threads to support CQA services.
 ere are a number of collaborative search systems beyond those we described, including CIRE (Romano et al., 1999) and S3 (Morris & Horvitz, 2007a). Morris (2008) provides a good survey of how practitioners actually use collaborative web search systems. Pickens et al. (2008) evaluate algorithms for iterative merging of ranked lists to support collaborative search.
Belkin and Cro  (1992) provide a perspective on the connection between ad hoc retrieval and document  ltering. Y. Zhang and Callan (2001) describe an effective method for automatically setting  ltering thresholds.  e work by Schapire et al. (1998) describes how boosting, an advanced machine learning tech- nique, and Rocchio’s algorithm can be applied to  ltering. Finally, Allan (1996) showed how incremental feedback, which is akin to online learning, can be used to improve  ltering effectiveness. Although not covered in this chapter, a research area within information retrieval called topic detection and tracking has largely focused on topical  ltering (tracking) of news articles. See Allan (2002) for an overview of research on the topic.
For a more complete treatment of collaborative  ltering algorithms described in this chapter, see Breese et al. (1998). Furthermore, Herlocker et al. (2004) detail the many aspects involved with evaluating collaborative  ltering systems.
Callan (2000) gives an excellent overview of research in distributed search. A more recent paper by Si and Callan (2004) compares the effectiveness of tech- niques for resource selection. A different approach to query-based sampling for generating resource descriptions, called query probing, is described by Ipeirotis and Gravano (2004).  is work is focused on providing access to deep Web data- bases, which are databases that are accessible through the Web, but only through a search interface.
 ere are many papers describing techniques for combining the output of mul- tiple search engines or retrieval models. Cro  (2000) gives an overview of much of this research, and Montague and Aslam (2002) provide pointers to more recent work.
A general overview of P2P search and more details of how distributed search is implemented in a hierarchical network can be found in Lu and Callan (2006, 2007).
Exercises
10.1. Describe how social media tags are similar to anchor text. How are they different?
10.2. Implement two algorithms for measuring the similarity between two tags.  e  rst algorithm should use a standard retrieval model, such as language mod- eling.  e second algorithm should use the Web or another resource to expand the tag representation. Evaluate the effectiveness of the two algorithms on a set of 10–25 tags. Describe the algorithms, evaluation metrics, tag set, and results.
10.3. Compute  ve iterations of HITS (see Algorithm 3) and PageRank (see Fig- ure 4.11) on the graph in Figure 10.3. Discuss how the PageRank scores compare to the hub and authority scores produced by HITS.
10.4. Describe two examples of online communities that were not already dis- cussed in this chapter. How can the community- nding algorithms presented in this chapter be used to detect each?
10.5. Find a community-based question answering site on the Web and ask two questions, one that is low-quality and one that is high-quality. Describe the answer quality of each question.
10.6. Find two examples of document  ltering systems on the Web. How do they build a pro le for your information need? Is the system static or adaptive?
10.7. List the basic operations an indexer must support to handle the following tasks: 1) static  ltering, 2) adaptive  ltering, and 3) collaborative  ltering.
10.8. Implement the nearest neighbor–based collaborative  ltering algorithm. Using a publicly available collaborative  ltering data set, compare the effective- ness, in terms of mean squared error, of the Euclidean distance and correlation similarity.
10.9. Both the clustering and nearest neighbor–based collaborative  ltering algo- rithms described in this chapter make predictions based on user/user similarity. Formulate both algorithms in terms of item/item similarity. How can the distance between two items be measured?
10.10. Form a group of 2–5 people and use a publicly available collaborative search system. Describe your experience, including the pros and cons of using such a system.
10.5 Peer-to-PeerandMetasearch 449
450 10 SocialSearch
10.11. Suggest how the maximum and minimum resource ranking scores, Rmax
and Rmin, could be estimated for a given query.
10.12. Use the rank-based version of CombMNZ to combine the results of two search engines for a sample set of queries. Evaluate the combined ranking and compare its effectiveness to the two individual result lists.
10.13. Choose your favorite  le-sharing application and  nd out how it works. Describe it and compare it to the P2P networks mentioned in this chapter.
10.14. In a P2P network with small-world properties, any two nodes are likely to be connected by a small number of hops.  ese networks are characterized by a node having local connections to nodes that are “close” and a few long-range connections to distant nodes, where distance can be measured by content simi- larity or some other attribute, such as latency. Do you think Gnutella 0.4 or 0.6 would have content-based small-world properties? What about a structured net- work based on Chord?
11
Beyond Bag of Words
“It means the future is here, and all bets are off.” Agent Mulder,  e X-Files
11.1 Overview
 e term “bag of words” is used to refer to a simple representation of text that is used in retrieval and classi cation models. In this representation, a document is considered to be an unordered collection of words with no relationships, either syntactic or statistical, between them.1 Many of the retrieval models discussed in Chapter 7, such as the query likelihood model, the BM25 model, and even the vector space model, are based on a bag of words representation. From a linguistic point of view, the bag of words representation is extremely limited. No one could read a sorted bag of words representation and get the same meaning as normal text.  e sorted version of the last sentence, for example, is “a and as bag could get meaning no normal of one read representation same sorted text the words”.
Despite its obvious limitations, the bag of words representation has been very successful in retrieval experiments compared to more complex representations of text content. Incorporating even simple phrases and word proximity into a word- based representation, which would seem to have obvious bene ts, took many years of research before retrieval models were developed that had signi cant and consistent effectiveness bene ts. Search applications, however, have evolved be- yond the stage where a bag of words representation of documents and queries would be adequate. For these applications, representations and ranking based on many different features are required. Features derived from the bag of words are still important, but linguistic, structural, metadata, and non-textual content fea- tures can also be used effectively in retrieval models such as the inference network
1 In mathematics, a bag is like a set, but duplicates (i.e., multiple occurrences of a word) are allowed.
  
452 11 BeyondBagofWords
or the ranking SVM. We start this chapter by examining the general properties of a feature-based retrieval model.
In previous chapters we have discussed a number of representation features and how they can be used in ranking. In this chapter, we look at four aspects of representation in more detail and describe how they could affect the future de- velopment of search engines. Bag of words models assume there is no relationship between words, so we  rst look at how term dependencies can be captured and used in a linear feature-based model. Document structure is ignored in a bag of words representation, but we have seen how it can be important in web search.  e second aspect of representation we look at is how the structured representa- tions used in a database system could be used in a search engine. In a bag of words representation, queries are treated the same as documents. In question-answering applications, however, the syntactic structure of the query can be particularly im- portant.  e third aspect of representation we look at is how query structure is used to answer questions. Finally, bags of words are based on words, and there are many applications, such as image search or music search, where the features used to represent the objects that are retrieved may not be words.  e fourth aspect of representation we look at is what these non-text features could be, and how they are used in ranking.
In the  nal section of this chapter, we indulge in some mild (not wild) specu- lation about the future of search.
11.2 Feature-Based Retrieval Models
We described feature-based retrieval models brie y in Chapter 7, and provide more detail here because of their growing importance as the basis for modern search engines.
For a set of documents D and a set of queries Q, we can de ne a scoring or ranking function SΛ(D; Q) parameterized by Λ, which is a vector of parameters. Given a query Qi, the scoring function SΛ(D; Qi) is computed for each D ∈ D, and documents are then ranked in descending order according to their scores. For linear feature-based models, we restrict the scoring function to those with the form:
∑
j
where fj(D,Q) is a feature function that maps query/document pairs to real values, and Z is a constant that does not depend on D (but may depend on Λ
SΛ(D;Q)=
λj ·fj(D,Q)+Z
11.2 Feature-BasedRetrievalModels 453
or Q).  e feature functions correspond to the features that we have previously mentioned. Although some models permit non-linear combinations of features, the scoring functions that have been used in research and applications to date are based on linear combinations. For this reason, we focus here on linear feature- based models. Note that this ranking function is a generalization of the abstract ranking model that we described in Chapter 5.
In addition to de ning the form of the scoring function, we also need to spec- ifythemethodfor ndingthebestvaluesfortheparameters.Todothis,weneed a set of training data T and an evaluation function E(RΛ; T ), where RΛ is the set of rankings produced by the scoring function for all the queries.  e evalu- ation function produces real-valued output given the set of ranked lists and the training data. Note that E is only required to consider the document rankings and not the document scores.  is is a standard characteristic of the evaluation measures described in Chapter 8, such as mean average precision, precision at 10, or NDCG.
 e goal of a linear feature-based retrieval model is to  nd a parameter setting Λ that maximizes the evaluation metric E for the training data. Formally, this can
be stated as:
where RΛ are the rankings produced by the linear scoring function ∑ λj ·
fj(D,Q) + Z.
For a small number of features, the optimal parameter values can be found by
a brute-force search over the entire space of possible values. For larger numbers of features, an optimization procedure, such as that provided by the Ranking SVM model, is needed.  e key advantages of the linear feature-based models compared to other retrieval models are the ease with which new features can be added to the model, and efficient procedures for optimizing effectiveness given training data. It is these advantages that make linear feature-based models the ideal framework for incorporating the range of representation features we discuss in this chapter.
A relatively small number of features have been used as the basis of the retrieval models described in Chapter 7 that focus on topical relevance.  ese include:
• Term occurrence: whether or not a term occurs within a document
• Term  equency: number of times a term occurs within a document
• Inverse document  equency: inverse of the proportion of documents that con-
tain a given term
• Document length: number of terms in a document
Λ  = arg max E(RΛ; T ) Λ
j
454 11 BeyondBagofWords
• Term proximity: occurrence patterns of terms within a document (the most common way of incorporating term dependency)
 e Galago query language (and the inference network model it is based on) provides a means of specifying a range of features and scoring documents based on a weighted linear combination of these features. Galago is more general in that it also supports the de nition and combination of arbitrary features using the #feature operator. For example, using this operator, it is possible to have a fea- ture based on the BM25 term-weighting function as part of the scoring function. Galago, like the inference network model, does not specify a particular optimiza- tion method for  nding the best parameter values (i.e., the feature weights).
11.3 Term Dependence Models
In Chapter 4, we discussed the potential importance of relationships between words that are part of phrases. In Chapter 5, we showed how term proximity in- formation can be incorporated into indexes. Chapter 6 described techniques for measuring the association between words, and Chapter 7 showed how term rela- tionships can be expressed in the Galago query language. Exploiting the relation- ships between words is clearly an important part of building an effective search engine, especially for applications such as web search that have large numbers of documents containing all or most of the query words. Retrieval models that make use of term relationships are o en called term dependence models because they do not assume that words occur independently of each other. More generally, term dependence information can be incorporated into a number of features that are used as part of the ranking algorithm.
 e Galago implementation of web search described in section 7.5 is based on a speci c linear feature-based model known as the Markov Random Field (MRF) model (Metzler & Cro , 2005b).  is model, in addition to allowing arbitrary features, explicitly represents dependencies between terms. Although a number of term dependence models have been proposed, we describe the MRF model because it has produced signi cant effectiveness bene ts in both document rank- ing and in the related process of pseudo-relevance feedback (Metzler & Cro , 2007a).
 e MRF model works by  rst constructing a graph that consists of a docu- ment node and one node per query term.  ese nodes represent random variables within a Markov random  eld, which is a general way of modeling a joint distri-
11.3 TermDependenceModels 455
bution.  us, in the MRF model, the joint distribution over the document ran- dom variable and query term random variables is being modeled. Markov random  elds are typically represented as graphs, by what is known as a graphical model. In particular, MRFs are undirected graphical models, which means the edges in the graph are undirected.  e inference network model described in Chapter 7 was an example of a directed graphical model.
DD
q1 q2 q3 q1 q2 q3 DD
q1 q2 q3 q1 q2 q3
Fig. 11.1. Example Markov Random Field model assumptions, including full indepen- dence (top le ), sequential dependence (top right), full dependence (bottom le ), and general dependence (bottom right)
 e MRF models dependencies between random variables by drawing an edge between them. Since the importance of query terms depends on the document, the document node is always connected to every query term node. It is straight- forward to model query term dependencies by drawing edges between the query term nodes.  ere are several possible ways to determine which query term nodes to draw an edge between.  ese different cases are summarized in Figure 11.1. In the simplest case, no edges are drawn between the query terms.  is corresponds to the full independence assumption, where no dependencies exist between the query terms.  is is analogous to a unigram language model or any of the bag of words models described in Chapter 7. Another possibility is to draw edges be- tween adjacent query terms.  is is known as the sequential dependence assump-
                                  
456 11 BeyondBagofWords
tion. Here it is assumed that adjacent terms are dependent on each other, but not on terms that are farther away.  is type of assumption is similar to a bigram lan- guage model. Another possible assumption is that all terms are somehow depen- dent on all other terms.  is is known as the full dependence assumption.  e  nal possibility is that edges are drawn between the query terms in some meaningful way, such as automatically or manually identifying terms that are dependent on each other.  is is referred to as general dependence. In practice, however, it has been shown that using the sequential dependence assumption is the best option. In fact, all attempts to manually or automatically determine which terms to model dependencies between have come up short against using the simple assumption that adjacent terms are dependent on each other.
A er the MRF graph has been constructed, a set of potential functions must be de ned over the cliques of the graph.  e potential functions are meant to mea- sure the compatibility between the observed values for the random variables in the clique. For example, in the sequential dependence graph shown in Figure 11.1, a potential function over the clique consisting of the terms q1, q2, and D might compute how many times the exact phrase “q1 q2” occurs in document D, or how many times the two terms occur within some window of each other.  erefore, these potential functions are quite general and can compute a variety of differ- ent features of the text. In this way, the MRF model is more powerful than other models, such as language modeling or BM25, because it allows dependencies and arbitrary features to be included in a straightforward manner.
By constructing queries that have a particular form, the Galago search engine can be used to emulate one of the instantiations of the MRF model that has been very effective in TREC experiments.  is approach was also used in section 7.5. For example, given the query president abraham lincoln, the full independence MRF model is computed using the following query:
#combine(president abraham lincoln)
Notice that this is the most basic formulation possible and does not consider any dependencies between terms.  e sequential dependence MRF model can be computed by issuing the following query:
#weight(0.8 #combine(president abraham lincoln) 0.1 #combine(#od:1(president abraham)
#od:1(abraham lincoln) 0.1 #combine(#uw:8(president abraham)
#uw:8(abraham lincoln)
11.3 TermDependenceModels 457
 is query formulation consists of three parts, each of which corresponds to a speci c feature type used within the MRF model.  e  rst part scores the con- tribution of matching individual terms.  e second part scores the contribution of matching subphrases within the query.  is gives higher weight to documents that match “president abraham” and “abraham lincoln” as exact phrases. Notice that these phrases only consist of adjacent pairs of query terms, because the se- quential dependence model only models dependencies between adjacent query terms.  e  rst part of the formulation scores the contribution of matching un- ordered windows of adjacent query terms. In particular, if the terms “president” and “abraham” occur within a window of eight terms of each other in any or- der, then the document’s score will be boosted. Notice that each component is weighted, with the individual term component having weight 0.8 and the exact phrase and unordered window components being weighted 0.1.  ese weights, which were derived empirically, show the relative importance of matching indi- vidual terms versus matching phrase and proximity features within text.  e in- dividual terms are by far the most important things to match, although the other features play an important role.
 e algorithm for converting a plain text query Q to a sequential dependence MRF query in Galago is very simple.  e  rst component (for individual terms) is simply #combine(Q).  e second component puts every adjacent pair of query terms in a #od:1 operator, and then combines all such operators with a #com- bine.  e third and  nal component puts every adjacent pair of query terms into a #uw:8 operator, and again combines them with a #combine operator.  e three components are then given weights 0.8, 0.1, and 0.1, as described earlier, and com- bined within a #weight operator.
Finally, the full dependence MRF model is much more complex, since many more dependencies are modeled. However, we show the full dependence MRF model query for the sake of completeness here:
#weight(0.8 #combine(president abraham lincoln) 0.1 #combine(#od:1(president abraham)
#od:1(abraham lincoln)
#od:1(president abraham lincoln)) 0.1 #combine(#uw:8(president abraham)
#uw:8(abraham lincoln) #uw:8(president lincoln) #uw:12(president abraham lincoln)))
458 11 BeyondBagofWords
It is important to note that the #combine operator used in all of the exam- ples here could easily be replaced with the #feature operator in order to compute the individual term, exact phrase, and unordered phrase features differently. For example, one could implement BM25 weighting using #feature and compute a dependence model based on it instead of Galago’s default weighting.
 e MRF model can also be used to model dependencies in pseudo-relevance feedback, which is an important technique for query expansion that we described in section 7.3.2. Figure 11.2 compares a graphical model representation of the rel- evance model technique used for pseudo-relevance feedback with the MRF ap- proach, which is known as latent concept expansion.2  e relevance model graph (the top one) represents a bag of words, or unigram, model where words occur in- dependently of each other, given a particular document. Pseudo-relevance feed- back uses a set of highly ranked documents to estimate the probability of expan- sion words (the question marks) given the query words.  e words with the high- est probability are added to the query.
In the lower latent concept expansion graph, there are dependencies rep- resented between query words and expansion words.  e process of pseudo- relevance feedback is still the same, in that highly ranked documents are used to estimate the probabilities for possible expansion words, but the dependencies will change the way that those probabilities are estimated and generally produce better results. By modeling dependencies between the expansion words, the latent con- cept expansion model can produce multiword phrases as expansion terms rather than only words. As an example, Table 11.1 shows the top-ranked one- and two- word concepts produced by the latent concept expansion model for the query “hubble telescope achievements”.
To summarize, the MRF model, which is a linear feature-based retrieval model, is an effective method of incorporating features based on term dependence in the scoring function used to rank documents. Latent concept expansion supports pseudo-relevance feedback in the MRF framework. Latent concept expansion can be viewed as a “feature expansion” technique, in that it enriches the original feature set by including new features based on the expanded query.
2 Latent, or hidden, concepts are words or phrases that users have in mind but do not mention explicitly when they express a query.
 
11.4 StructureRevisited 459
 D
        hubble telescope achievements ? ?
D
hubble telescope achievements ? ?
Fig. 11.2. Graphical model representations of the relevance model technique (top) and latent concept expansion (bottom) used for pseudo-relevance feedback with the query “hubble telescope achievements”
11.4 Structure Revisited
 e goal of having a common platform for dealing with both structured and un- structured data is a long-standing one, going back to the 1960s. A number of ap- proaches have been suggested, both from the database and information retrieval perspective, but the motivation for  nding a solution or solutions that work has grown tremendously since the advent of very large-scale web databases. Areas that were once the exclusive concerns of information retrieval, such as statistical infer- ence and ranking, have now become important topics for database researchers, and both communities have a common interest in providing efficient indexing and optimization techniques for web-scale data. Exploiting document structure is a critical part of web search, and combining different sources of evidence ef- fectively is an important part of many database applications.  ere are many pos- sibilities for integration, such as extending a database model to more effectively deal with probabilities, extending an information retrieval model to handle more complex structures and multiple relations, or developing a uni ed model and sys-
             
460 11 BeyondBagofWords
1-word concepts
telescope hubble space mirror NASA launch astronomy shuttle test new discovery time universe optical light
2-word concepts
hubble telescope space telescope hubble space telescope mirror telescope hubble mirror telescope telescope NASA telescope space hubble mirror NASA hubble telescope astronomy telescope optical hubble optical telescope discovery telescope shuttle
                   Table 11.1. Most likely one- and two-word concepts produced using latent concept ex- pansion with the top 25 documents retrieved for the query “hubble telescope achieve- ments” on the TREC ROBUST collection
tem. Applications such as web search, e-commerce, and data mining provide the testbeds where these systems are being evaluated and compared.
In Chapter 7, we showed how document structure can be handled in the Galago query language. From a conventional database perspective, there are major problems with using Galago to represent and query data. Using relational database terminology, there is no schema,3 no means of de ning the data type of attributes, and no joins between relations.4 Instead, as described in Chapter 7, a document is represented as a (possibly nested) set of contexts de ned by tag pairs. Documents are stored in a simple database with only primary-key access, where the primary key is the document identi er.  e query language supports the de nition and combination of search features based on the structure and contents of the doc- uments. Different document types with different contexts can be incorporated
3 A schema is a description of the logical structure of the database, which in this case would be the names of the relations (tables) and the attributes in each relation.
4 A join connects tuples (rows) from two different relations based on one or more com- mon attributes. An example would be to connect product information with vendor information based on the product number attribute.
 
11.4 StructureRevisited 461
into a single Galago database. Each document is indexed only by the contexts it contains. Although there is no way of de ning the data type of contexts, opera- tors associated with a speci c data type could be de ned and applied to particular contexts. For example, a date range operator could be applied to a context that contains the creation date for a document.
Although this is very different from the functionality of a full relational data- base system, in many applications involving search engines this additional func- tionality is not needed.  e BigTable storage system described in Chapter 3, for example, does not have data types or joins. Additionally, it has only a very simple speci cation of tuple and attribute names. Systems such as BigTable focus on pro- viding data persistence and reliable access to data in an environment where many components can fail, and scalable performance using distributed computing re- sources. Access to the data is provided through a simple API that allows client applications to read, write, or delete values. Figure 11.3 summarizes the functions provided by the search engine and the database system in applications such as web search or e-commerce. Note that the indexes created by the search engine are not stored in the database system.
simple API
Fig. 11.3. Functions provided by a search engine interacting with a simple database system
11.4.1 XML Retrieval
XML is an important standard for both exchanging data between applications and encoding documents. To support this more data-oriented view, the database community has de ned languages for describing the structure of XML data (XML Schema), and querying and manipulating that data (XQuery and XPath). XQuery is a query language that is similar to the database language SQL, with the major difference that it must handle the hierarchical structure of XML data instead of
 Search Engine
 indexing  query language  ranking  scalability
 Database
 persistence  reliability  scalability
 
462 11 BeyondBagofWords
the simpler tabular structure of relational data. XPath is a subset of XQuery that is used to specify the search constraints for a single type of XML data or document. XPath, for example, could be used in an XML movie database to  nd the movies that were directed by a particular person and were released in a particular year. XQuery could be used to combine information about movies with information about actors, assuming that the XML movie database contained both movie and actor “documents.” An example would be to  nd movies starring actors who were born in Australia.
Complex database query languages, such as XQuery, that focus on the struc- ture of data and combining data are generally less useful in text search applica- tions.  e extent to which structure is useful in queries for databases of XML documents has been studied in the INEX project.5  is project has taken a simi- lar approach to TREC for search evaluation.  is means that a number of XML search tasks have been de ned, and appropriate test collections constructed for evaluating those tasks. One type of query used in these evaluations is the content- and-structure (CAS) query.  ese queries contain a description of a topic and ex- plicit references to the XML structure. CAS queries are speci ed using a simpli-  ed version of XPath called NEXI.6  e two important constructs in this query language are paths and path  lters. A path is a speci cation of an element (or node) in the XML tree structure. Some examples of path speci cations in NEXI are:
//A//B - any B element7 that is a descendant of an A element in the XML tree. A descendant element will be contained in the parent element.
//A/* - any descendant element of an A element.
A path  lter restricts the results to those that satisfy textual or numerical con- straints. Some examples are:
//A[about(.//B,”topic”)] - A elements that contain a B element that is about “topic”.  e about predicate is not de ned, but is implemented using some type of retrieval model. .//B is a relative path.
//A[.//B = 777] - A elements that contain a B element with value equal to 777.
5 INitiative for the Evaluation of XML Retrieval, http://inex.is.informatik.uni- duisburg.de/.
6 Narrowed Extended XPath (Trotman & Sigurbjörnsson, 2004).
7  is means an element that has a start tag <B> and an end tag </B>.
 
11.4 StructureRevisited 463
 e database used in the earlier INEX experiments consisted of technical ar- ticles from computer science publications.  e following are examples of some of the CAS queries:
//article[.//fm/yr < 2000]//sec[about(.,”search engines”)]
-  nd articles published before 2000 (fm is the front matter of the article) that contain sections discussing the topic “search engines”.
//article[about(.//st,+comparison) AND about (.//bib,”machine learning”)]
-  nd articles with a section title containing the word “comparison” and with a bibliography that mentions “machine learning”.
//*[about(.//fgc, corba architecture) AND about(.//p, figure
corba architecture)]
-  nd any elements that contain a  gure caption about “corba architecure” and a paragraph mentioning “ gure corba architecture”.
Although these queries seem reasonable, INEX experiments and previous re- search indicate that people do not use structural cues in their queries or, if they are forced to, generally will use them incorrectly.  ere is essentially no evidence that structure in user queries improves search effectiveness. For this reason, the INEX project has increasingly focused on content-only queries, which are the same as the queries we have been discussing throughout this book, and on techniques for automatically ranking XML elements rather than documents.
To summarize, structure is an important part of de ning features for effective ranking, but not for user queries. In applications such as web search, relatively sim- ple user queries are transformed into queries that involve many features, including features based on document structure.  e Galago query language is an example of a language that can be used to specify the features that are used for ranking, and user queries can be transformed into Galago queries. Most of the structural cues that can be speci ed in NEXI can be incorporated into features using Galago.
Database systems are used in many search applications.  e requirements for these applications, however, are different than those for typical database applica- tions, such as banking.  is has led to the development of storage systems that are simple from a database schema and query language perspective, but are also efficient, reliable, and scalable.
464 11 BeyondBagofWords
11.4.2 Entity Search
In addition to exploiting the existing structure in documents, it is also possible to create structure by analyzing the document contents. In Chapter 4, we described information extraction techniques that can be used to identify entities in text such as people, organizations, and locations. Entity search uses this structure to provide arankedlistofentitiesinresponsetoaqueryinsteadofalistofdocuments.Todo this, a representation for each entity is generated based on the words that occur near the entity in the document texts.  e simplest approach to building these representations is to create “pseudo-documents” by accumulating all words that occur within a speci ed text window (e.g., 20 words) for each occurrence of an entity. For example, if the organization entity “california institute of technology” occurred 65 times in a corpus, every word within 20 words of those 65 occur- rences would be accumulated into the pseudo-document representation.  is is the approach used in the early research on entity search, such as Conrad and Utt (1994).  ese large word-based representations can be stored in a search engine and then used to rank entities in response to a query.
Figure 11.4 shows an example of entity retrieval given in the Conrad paper.  e top-ranked organizations for this query will have co-occurred with words such as “biomedical” a number of times in the corpus.
Query:  
biomedical research and technology
Top Ranked Results:
minneapolis research   
signs inc.
syntex
california institute of technology massachusetts institute of technology therapeutic products
Fig.11.4.ExampleofanentitysearchfororganizationsusingtheTRECWallStreetJour- nal 1987 Collection
 e approach of building representations from words that occur in the context or locality of a target word or phrase (sometimes called context vectors) has been used to automatically build a thesaurus that retrieves words and phrases for query
11.4 StructureRevisited 465
expansion (Jing & Cro , 1994). Interestingly, it has also been used by cognitive scientists as the basis for a model of semantic memory (Lund & Burgess, 1996).
Much of the recent research on entity search has focused on methods for  nd- ing people who have expertise in a particular area or topic.  is task, known as expert search, has been studied for some time but has been evaluated more thor- oughly since it became part of a TREC track in 2005.  e main contribution of this research has been the development of probabilistic retrieval models for enti- ties (or experts) based on the language modeling approach (Balog et al., 2006). In general, given a set of documents D and a query q, we can rank candidate entities e by the joint distribution P (e, q) of entities and query terms. We can represent this distribution as:
∑
P(e,q) =
If we focus on the P (e, q|d) term, the problem of entity ranking can be decom-
posed into two components:
P (e, q|d) = P (q|e, d)P (e|d)
where the P (e|d) component corresponds to  nding documents that provide in- formation about an entity, and the P (q|e, d) component involves ranking en- tities in those documents with respect to a query. Different ways of estimating these probabilities result in different entity ranking algorithms. If we assume that words and entities are independent given a document, then P (e, q|d) = P (q|d)P (e|d), and the two components could be computed separately by using q and e as queries for probabilistic retrieval.  is assumption, however, ignores the relationship between words and entities that appear in the same document (which is captured in the context vector approach) and consequently the effectiveness of the method suffers. Instead, we can estimate the strength of association between e and q using the proximity of co-occurrence of the query words and the entities in a document. One way to do this, assuming a query consisting of a single term and a single entity occurrence in a document, is to estimate P (q|e, d) as:
1 ∑N
P (q|e, d) = Z
where δd is an indicator function that is 1 when the term at position i in d is q and 0 otherwise, k is a proximity-kernel function, Z = ∑Ni=1 k(q, e) is a normalizing constant, and N is the length of the document.
d∈D
P(e,q|d)P(d)
δd(i, q)k(q, e)
 i=1
466 11 BeyondBagofWords
If the query has multiple terms, we can compute the ranking score as follows:
rank ∏{∑ }
P (e, q) =
Petkova and Cro  (2007) showed that the most effective kernel function is
the Gaussian kernel, which is shown in Table 9.1 and, in this case, is
exp −||q − e||2/2σ2
where q − e is the distance in words between the query q and the entity e.  is paper also showed that, for expert search, accurate named entity recognition does not have a large effect on performance and that using a simple Galago query such as #od:2( <first name> <last name>) works well for estimating P (e|d) for people entities.
11.5 Longer Questions, Better Answers
In nearly all visions of the future that we see in movies or on television, the search engine, disguised as a computer system such as HAL 9000 in 2001: A Space Odyssey or the Computer in the Star Trek series, has evolved into a human-like as- sistant that can answer complex questions about any subject. Although web search engines provide access to a huge range of information, we are still a long way from achieving the capabilities of these intelligent assistants. One obvious difference is that queries to web search engines are generally formulated as a small number of keywords, rather than as actual questions expressed in natural language. In Chap- ter 10, we described how people who use community-based question answering systems describe their information needs in sentences, or even paragraphs, be- cause they know that other people will read them and give better responses if the problem is described well. In contrast, the same long queries will generally pro- duce very poor responses or nothing at all from a web search engine. People are forced to translate their problem into one or more appropriate keywords to get a reasonable result list. A long-term goal of information retrieval research is to de- velop retrieval models that produce accurate results from a longer, more speci c query.
 e task of question answering, which we mentioned brie y in Chapter 1 and again in Chapter 10, involves providing a speci c answer to a user’s query, rather
qi ∈q d∈D
P (qi|e, d)P (e|d)
11.5 LongerQuestions,BetterAnswers 467
than a ranked list of documents.  is task has a long history in the  elds of natural language processing and arti cial intelligence. Early question answering systems relied on detailed representations in logic of small, very speci c domains such as baseball, lunar rocks, or toy blocks. More recently, the focus has shi ed to an in- formation retrieval perspective where the task involves identifying or extracting answers found in large corpora of text.
     Question
Analysis/ Classification
Passage  Retrieval
Answer Selection
  Question
Corpus/Web Query
Answer  Features
Fig. 11.5. Question answering system architecture
Answers
  Figure 11.5 shows the typical components of a question answering system that retrieves answers from a text corpus.  e range of questions that is handled by such a system is usually limited to fact-based questions with simple, short answers, such as who, where, and when questions that have people’s names, organization names, places, and dates as answers.  e following questions are a sample from the TREC question answering (QA) track:8
Who invented the paper clip?
Where is the Valley of the Kings?
When was the last major eruption of Mt. St. Helens?
 ere are, of course, other types of fact-based questions that could be asked, and they can be asked in many different ways.  e task of the question analysis and classi cation component of the system is to classify a question by the type of answer that is expected. For the TREC QA questions, one classi cation that is frequently used has 31 different major categories,9 many of which correspond to
8  e TREC QA questions were drawn from the query logs of a variety of search appli- cations (Voorhees & Harman, 2005).
9  is is the question classi cation created by BBN.  is classi cation and others are discussed in Metzler and Cro  (2005a).
 
468 11 BeyondBagofWords
named entities (see Chapter 4) that can be automatically identi ed in text. Table 11.2 gives an example of a TREC question for each of these categories. Question classi cation is a moderately difficult task, given the large variation in question formats.  e question word what, for example, can be used for many different types of questions.
 e information derived from question analysis and classi cation is used by the answer selection component to identify answers in candidate text passages, which are usually sentences.  e candidate text passages are provided by the pas- sage retrieval component based on a query generated from the question. Text passages are retrieved from a speci c corpus or the Web. In TREC QA experi- ments, candidate answer passages were retrieved from TREC news corpora, and the Web was o en used as an additional resource.  e passage retrieval compo- nent of many question answering systems simply  nds passages containing all the non-stopwords in the question. In general, however, passage retrieval is similar to other types of search, in that features associated with good passages can be com- bined to produce effective rankings. Many of these features will be based on the question analysis. Text passages containing named entities of the type associated with the question category as well as all the important question words should ob- viously be ranked higher.
For example, with the question “where is the valley of the kings”, sentences containing text tagged as a location and the words “valley” and “kings” would be preferred. Some systems identify text patterns associated with likely answers for the question category, using either text mining techniques with the Web or pre- de ned rules. Patterns such as <question-location> in <location>, where question- location is “valley of the kings” in this case, o en may be found in answer pas- sages.  e presence of such a pattern should improve the ranking of a text passage. Another feature that has been shown to be useful for ranking passages is related words from a thesaurus such as Wordnet. For example, using Wordnet relations, words such as “fabricates”, “constructs”, and “makes” can be related to “manu- factures” when considering passages for the question “who manufactures magic chef appliances”. A linear feature-based retrieval model provides the appropriate framework for combining features associated with answer passages and learning effective weights.
 e  nal selection of an answer from a text passage can potentially involve more linguistic analysis and inference than is used to rank the text passages. In most cases, however, users of a question answering system will want to see the context of an answer, or even multiple answers, in order to verify that it appears
11.5 LongerQuestions,BetterAnswers 469
Question Category
Animal Biography Cardinal Cause/Effect Contact Info Date De nition Disease
Event Facility Facility Description Game Geo-Political Entity Language Location Money Nationality Organization Org. Description Other Percent Person
Plant Product Product Description Quantity Reason Substance Time
Use
Work of Art
 Example Question
What do you call a group of geese? Who was Monet?
How many types of lemurs are there? What is the effect of acid rain?
What is the street address of the White House? Boxing Day is celebrated on what day?
What is sake?
What is another name for nearsightedness? What was the famous battle in 1836 between Texas and Mexico?
What is the tallest building in Japan?
What type of bridge is the Golden Gate Bridge? What is the most popular sport in Japan?
What is the capital of Sri Lanka?
Name a Gaelic language.
What is the world’s highest peak?
How much money does the Sultan of Brunei have? Jackson Pollock is of what nationality?
Who manufactures Magic Chef appliances?
What kind of sports team is the Buffalo Sabres? What color is yak milk?
How much of an apple is water?
Who was the  rst Russian astronaut to walk in space? What is Australia’s national  ower?
What is the most heavily caffeinated so  drink? What does the Peugeot company manufacture?
How far away is the moon?
Why can’t ostriches  y?
What metal has the highest melting point?
What time of day did Emperor Hirohito die?
What does your spleen do?
What is the best-selling book of all time?
  Table 11.2. Example TREC QA questions and their corresponding question categories
470 11 BeyondBagofWords
to be correct or possibly to make a decision about which is the best answer. For example, a system might return “Egypt” as the answer to the Valley of the Kings question, but it would generally be more useful to return the passage “ e Val- ley of the Kings is located on the West Bank of the Nile near Luxor in Egypt.” From this perspective, we could view search engines as providing a spectrum of responses for different types of queries, from focused text passages to entire doc- uments. Longer, more precise questions should produce more accurate, focused responses, and in the case of fact-oriented questions such as those shown in Table 11.2, this will generally be true.
 e techniques used in question answering systems show how syntactic and semantic features can be used to obtain more accurate results for some queries, but they do not solve the more difficult challenges of information retrieval. A TREC query such as “Where have dams been removed and what has been the environmental impact?” looks similar to a fact-based question, but the answers need to be more comprehensive than a list of locations or a ranked list of sen- tences. On the other hand, using question answering techniques to identify the different text expressions for dam removal should be helpful in ranking answer passages or documents. Similarly, a TREC query such as “What is being done to increase mass transit use?”, while clearly not a fact-based question, should also bene t from techniques that could recognize discussions about the use of mass transit.  ese potential bene ts, however, have yet to be demonstrated in retrieval experiments, which indicates that there are signi cant technical issues involved in applying these techniques to large numbers of queries. Search engines currently rely on users learning, based on their experience, to submit queries such as “mass transit” instead of the more precise question.
11.6 Words, Pictures, and Music
Although information retrieval has traditionally focused on text, much of the in- formation that people are looking for, particularly on the Web, is in the form of images, videos, or audio. Web search engines, as well as a number of other sites, provide searches speci cally for images and video, and online music stores are a very popular way of  nding music. All of these services are text-based, relying on titles, captions, user-supplied “tags,” and other related text to create representa- tions of non-text media for searching.  is approach can be effective and is rel- atively straightforward to implement. In some cases, however, there may not be
11.6 Words,Pictures,andMusic 471
any associated text, or the text may not capture important aspects of the object be- ing represented. Many of the videos stored at video-sharing sites do not have good textual descriptions, for example, and because there are so many of them, user tags do not solve this problem. Another example is that titles do not provide an appro- priate description for searching music  les to  nd a particular melody. For these situations, researchers have been developing content-based retrieval techniques for non-text media.
Some non-text media use words to convey information and can be converted into text. Optical character recognition (OCR) technology, for example, is used to convert scanned documents containing written or printed text into machine- readable text. Speech recognition technology10 is used to convert recorded speech (or spoken documents) into text. Both OCR and speech recognition produce “noisy” text, meaning that the text has errors relative to the original printed text or speech transcript. Figure 11.6 shows two examples of the errors produced by OCR. In both cases, the OCR output was produced by off-the-shelf OCR so - ware.  e  rst example is based on a text passage created using a word proces- sor, printed, copied multiple times (to reduce the quality), and  nally scanned for OCR.  e resulting output has some small errors, but in general the OCR er- ror rate for high-quality printed text is low.  e second example uses much lower quality11 input that was created by scanning a photocopy of an old conference pa- per. In this case, the OCR output contains signi cant errors, with words such as “sponsorship” and “effectiveness” being virtually unreadable. Note that OCR er- rors occur at the character level. In other words, the errors are a result of confusion about individual characters and cause the output of incorrect characters.
Figure 11.7 shows the output of high-quality speech recognition so ware for a news broadcast. Most words are recognized correctly, but when the system en- counters words that it has not seen before (known as out-of-vocabulary, or OOV, words), it makes some signi cant errors. Many of the OOV words come from personal or organization names, such as “Pinochet” in this example, and consid- erable research effort has gone into addressing this problem. Note that speech recognition errors tend to create new words in the output, such as “coastal  sh” in the example, since the so ware attempts to  nd known words that best match the sound patterns as well as a language model. Both this type of error and the
10 Sometimes referred to as ASR (Automatic Speech Recognition).
11 By “quality” we mean image quality measured in terms of contrast, sharpness, clean
background, etc.
 
472 11 BeyondBagofWords
Original: 
    
OCR: 
   The fishing supplier had many items in stock, including a large variety of      tropical fish and aquariums ot aH sizes~ 
 
Original: 
    
OCR: 
   This work was carried out under the sp011J!0rship 01 NatiolUl1 Setenee     Foundation 0rant. NSF 0N SB0 (Studl .. In Indexing Depth and Retrieval     Eflccth”ene&&) and NSF 0N 482 (Requirements Study lor Future ‘Catalogs)• 
Fig. 11.6. Examples of OCR errors
character-level errors from OCR have the potential to reduce the effectiveness of search.
A number of evaluations with OCR and ASR data have been done at TREC and other forums.  ese studies indicate that retrieval effectiveness generally is not signi cantly impacted by OCR or ASR errors.  e main reason for this is the large amount of redundancy in the collections that are used for these eval- uations. For example, in ASR evaluations, there are o en many relevant spoken documents that each contain many instances of the important words for a query. Even if some instances of words are not recognized, other instances may be. In addition, even if a query word is consistently not recognized, there are usually a number of other words in the query that can be used to estimate relevance. A sim- ilar situation occurs with OCR data, where some instances of a word in a scanned document may be of higher image quality than others, and thus will be recog- nized successfully.  e only situation where OCR and ASR errors were shown to signi cantly reduce effectiveness was with short documents, which have little redundancy, and in environments with high error rates. Techniques such as char- acter n-gram indexing and expansion with related terms can improve performance in these situations.
  
Transcript: 
French prosecutors are investigating former Chilean strongman Augusto   Pinochet.  The French justice minister may seek his extradition from  Britain.  Three French families whose relatives disappeared in Chile  have filed a Complaint charging Pinochet with crimes against humanity.  The national court in Spain has ruled crimes committed by the   Pinochet regime fall under Spanish jurisdiction.   
 
Speech recognizer output: 
french prosecutors are investigating former chilean strongman of  coastal fish today the french justice minister may seek his  extradition from britain three french families whose relatives  disappeared until i have filed a complaint charging tenants say with  crimes against humanity the national court in spain has ruled crimes  committed by the tennessee with james all under spanish jurisdiction   
Fig. 11.7. Examples of speech recognizer errors
In contrast to media that can be converted to noisy text, content-based re- trieval of pictures12 is a more challenging problem.  e features that can be ex- tracted from an image, such as color, texture, and shape, have little semantic con- tent relative to words. For example, one of the common features used in image retrieval applications is the color histogram. Color in images is represented using a speci c color model, such as RGB.13  e RGB model represents colors as mix- tures of red, blue, and green, typically with 256 values (8 bits) being used for each component. A color histogram for an image can be created by  rst “quantizing” the color values to reduce the number of possible “bins” in the histogram. If the RGB values are quantized into 8 levels instead of 256 levels, for example, the num- ber of possible color combinations is reduced from 256×256×256 to 8×8×8 = 512 values or bins.  en, for each pixel in the image, the bin corresponding to the color value for that pixel is incremented by one.  e resulting histogram can be used to represent the images in a collection and also be indexed for fast re- trieval. Given a new image as a query, the color histogram for that image would be compared with the histograms from the image collection using some similarity measure, and images would be ranked using the similarity values.
12 Also known as content-based image retrieval (CBIR).
13 Another common model is HSV (Hue, Saturation, and Value).
11.6 Words,Pictures,andMusic 473
 
474 11 BeyondBagofWords
Fig. 11.8. Two images (a  sh and a  ower bed) with color histograms.  e horizontal axis is hue value.
Figure 11.8 shows two example images and their color histograms. In this case, the histogram is based only on hue values (rather than RGB values), so the peaks in the histogram correspond to peaks in colors. Both the  sh and the  owers are predominantly yellow, so both histograms have a similar peak in that area of the spectrum (on the le ).  e other smaller peaks are associated with greens and blues.
 e color feature is useful for  nding images with strong color similarity, such as pictures of sunsets, but two pictures with completely different content can be considered very similar based solely on their colors.  e picture of the  owers in Figure 11.8, for example, may be ranked highly in comparison with the picture of the  sh because of the similar peaks in the histograms.  e ability to  nd seman- tically related images can be improved by combining color features with texture and shape features. Figure 11.9 shows two examples of image retrieval based on texture features (the cars and the trains), and one example of retrieval based on shape features (the trademarks). Texture is broadly de ned as the spatial arrange- ment of gray levels in the image, and shape features describe the form of object boundaries and edges.  e examples show that images with similar appearance can generally be found using these types of representations, although the second
        
11.6 Words,Pictures,andMusic 475
Fig. 11.9.  ree examples of content-based image retrieval.  e collection for the  rst two consists of 1,560 images of cars, faces, apes, and other miscellaneous subjects.  e last example is from a collection of 2,048 trademark images. In each case, the le most image is the query.
example makes it clear that similarity in terms of texture does not guarantee se- mantic similarity. In the case of text-based search, high-ranking documents that are not relevant can at least be easily understood by the user. With content-based image retrieval, a retrieval failure such as the picture of the ape in Figure 11.9 will be difficult to explain to a user who is looking for pictures of trains.
Retrieval experiments have shown that the most effective way of combining image features is with a probabilistic retrieval model. If images have text captions or user tags, these features can easily be incorporated into the ranking, as we have discussed previously. Video retrieval applications are similar to image retrieval, except that they may have even more features, such as closed caption text or text generated from speech recognition.  e image component of a video is typically represented as a series of key  ame images. To generate key frames, the video is  rst segmented into shots or scenes. A video shot can be de ned as a continuous sequence of visually coherent frames, and boundaries can be detected by visual discontinuities, such a sharp decrease in the similarity of one frame to the next. Given the segmentation into shots, a single frame (picture) is selected as the key frame.  is can be done simply by using the  rst frame in each shot, or by more
   
476 11 BeyondBagofWords
sophisticated techniques based on visual similarity and motion analysis. Figure 11.10 shows an example of four key frames extracted from a video of a news con- ference.
Fig. 11.10. Key frames extracted from a TREC video clip
 e image retrieval techniques we have discussed assume that the query is an image. In many applications, users would prefer to describe the images they are looking for with a text query. Words cannot be compared directly to features de- rived from the image. Recent research has shown, however, that given enough training data, a probabilistic retrieval model can learn to associate words or cat- egories with image-based features and, in effect, automatically annotate images.  ere are actually two questions that we need to answer using this model:
- Given an image with no text annotation, how can we automatically assign meaningful keywords to that image?
- Given a text query {q1, ...qn}, how can we retrieve images that are relevant to the query?
 e relevance model described in section 7.3.2 has been shown to be effective for this task. Instead of estimating the joint probability of observing a word w with the query words P (w, q1, . . . qn), the relevance model is used to estimate P (w, i1, . . . im), where an image is assumed to be represented by a set of image terms {i1, ...im}.  e “vocabulary” of image terms in this approach is constrained by using image segmentation and clustering techniques to identify regions that are visually similar.14 As an example, in one test corpus of 5,000 images, a vocabulary of 500 image terms was used to represent the images, with each image being de- scribed by 1–10 terms.  e joint probabilities are estimated using a training set of images that have text annotations.  is model can be used to answer both of the questions just mentioned. To retrieve images for a text query, a process similar to pseudo-relevance feedback would be followed:
14 One such technique represents images as “blobs” (Carson et al., 1999).
     
11.6 Words,Pictures,andMusic 477
1. Use the text query to rank images that do contain text annotations.
2. Estimate joint probabilities for the image vocabulary given the top-ranked
images.
3. Rerankimagesusingthequeryexpandedwithimagetermsto ndimagesthat
do not have text annotations.
people, pool,  cars, formula,  clouds, jet,  fox, forest,  swimmers, water tracks, wall plane, sky river, water
Fig. 11.11. Examples of automatic text annotation of images
Alternatively, the joint probability estimates from the training set can be used to assign keywords to images that do not have annotations. Figure 11.11 shows examples of images annotated using this approach. Most of the words used to de- scribe the images are reasonable, although the annotation for the picture of the bear shows that signi cant errors can be made. Retrieval experiments using auto- matic annotation techniques have shown that this approach has promise and can increase the effectiveness of image and video retrieval in some applications. Sig- ni cant questions remain about the type and size of text and image vocabularies that will be the most effective.
Music is a media that is even less associated with words than pictures. Pictures can at least be described by words, and automatic annotation techniques can be used to retrieve images using text queries. Apart from the title, composer, per- former, and lyrics, however, it is very difficult to describe a piece of music using words. Music has a number of representations that are used for different purposes. Figure 11.12 shows three of these representations for “Fugue #10” composed by Bach.  e  rst is the audio signal from a performance of the music.  is is what is stored in a compressed form in MP3  les.  e second is a MIDI15 represen- tation that provides a digital speci cation of “events” in the music, such as the pitch, intensity, duration, and tempo of musical notes. MIDI is the standard for
15 Musical Instrument Digital Interface
     
478 11 BeyondBagofWords
communication between electronic instruments and computers.  e third rep- resentation is conventional musical notation, which contains the most explicit information, especially for polyphonic music made up of multiple parts or voices.
Fig. 11.12.  ree representations of Bach’s “Fugue #10”: audio, MIDI, and conventional music notation
A number of approaches have been developed for deriving index terms for searching from these basic representations. In the case of audio, one of the most successful has been the use of “signatures” created by hashing to represent the mu- sic. A signature could be created, for example, based on the peaks in a spectrogram of the audio in a time slice.16  is type of indexing is the basis of services that can
16 Aspectrogramrepresentstheenergyoramplitudeofeachfrequencyintheaudiosignal at a given time.
    
11.7 OneSearchFitsAll? 479
identify music based on a short recording captured using a mobile phone (e.g., Wang,2006).
Another popular approach to content-based retrieval of music is query-by- humming. In this type of system, a user literally sings, hums, or plays a tune, and a music collection is searched for similar melodies.  is type of searching is based on music that has a single melody line (monophonic).  e query is converted into a representation of a melody that consists of information such as the sequence of notes, relative pitches, and intervals between notes.  e music in the collection must also be converted to the same type of representation, and this is most read- ily done using MIDI as the starting point.  e query will be a very noisy repre- sentation of the relevant melody, so a number of retrieval models developed for text searching, such as n-gram matching and language models, have been adapted for this task (Dannenberg et al., 2007). Search techniques for polyphonic music based on probabilistic models have also been used for the retrieval of music scores represented in conventional music notation.
In summary, search techniques based on retrieval models for text have been developed for a large range of non-text media. In the case of scanned and spoken documents, retrieval effectiveness is similar to text documents because OCR and speech recognition tools generally have low error rates. Content-based retrieval of images and video shows promise, but search applications for these media must rely on associated text from sources such as captions and user tags to achieve good effectiveness. Music is difficult to describe in text, but effective music search appli- cations have been developed because index terms that have a strong relationship to what users look for can be derived from the audio or MIDI representations.
11.7 One Search Fits All?
So what is the future of search? Certainly it does not look like we will build the omniscient assistant system anytime soon. On the other hand, as we discussed before, using that type of system as our goal makes it clear how much further we have to go from current search engines. Our knowledge and understanding of search and effectiveness are steadily increasing, despite the  eld being more than 40 years old. Interestingly, this has produced a variety of different search services, rather than a single search engine with more and more capability. At the home page for a popular web search engine, we  nd links to search engines for the Web, images, blogs, maps, academic papers, products, patents, news, books,
480 11 BeyondBagofWords
 nancial information, videos, government documents, and photographs. In addi- tion, there are links to tools for desktop search, enterprise search, and advertising search. Rather than being simply multiple instances of the same search engine, it is clear that many of these use somewhat different features, ranking algorithms, and interfaces. If we look at the research literature, an even greater range of appli- cations, media, and approaches to search are being developed and evaluated. One safe prediction is that this expansion of new ideas will continue.
Despite the proliferation of customized search engines, there is also a growing consensus on the principles that underlie them. Researchers from information retrieval and related  elds, such as machine learning and natural language pro- cessing, have developed similar approaches to representing text and modeling the process of retrieval.  ese approaches are rapidly being expanded to include struc- tured data and non-text media. Research results based on new applications or data have consistently reinforced the view that probabilistic models of text and linear feature-based models for retrieval provide a powerful and effective framework for understanding search. Another prediction is that the sophistication of this under- lying “theory” of search will continue to grow and provide steady improvements in effectiveness for a range of applications.
What impact will this “theory” have on search engines? If there is more agree- ment about how to represent text and other types of data, and more agreement about how to rank potential answers in response to questions, then the search tools that developers use will become more similar than they are currently. Open source search engines, web search engines, desktop search engines and enterprise search engines available today all use different term weighting, different features, different ranking algorithms, and different query languages.  is is simply due to the fact that there is no consensus yet on the right way to do these things, but this will change. When there is more agreement on the underlying models, there will still be a variety of search tools available, but the choice between them will be based more on the efficiency of the implementation, the  exibility and adaptabil- ity for new applications, and the extent to which the tools implement the more sophisticated algorithms suggested by the models.  is is analogous to what hap- pened with database systems, where there have been multiple vendors and orga- nizations providing tools based on the relational model since the 1980s.
Another aspect of search, which we have not devoted enough coverage to in this book, is the fundamental importance of the interaction between users and the search engine, and the impact of the user’s task on this process. Information sci- entists in particular have focused on these issues and have contributed important
11.7 OneSearchFitsAll? 481
insights to our understanding of how people  nd relevant information. As social search and social networking applications have grown, the study of interaction has also expanded to include interaction between users as well as between a user and a search engine. In the near future, we can expect to see theories and models of search that incorporate users and interaction in a more explicit way than current models. Fuhr (2008) is a recent example of this development.
A common vision of the future, supported by countless  lms and television series, assumes an interface based on a simple natural language input with multi- media output. Interaction in this interface is based on dialog. Despite, or because of, its simplicity, this seems like a reasonable long-term goal. In current search interfaces, queries are certainly simple but, as we have discussed, a considerable effort can go into formulating these simple queries in order to  nd the one that retrieves relevant information. In addition, there is very little dialog or interaction, and even simple ideas, such as relevance feedback, are not used. Even worse, the availability of many different search engines means that another decision (which search engine?) is added to the interaction.
Developers in search engine companies and researchers are studying tech- niques for improving interaction. Some interfaces put results from multiple types of search engines into a single results display. Simple examples of this include putting a map search engine result at the top of the result list when the query includes an address, or a link to an academic paper at the top when the query matches the title closely. It is clear, however, that interfaces in the future must continue to evolve in order to more actively incorporate users and their knowl- edge into the search process.
A  nal prediction is self-serving in the context of this book. Search, in all its forms, will continue to be of critical importance in future so ware applica- tions. Training people to understand the principles, models, and evaluation tech- niques that underlie search engines is an important part of continuing to improve their effectiveness and efficiency.  ere are not enough courses that focus on this topic, but through this book and others like it, more people will know more about search.
References and Further Reading
Linear feature-based retrieval models are discussed in Metzler and Cro  (2007b).  is paper contains references to other models that were discussed in Chapter 7. Another recent paper discussing linear models is Gao et al. (2005).
482 11 BeyondBagofWords
Many term dependency models have been proposed in the information re- trieval literature, although few have produced interesting results. van Rijsbergen (1979) presented one of the most-cited dependency models as an extension of the Bayesian classi cation approach to retrieval. In another early paper, Cro  et al. (1991) showed that phrases and term proximity could potentially improve ef- fectiveness by modeling them as dependencies in the inference net model. Gao et al. (2004) describe a dependence model that showed signi cant effectiveness ben- e ts, especially for sequential dependencies (or n-grams). Other recent research with larger test collections have shown that term proximity information is an ex- tremely useful feature.
From an information retrieval perspective, dealing with structure in data start- ed in the 1970s with commercial search services such as MEDLINE and DIA- LOG that had Boolean  eld restrictions. In the 1970s and 1980s, a number of papers described the implementation of search engines using relational database systems (e.g., Crawford, 1981). Efficiency issues persisted with this approach un- til the 1990s, although object management systems were successfully used to sup- port indexes (e.g., Brown et al., 1994).  e 1990s were also the period when im- portant work was done on developing probabilistic extensions of database models for search applications. Fuhr and his colleagues described a probabilistic relational algebra (Fuhr & Rölleke, 1997) and a probabilistic datalog system (Fuhr, 2000).
In the commercial world, text retrieval had become a standard function in database systems such as Oracle by the early 1990s, but the explosion of web data and the growth of text-based web applications later that decade made the abil- ity to handle text effectively a critical part of most information systems. An in- teresting discussion of database and search engine integration from the database perspective can be found in Chaudhuri et al. (2005).
Another important line of research has been retrieval using structured and XML documents. Early work in this area dealt with office documents (Cro , Krovetz, & Turtle, 1990) and document markup (Cro  et al., 1992).  e XQuery query language for XML data is described in Chamberlin (2002). Kazai et al. (2003) describe the INEX project that evaluates retrieval methods for XML doc- uments. Trotman and Lalmas (2006) describe the NEXI language.
In the area of question answering, Metzler and Cro  (2005a) give an overview of techniques for question classi cation. Probabilistic approaches to question an- swering that have been shown to be effective include the maximum entropy model (Ittycheriah et al., 2001) and the translation model (Echihabi & Marcu, 2003). Both are very similar to the retrieval models described in Chapter 7.
11.7 OneSearchFitsAll? 483
Taghva et al. (1996) describe the  rst comprehensive set of experiments show- ing that OCR errors generally have little effect on retrieval effectiveness. Harding et al. (1997) show how n-grams can compensate for situations where there are signi cant OCR error rates.
 e book by Coden et al. (2002) contains a collection of papers about spo- ken document retrieval. Singhal and Pereira (1999) describe an expansion tech- nique for spoken documents that gave signi cant effectiveness improvements.  e data and major results from the TREC spoken document track are described in Voorhees and Harman (2005).
Many papers have been published about content-based image retrieval. Flickner et al. (1995) describe QBIC, one of the  rst commercial systems to incorporate re- trieval using color, texture, and shape features.  e Photobook system (Pentland et al., 1996) also had a signi cant impact on other CBIR projects. Ravela and Manmatha (1997) describe one of the  rst texture-based retrieval techniques to be evaluated using an information retrieval approach.  e SIFT (scale-invariant feature transform) algorithm (Lowe, 2004) is currently a popular method for rep- resentingimagesforsearch.Vasconcelos(2007)givesarecentoverviewofthe eld of CBIR.
In the area of content-based retrieval of music, most research is published in the International Conference on Music Information Retrieval (ISMIR).17 Byrd and Crawford (2002) give a good overview of the research issues in this  eld. Midomi18 is an example of searching music by “humming.”
With regard to the information science perspective on search and interaction, Belkin has written a number of key papers, such as Koenemann and Belkin (1996) and Belkin (2008).  e book by Ingwersen and Järvelin (2005) contains in-depth discussions of the role of interaction and context in search. Marchionini (2006) discusses similar issues with an emphasis on the search interface.
Exercises
11.1. Can you  nd other “visions of the future” related to search engines on the Web or in books,  lms, or television? Describe these systems and any unique fea- tures they may have.
17 http://www.ismir.net/
18 http://www.midomi.com
 
484 11 BeyondBagofWords
11.2. Does your favorite web search engine use a bag of words representation? How can you tell whether it does or doesn’t?
11.3. Use the Galago #feature operator to create a ranking algorithm that uses both a BM25 feature and a query likelihood feature.
11.4. Show how the linear feature-based ranking function is related to the ab- stract ranking model from Chapter 5.
11.5. How many papers dealing with term dependency can you  nd in the SIGIR proceedings since 2000? List their citations.
11.6. Write a program that converts textual queries to sequential dependence MRF queries in Galago, as described in the text. Run some queries against an index, and compare the quality of the results with and without term dependence. Which types of queries are the most improved using the dependence model? Which are hurt the most?
11.7.  ink of  ve queries where you are searching for documents structured us- ing XML.  e queries must involve structure and content features. Write the queries in English and in NEXI (explain the XML structure if it is not obvious). Do you think the structural part of the query will improve effectiveness? Give a detailed example.
11.8. Find out about the text search functionality of a database system (either commercial or open source). Describe it in as much detail as you can, including the query language. Compare this functionality to a search engine.
11.9. Find a demonstration of a question answering system running on the Web. Using a test set of questions, identify which types of questions work and which don’t on this system. Report effectiveness using MRR or another measure.
11.10. Using the text-based image search for a web search engine,  nd examples of images that you think could be retrieved by similarity based on color. Use a tool such as Photoshop to generate color histogram values based on hue (or one of the RGB channels) for your test images. Compare the histograms using some simi- larity measure (such as the normalized sum of the differences in the bin values). How well do these similarities match your visual perceptions?
11.7 OneSearchFitsAll? 485
11.11. Look at a sample of images or videos that have been tagged by users and separate the tags into three groups: those you think could eventually be done au- tomatically by image processing and object recognition, those you think would not be possible to derive by image processing, and spam. Also decide which of the tags should be most useful for queries related to those images. Summarize your  ndings.
11.12. What features would you like to have to support indexing and retrieval of personal digital photographs and videos? Which of these features are available in off-the-shelf so ware? Which of the features are discussed in research papers?
11.13. Starting with two MP3  les of two versions of the same song (i.e., different artists), use tools available on the Web to analyze and compare them. You should be able to  nd tools to generate MIDI from the audio, spectrograms, etc. Can you  nd any similarities between these  les that come from the melody? You could also try recording a song using a microphone and comparing the audio  le created from the recording with the original.
 References
AbdulJaleel, N., & Larkey, L. S. (2003). Statistical transliteration for English- Arabic cross language information retrieval. In CIKM ’03: Proceedings of the twel h international conference on information and knowledge manage- ment (pp. 139–146). ACM.
Agichtein, E., Brill, E., & Dumais, S. (2006). Improving web search ranking by incorporating user behavior information. In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on research and develop- ment in information retrieval (pp. 19–26). ACM.
Agichtein,E.,Brill,E.,Dumais,S.,&Ragno,R. (2006). Learninguserinter- action models for predicting web search result preferences. In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on re- search and development in information retrieval (pp. 3–10). ACM.
Agichtein, E., Castillo, C., Donato, D., Gionis, A., & Mishne, G. (2008). Finding high-quality content in social media. In WSDM ’08: Proceedings of the international conference on web search and web data mining (pp. 183–194). ACM.
Allan,J. (1996). Incrementalrelevancefeedbackforinformation ltering. In
SIGIR ’96: Proceedings of the 19th annual international ACM SIGIR con- ference on research and development in information retrieval (pp. 270–278). ACM.
Allan, J. (Ed.). (2002). Topic detection and tracking: Event-based information organization. Norwell, MA: Kluwer Academic Publishers.
Almeida, R. B., & Almeida, V. A. F. (2004). A community-aware search engine. In
WWW ’04: Proceedings of the 13th international conference on World Wide Web (pp. 413–421). ACM.
488 References
Amershi, S., & Morris, M. R. (2008). CoSearch: A system for co-located collaborative web search. In CHI ’08: Proceeding of the twenty-sixth annual SIGCHI conference on human factors in computing systems (pp. 1,647–1,656). ACM.
Anagnostopoulos, A., Broder, A., & Carmel, D. (2005). Sampling search-engine results. In WWW ’05: Proceedings of the 14th international conference on World Wide Web (pp. 245–256). ACM.
Anh, V. N., & Moffat, A. (2005). Simpli ed similarity scoring using term ranks. In SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval (pp. 226–233). ACM.
Anh, V. N., & Moffat, A. (2006). Pruned query evaluation using pre-computed impacts. In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval (pp. 372–379). New York: ACM.
Baeza-Yates, R., & Ramakrishnan, R. (2008). Data challenges at Yahoo! In EDBT ’08: Proceedings of the 11th international conference on extending database technology (pp. 652–655). ACM.
Baeza-Yates,R.,&Ribeiro-Neto,B.A. (1999). Moderninformationretrieval. New York: ACM/Addison-Wesley.
Balakrishnan, H., Kaashoek, M. F., Karger, D., Morris, R., & Stoica, I. (2003). Looking up data in P2P systems. Communications of the ACM, 46(2), 43–48.
Balog, K., Azzopardi, L., & de Rijke, M. (2006). Formal models for expert  nding in enterprise corpora. In SIGIR ’06: Proceedings of the 29th annual interna- tional ACM SIGIR conference on research and development in information retrieval (pp. 43–50). ACM.
Barroso, L. A., Dean, J., & Hölzle, U. (2003). Web search for a planet:  e Google cluster architecture. IEEE Micro, 23(2), 22–28.
Beeferman,D.,&Berger,A. (2000). Agglomerativeclusteringofasearchen- gine query log. In Proceedings of the sixth ACM SIGKDD international conference on knowledge discovery and data mining (pp. 407–416). ACM.
Belew, R. K. (2000). Finding out about. Cambridge, UK: Cambridge University Press.
Belkin, N. J. (2008). (Somewhat) grand challenges for information retrieval. SIGIR Forum, 42(1), 47–54.
Belkin, N. J., & Cro , W. B. (1992). Information  ltering and information re-
References 489
trieval: Two sides of the same coin? Communications of the ACM, 35(12),
29–38.
Belkin, N. J., Oddy, R. N., & Brooks, H. M. (1997). ASK for information re-
trieval: Part I.: background and theory. In Readings in information retrieval (pp. 299–304). San Francisco: Morgan Kaufmann. (Reprinted from Jour- nal of Documentation, 1982, 38, 61–71)
Benczúr, A., Csalogány, K., Sarlós, T., & Uher, M. (2005). Spamrank – fully automatic link spam detection. In AIRWeb: 1st international workshop on adversarial information retrieval on the web (pp. 25–38).
Berger,A.,Caruana,R.,Cohn,D.,Freitag,D.,&Mittal,V. (2000). Bridging the lexical chasm: Statistical approaches to answer- nding. In SIGIR ’00: Proceedings of the 23rd annual international ACM SIGIR conference on re- search and development in information retrieval (pp. 192–199). ACM.
Berger, A., & Lafferty, J. (1999). Information retrieval as statistical translation. In
SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR con- ference on research and development in information retrieval (pp. 222–229). ACM.
Berger, A., & Mittal, V. O. (2000). Ocelot: a system for summarizing web pages. InSIGIR’00:Proceedingsofthe23rdannualinternationalACM SIGIR conference on research and development in information retrieval (pp. 144–151). ACM.
Bergman, M. K. (2001).  e deep web: Surfacing hidden value. Journal of Elec- tronic Publishing, 7(1).
Bernstein, Y., & Zobel, J. (2005). Redundant documents and search effective- ness. In CIKM ’05: Proceedings of the 14th ACM international conference on information and knowledge management (pp. 736–743). ACM.
Bernstein, Y., & Zobel, J. (2006). Accurate discovery of co-derivative documents via duplicate text detection. Information Systems, 31, 595–609.
Bikel, D. M., Miller, S., Schwartz, R., & Weischedel, R. (1997). Nymble: A high- performance learning name- nder. In Proceedings of the   h conference on applied natural language processing (pp. 194–201). Morgan Kaufmann.
Bikel, D. M., Schwartz, R. L., & Weischedel, R. M. (1999). An algorithm that learns what’s in a name. Machine Learning, 34(1–3), 211–231.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal of Machine Learning Research, 3, 993–1,022.
Borgs, C., Chayes, J., Mahdian, M., & Saberi, A. (2004). Exploring the commu- nity structure of newsgroups. In KDD ’04: Proceedings of the tenth ACM
490 References
SIGKDD international conference on knowledge discovery and data mining
(pp. 783–787). ACM.
Breese, J., Heckerman, D., & Kadie, C. (1998). Empirical analysis of predictive
algorithms for collaborative  ltering. In UAI ’98: Proceedings of the uncer-
tainty in arti cal intelligence conference (pp. 43–52).
Brill, E. (1994). Some advances in transformation-based part of speech tagging.
In AAAI ’94: National conference on arti cial intelligence (pp. 722–727). Brin, S., & Page, L. (1998).  e anatomy of a large-scale hypertextual Web search
engine. Computer Networks and ISDN Systems, 30(1–7), 107–117. Broder, A. (2002). A taxonomy of web search. SIGIR Forum, 36(2), 3–10. Broder, A., Fontoura, M., Josifovski, V., & Riedel, L. (2007). A semantic ap-
proach to contextual advertising. In SIGIR ’07: Proceedings of the 30th annual international ACM SIGIR conference on research and development in information retrieval (pp. 559–566). ACM.
Broder, A., Fontura, M., Josifovski, V., Kumar, R., Motwani, R., Nabar, S., ... Xu, Y. (2006). Estimating corpus size via queries. In CIKM ’06: Proceedings of the 15th ACM international conference on information and knowledge man- agement (pp. 594–603). ACM.
Broder, A., Glassman, S. C., Manasse, M. S., & Zweig, G. (1997). Syntactic clustering of the Web. Computer Networks and ISDN Systems, 29(8–13), 1157–1166.
Brown, E. W., Callan, J., Cro , W. B., & Moss, J. E. B. (1994). Supporting full-text information retrieval with a persistent object store. In EDBT ’94: 4th international conference on extending database technology (Vol. 779, pp. 365–378). Springer.
Buckley, C., & Voorhees, E. M. (2004). Retrieval evaluation with incom- plete information. In SIGIR ’04: Proceedings of the 27th annual interna- tional ACM SIGIR conference on research and development in information retrieval (pp. 25–32). ACM.
Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., & Hul- lender, G. (2005). Learning to rank using gradient descent. In ICML ’05: Proceedings of the 22nd international conference on machine learning (pp. 89–96). ACM.
Burges, C. J. C. (1998). A tutorial on support vector machines for pattern recog- nition. Data Mining and Knowledge Discovery, 2(2), 121–167.
Burke, R. D., Hammond, K. J., Kulyukin, V. A., Lytinen, S. L., Tomuro, N., & Schoenberg, S. (1997). Question answering  om  equently asked question
References 491
 les: Experiences with the FAQ  nder system (Tech. Rep.). Chicago, IL,
USA.
Büttcher, S., & Clarke, C. L. A. (2007). Index compression is good, especially for
random access. In CIKM ’07: Proceedings of the sixteenth ACM conference
on information and knowledge management (pp. 761–770). ACM. Büttcher, S., Clarke, C. L. A., & Lushman, B. (2006). Hybrid index maintenance for growing text collections. In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on research and development in infor-
mation retrieval (pp. 356–363). ACM.
Byrd, D., & Crawford, T. (2002). Problems of music information retrieval in the
real world. Information Processing and Management, 38(2), 249–272. Callan,J. (2000). Distributedinformationretrieval. InAdvancesininforma- tion retrieval: Recent research  om the CIIR (pp. 127–150). Norwell, MA:
Kluwer Academic Publishers.
Callan, J., Cro , W. B., & Broglio, J. (1995). TREC and Tipster experiments
with Inquery. Information Processing and Management, 31(3), 327–343. Callan, J., Cro , W. B., & Harding, S. M. (1992).  e Inquery retrieval system. In Proceedings of DEXA-92, 3rd international conference on database and
expert systems applications (pp. 78–83).
Cao, Y., Xu, J., Liu, T.-Y., Li, H., Huang, Y., & Hon, H.-W. (2006). Adapting
ranking SVM to document retrieval. In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval (pp. 186–193). ACM.
Carbonell, J., & Goldstein, J. (1998).  e use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR ’98: Pro- ceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval (pp. 335–336). ACM.
Carson, C.,  omas, M., Belongie, S., Hellerstein, J. M., & Malik, J. (1999). Blob- world: A system for region-based image indexing and retrieval. In VISUAL ’99:  ird international conference on visual information and information systems (pp. 509–516). Springer.
Carterette,B.,Allan,J.,&Sitaraman,R. (2006). Minimaltestcollectionsfor retrieval evaluation. In SIGIR ’06: Proceedings of the 29th annual interna- tional ACM SIGIR conference on research and development in information retrieval (pp. 268–275). ACM.
Carterette, B., & Jones, R. (2007). Evaluating search engines by modeling the relationship between relevance and clicks. In NIPS ’07: Proceedings of the
492 References
conference on neural information processing systems (pp. 217–224). MIT
Press.
Chakrabarti, S., van den Berg, M., & Dom, B. (1999). Focused crawling: A new
approach to topic-speci c web resource discovery. Computer Networks,
31(11-16), 1,623–1,640.
Chamberlin, D. (2002). XQuery: An XML query language. IBM Systems Jour-
nal, 41(4), 597–615.
Chang, F., Dean, J., Ghemawat, S., Hsieh, W. C., Wallach, D. A., Burrows, M., ...
Gruber, R. E. (2006). Bigtable: a distributed storage system for structured data. In OSDI ’06: Proceedings of the 7th symposium on operating systems design and implementation (pp. 205–218). USENIX Association.
Charikar, M. S. (2002). Similarity estimation techniques from rounding algo- rithms. In STOC ’02: Proceedings of the annual ACM symposium on theory of computing (pp. 380–388). ACM.
Chaudhuri, S., Ramakrishnan, R., & Weikum, G. (2005). Integrating DB and IR technologies: What is the sound of one hand clapping? In CIDR 2005: Second biennial conference on innovative data systems research (pp. 1–12).
Chen, Y.-Y., Suel, T., & Markowetz, A. (2006). Efficient query processing in geographic web search engines. In SIGMOD ’06: Proceedings of the ACM SIGMOD international conference on management of data (pp. 277–288). ACM.
Cho, J., & Garcia-Molina, H. (2002). Parallel crawlers. In WWW 2002: Pro- ceedings of the 11th annual international world wide web conference (pp. 124–135). ACM.
Cho, J., & Garcia-Molina, H. (2003). Effective page refresh policies for web crawlers. ACM Transactions on Database Systems, 28, 390–426.
Church, K. W. (1988). A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the second conference on applied natural language processing (pp. 136–143). Association for Computational Lin- guistics.
Church, K. W., & Hanks, P. (1989). Word association norms, mutual informa- tion, and lexicography. In Proceedings of the 27th annual meeting on Asso- ciation for Computational Linguistics (pp. 76–83). Association for Com- putational Linguistics.
Clarke, C. L., Agichtein, E., Dumais, S., & White, R. W. (2007).  e in uence of caption features on clickthrough patterns in web search. In SIGIR ’07: Proceedings of the 30th annual international ACM SIGIR conference on re-
References 493
search and development in information retrieval (pp. 135–142). ACM. Cleverdon, C. (1970). Evaluation tests of information retrieval systems. Journal
of Documentation, 26(1), 55–67.
Coden, A., Brown, E. W., & Srinivasan, S. (Eds.). (2002). Information retrieval
techniques for speech applications. London: Springer-Verlag.
Cong, G., Wang, L., Lin, C.-Y., Song, Y.-I., & Sun, Y. (2008). Finding question- answer pairs from online forums. In SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on research and development
in information retrieval (pp. 467–474). ACM.
Conrad, J. G., & Utt, M. H. (1994). A system for discovering relationships by
feature extraction from text databases. In SIGIR ’94: Proceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval (pp. 260–270). Springer-Verlag.
Cooper, W. S. (1968). Expected search length: A single measure of retrieval effec- tiveness based on the weak ordering action of retrieval systems. American Documentation, 19(1), 30–41.
Cooper, W. S., Gey, F. C., & Dabney, D. P. (1992). Probabilistic retrieval based on staged logistic regression. In SIGIR ’92: Proceedings of the 15th annual international ACM SIGIR conference on research and development in infor- mation retrieval (pp. 198–210). ACM.
Cowie, J., & Lehnert, W. (1996). Information extraction. Communications of the ACM, 39(1), 80–91.
Crawford, R. (1981).  e relational model in information retrieval. Journal of the American Society for Information Science, 32(1), 51–64.
Cro , W. B. (2000). Combining approaches to information retrieval. In Advances in information retrieval: Recent research  om the CIIR (pp. 1–36). Norwell, MA: Kluwer Academic Publishers.
Cro , W. B., Krovetz, R., & Turtle, H. (1990). Interactive retrieval of complex documents. Information Processing and Management, 26(5), 593–613.
Cro , W. B., & Lafferty, J. (2003). Language modeling for information retrieval. Norwell, MA: Kluwer Academic Publishers.
Cro , W. B., Smith, L. A., & Turtle, H. R. (1992). A loosely-coupled integration of a text retrieval system and an object-oriented database system. In SIGIR ’92: Proceedings of the 15th annual international ACM SIGIR conference on research and development in information retrieval (pp. 223–232). ACM.
Cro ,W.B.,&Turtle,H. (1989). Aretrievalmodelincorporatinghypertext links. In Hypertext ’89: Proceedings of the second annual ACM conference on
494 References
hypertext (pp. 213–224). ACM.
Cro , W. B., Turtle, H. R., & Lewis, D. D. (1991).  e use of phrases and struc-
tured queries in information retrieval. In SIGIR ’91: Proceedings of the 14th annual international ACM SIGIR conference on research and development in information retrieval (pp. 32–45). ACM.
Cronen-Townsend,S.,Zhou,Y.,&Cro ,W.B. (2006). Precisionprediction based on ranked list coherence. Information Retrieval, 9(6), 723–755.
Cucerzan, S., & Brill, E. (2004). Spelling correction as an iterative process that exploits the collective knowledge of web users. In D. Lin & D. Wu (Eds.), Proceedings of EMNLP 2004 (pp. 293–300). Association for Computa- tional Linguistics.
Cui, H., Wen, J.-R., Nie, J.-Y., & Ma, W.-Y. (2003). Query expansion by mining user logs. IEEE Transactions on Knowledge and Data Engineering, 15(4), 829–839.
Dannenberg, R. B., Birmingham, W. P., Pardo, B., Hu, N., Meek, C., & Tzane- takis, G. (2007). A comparative evaluation of search techniques for query- by-humming using the MUSART testbed. Journal of the American Society for Information Science and Technology, 58(5), 687–701.
Dean,J.,&Ghemawat,S. (2008). MapReduce:simpli eddataprocessingon large clusters. Communications of the ACM, 51(1), 107–113.
DeCandia, G., Hastorun, D., Jampani, M., Kakulapati, G., Lakshman, A., Pilchin, A., ... Vogels, W. (2007). Dynamo: Amazon’s highly available key-value store. In SOSP ’07: Proceedings of the twenty- rst ACM SIGOPS symposium on operating systems principles (pp. 205–220). ACM.
Deerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W., & Harshman, R. A. (1990). Indexing by Latent Semantic Analysis. Journal of the Amer- ican Society of Information Science, 41(6), 391–407.
Deutsch, P. (1996). DEFLATE compressed data format speci cation version 1.3(RFCNo.1951). InternetEngineeringTaskForce. Retrievedfrom http://www.rfc-editor.org/rfc/rfc1951.txt
Diaz, F. (2005). Regularizing ad hoc retrieval scores. In CIKM ’05: Proceedings of the 14th ACM international conference on information and knowledge man- agement (pp. 672–679). ACM.
Duda, R. O., Hart, P. E., & Stork, D. G. (2000). Pattern classi cation (2nd ed.). Wiley-Interscience.
Dunlop,M.D.,&vanRijsbergen,C.J. (1993). Hypermediaandfreetextre- trieval. Information Processing and Management, 29(3), 287–298.
References 495
Echihabi, A., & Marcu, D. (2003). A noisy-channel approach to question answer- ing. In ACL ’03: Proceedings of the 41st annual meeting on Association for Computational Linguistics (pp. 16–23). Association for Computational Linguistics.
E himiadis, E. N. (1996). Query expansion. In M. E. Williams (Ed.), An- nual review of information systems and technology (ARIST) (Vol. 31, pp. 121–187).
Elmasri, R., & Navathe, S. (2006). Fundamentals of database systems (5th ed.). Reading, MA: Addison-Wesley.
Fagin, R., Lotem, A., & Naor, M. (2003). Optimal aggregation algorithms for middleware. Journal of Computer and Systems Sciences, 66(4), 614–656.
Feng, J., Bhargava, H. K., & Pennock, D. M. (2007). Implementing spon- sored search in web search engines: Computational evaluation of alterna- tive mechanisms. INFORMS Journal on Computing, 19(1), 137–148.
Fetterly, D., Manasse, M., & Najork, M. (2003). On the evolution of clusters of near-duplicate web pages. In LA-WEB ’03: Proceedings of the  rst conference on Latin American Web Congress (pp. 37–45). IEEE Computer Society.
Finn,A.,Kushmerick,N.,&Smyth,B. (2001). Factor ction:Contentclas- si cation for digital libraries. In DELOS workshop: Personalisation and recommender systems in digital libraries.
Flake, G. W., Lawrence, S., & Giles, C. L. (2000). Efficient identi cation of web communities. In KDD ’00: Proceedings of the sixth ACM SIGKDD inter- national conference on knowledge discovery and data mining (pp. 150–160). ACM.
Flickner, M., Sawhney, H. S., Ashley, J., Huang, Q., Dom, B., Gorkani, M., ... Yanker, P. (1995). Query by image and video content:  e QBIC system. IEEE Computer, 28(9), 23–32.
Fox, S., Karnawat, K., Mydland, M., Dumais, S., & White, T. (2005). Evaluating implicit measures to improve web search. ACM Transactions on Informa- tion Systems, 23(2), 147–168.
Fuhr, N. (2000). Probabilistic datalog: Implementing logical information re- trieval for advanced applications. Journal of the American Society for Infor- mation Science and Technology, 51(2), 95–110.
Fuhr, N. (2008). A probability ranking principle for interactive information retrieval. Information Retrieval, 11(3), 251–265.
Fuhr, N., & Buckley, C. (1991). A probabilistic learning approach for document indexing. ACM Transactions on Information Systems, 9(3), 223–248.
496 References
Fuhr, N., & Rölleke, T. (1997). A probabilistic relational algebra for the integra- tion of information retrieval and database systems. ACM Transactions on Information Systems, 15(1), 32–66.
Fujii, H., & Cro , W. B. (1993). A comparison of indexing techniques for Japanese text retrieval. In SIGIR ’93: Proceedings of the 16th annual in- ternational ACM SIGIR conference on research and development in infor- mation retrieval (pp. 237–246). ACM.
Gao, J., Nie, J.-Y., Wu, G., & Cao, G. (2004). Dependence language model for information retrieval. In SIGIR ’04: Proceedings of the 27th annual interna- tional ACM SIGIR conference on research and development in information retrieval (pp. 170–177). ACM.
Gao, J., Qi, H., Xia, X., & Nie, J.-Y. (2005). Linear discriminant model for in- formation retrieval. In SIGIR ’05: Proceedings of the 28th annual interna- tional ACM SIGIR conference on research and development in information retrieval (pp. 290–297). ACM.
Garcia-Molina, H., Ullman, J. D., & Widom, J. D. (2008). Database systems:  e complete book. Prentice Hall.
Gibson,D.,Kleinberg,J.,&Raghavan,P. (1998). Inferringwebcommunities from link topology. In HYPERTEXT ’98: Proceedings of the ninth ACM conference on hypertext and hypermedia (pp. 225–234). ACM.
Golder, S. A., & Huberman, B. A. (2006). Usage patterns of collaborative tagging systems. Journal of Information Science, 32(2), 198–208.
Goldstein, J., Kantrowitz, M., Mittal, V., & Carbonell, J. (1999). Summarizing text documents: sentence selection and evaluation metrics. In SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR conference on re- search and development in information retrieval (pp. 121–128). ACM.
Grefenstette,G. (1998). Cross-languageinformationretrieval. Norwell,MA: Kluwer Academic Publishers.
Guo, J., Xu, G., Li, H., & Cheng, X. (2008). A uni ed and discriminative model for query re nement. In SIGIR ’08: Proceedings of the 31st annual interna- tional ACM SIGIR conference on research and development in information retrieval (pp. 379–386). ACM.
Gupta, S., Kaiser, G., Neistadt, D., & Grimm, P. (2003). DOM-based content extraction of HTML documents. In WWW ’03: Proceedings of the 12th international conference on World Wide Web (pp. 207–214). ACM.
Gyöngyi, Z., & Garcia-Molina, H. (2005). Web spam taxonomy. In AIRWeb: 1st international workshop on adversarial information retrieval on the web (pp.
References 497
39–47).
Gyöngyi, Z., Garcia-Molina, H., & Pedersen, J. (2004). Combating web spam
with TrustRank. In VLDB 2004: Proceedings of the thirtieth international
conference on very large data bases (pp. 576–587). Morgan Kaufmann. Ha, L. Q., Sicilia-Garcia, E. I., Ming, J., & Smith, F. J. (2002). Extension of Zipf ’s law to words and phrases. In Proceedings of the 19th international confer- ence on computational linguistics (pp. 1–6). Association for Computational
Linguistics.
Harding, S. M., Cro , W. B., & Weir, C. (1997). Probabilistic retrieval of OCR
degraded text using n-grams. In ECDL ’97: Proceedings of the  rst Euro- pean conference on research and advanced technology for digital libraries (pp. 345–359). Springer-Verlag.
Hastie, T., Tibshirani, R., & Friedman, J. (2001).  e elements of statistical learn- ing: Data mining, inference, and prediction. Springer.
Hatcher, E., & Gospodnetic, O. (2004). Lucene in action. Manning Publications. Haveliwala, T. H. (2002). Topic-sensitive PageRank. In WWW ’02: Proceed- ings of the 11th international conference on World Wide Web (pp. 517–526).
ACM.
Hawking, D., & Zobel, J. (2007). Does topic metadata help with web search?
Journal of the American Society for Information Science and Technology,
58(5), 613–628.
He, B., Patel, M., Zhang, Z., & Chang, K. (2007). Accessing the deep web. Com-
munications of the ACM, 50(5), 94–101.
Heaps, H. (1978). Information retrieval: Computational and theoretical aspects.
New York: Academic Press.
Hearst, M. A. (1999). User interfaces and visualization. In Modern information
retrieval (pp. 257–324). ACM/Addison-Wesley.
Hearst, M. A. (2006). Clustering versus faceted categories for information ex-
ploration. Communications of the ACM, 49(4), 59–61.
Hearst, M. A., & Pedersen, J. O. (1996). Reexamining the cluster hypothesis: scatter/gather on retrieval results. In SIGIR ’96: Proceedings of the 19th annual international ACM SIGIR conference on research and development
in information retrieval (pp. 76–84). ACM.
Henzinger, M. (2006). Finding near-duplicate web pages: A large-scale evalua-
tion of algorithms. In SIGIR ’06: Proceedings of the 29th annual interna- tional ACM SIGIR conference on research and development in information retrieval (pp. 284–291). ACM.
498 References
Herlocker, J. L., Konstan, J. A., Terveen, L. G., & Riedl, J. T. (2004). Evaluating collaborative  ltering recommender systems. ACM Transactions on Infor- mation Systems, 22(1), 5–53.
Heymann, P., Koutrika, G., & Garcia-Molina, H. (2008). Can social bookmark- ing improve web search? In WSDM ’08: Proceedings of the international conference on web search and web data mining (pp. 195–206). ACM.
Heymann, P., Ramage, D., & Garcia-Molina, H. (2008). Social tag prediction. In
SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR con- ference on research and development in information retrieval (pp. 531–538). ACM.
Hiemstra,D. (1998). Alinguisticallymotivatedprobabilisticmodelofinfor- mation retrieval. In ECDL ’98: Proceedings of the second european confer- ence on research and advanced technology for digital libraries (pp. 569–584). Springer-Verlag.
Hoad, T., & Zobel, J. (2003). Methods for identifying versioned and plagia- rised documents. Journal of the American Society of Information Science and Technology, 54(3), 203–215.
Hobbs, J., Douglas, R., Appelt, E., Bear, J., Israel, D., Kameyama, M., ... Tyson, M. (1997). Fastus:Acascaded nite-statetransducerforextractingin- formationfromnatural-languagetext. InFinitestatelanguageprocessing (chap. 13). Cambridge, MA: MIT Press.
Hofmann, T. (1999). Probabilistic latent semantic indexing. In SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR conference on re- search and development in information retrieval (pp. 50–57). ACM.
Hopcro , J., Khan, O., Kulis, B., & Selman, B. (2003). Natural communi- ties in large linked networks. In KDD ’03: Proceedings of the ninth ACM SIGKDD international conference on knowledge discovery and data mining (pp. 541–546). ACM.
Ingwersen, P., & Järvelin, K. (2005).  e turn: Integration of information seeking and retrieval in context. Secaucus, NJ: Springer-Verlag.
Ipeirotis, P. G., & Gravano, L. (2004). When one sample is not enough: Improv- ing text database selection using shrinkage. In SIGMOD ’04: Proceedings of the 2004 ACM SIGMOD international conference on management of data (pp. 767–778). ACM.
Ittycheriah, A., Franz, M., Zhu, W.-J., Ratnaparkhi, A., & Mammone, R. J. (2001). Question answering using maximum entropy components. In NAACL ’01: Second meeting of the North American Chapter of the Association for
References 499
Computational Linguistics on language technologies (pp. 1–7). Association
for Computational Linguistics.
Järvelin,K.,&Kekäläinen,J. (2002). Cumulatedgain-basedevaluationofIR
techniques. ACM Transactions on Information Systems, 20(4), 422–446. Jeon, J., Cro , W. B., & Lee, J. H. (2005). Finding similar questions in large question and answer archives. In CIKM ’05: Proceedings of the 14th ACM international conference on information and knowledge management (pp.
84–90). ACM.
Jeon, J., Cro , W. B., Lee, J. H., & Park, S. (2006). A framework to predict
the quality of answers with non-textual features. In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval (pp. 228–235). ACM.
Jijkoun,V.,&deRijke,M. (2005). Retrievinganswersfromfrequentlyasked questions pages on the web. In CIKM ’05: Proceedings of the 14th ACM international conference on information and knowledge management (pp. 76–83). ACM.
Jing,Y.,&Cro ,W.B. (1994). Anassociationthesaurusforinformationre- trieval. In Proceedings of RIAO-94, 4th international conference “recherche d’information assistee par ordinateur” (pp. 146–160).
Joachims, T. (2002a). Learning to classify text using support vector machines: Meth- ods, theory and algorithms. Norwell, MA: Kluwer Academic Publishers.
Joachims, T. (2002b). Optimizing search engines using clickthrough data. In
KDD ’02: Proceedings of the eighth ACM SIGKDD international conference
on knowledge discovery and data mining (pp. 133–142). ACM.
Joachims, T., Granka, L., Pan, B., Hembrooke, H., & Gay, G. (2005). Accurately interpreting clickthrough data as implicit feedback. In SIGIR ’05: Proceed- ings of the 28th annual international ACM SIGIR conference on research and
development in information retrieval (pp. 154–161). ACM.
Jones, R., Rey, B., Madani, O., & Greiner, W. (2006). Generating query substi- tutions. In WWW ’06: Proceedings of the 15th international conference on
World Wide Web (pp. 387–396). ACM.
Jurafsky, D., & Martin, J. H. (2006). Speech and language processing (2nd ed.).
London: Prentice Hall.
Kazai,G.,Gövert,N.,Lalmas,M.,&Fuhr,N. (2003).  eINEXevaluation
initiative. In H. Blanken, T. Grabs, H.-J. Schek, R. Schenkel, & G. Weikum
(Eds.), Intelligent XML retrieval (pp. 279–293). Springer.
Kelly, D., & Teevan, J. (2003). Implicit feedback for inferring user preference: A
500 References
bibliography. SIGIR Forum, 32(2).
Kleinberg,J.M. (1999). Authoritativesourcesinahyperlinkedenvironment.
Journal of the ACM, 46(5), 604–632.
Knuth,D.E. (1998).  eartofcomputerprogramming:Sortingandsearching
(2nd ed., Vol. 3). Redwood City, CA: Addison-Wesley Longman. Koenemann, J., & Belkin, N. J. (1996). A case for interaction: a study of interac- tive information retrieval behavior and effectiveness. In CHI ’96: Proceed- ings of the SIGCHI conference on human factors in computing systems (pp.
205–212). ACM.
Kraaij, W., Westerveld, T., & Hiemstra, D. (2002).  e importance of prior
probabilities for entry page search. In SIGIR ’02: Proceedings of the 25th annual international ACM SIGIR conference on research and development in information retrieval (pp. 27–34). ACM.
Krovetz, R. (1993). Viewing morphology as an inference process. In SIGIR ’93: Proceedings of the 16th annual international ACM SIGIR conference on research and development in information retrieval (pp. 191–202). ACM.
Kukich, K. (1992). Techniques for automatically correcting words in text. ACM Computing Surveys, 24(4), 377–439.
Kurland, O. (2008).  e opposite of smoothing: A language model approach to ranking query-speci c document clusters. In SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on research and develop- ment in information retrieval (pp. 171–178). ACM.
Kurland, O., & Lee, L. (2004). Corpus structure, language models, and ad hoc information retrieval. In SIGIR ’04: Proceedings of the 27th annual interna- tional ACM SIGIR conference on research and development in information retrieval (pp. 194–201). ACM.
Kurland, O., & Lee, L. (2005). Pagerank without hyperlinks: structural re- ranking using links induced by language models. In SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on research and de- velopment in information retrieval (pp. 306–313). ACM.
Lafferty, J., & Zhai, C. (2001). Document language models, query models, and risk minimization for information retrieval. In SIGIR ’01: Proceedings of the 24th annual international ACM SIGIR conference on research and de- velopment in information retrieval (pp. 111–119). ACM.
Lankes, R. D. (2004).  e digital reference research agenda. Journal of the Amer- ican Society for Information Science and Technology, 55(4), 301–311. Larkey, L. S., Ballesteros, L., & Connell, M. E. (2002). Improving stemming
References 501
for Arabic information retrieval: Light stemming and co-occurrence anal- ysis. In SIGIR ’02: Proceedings of the 25th annual international ACM SI- GIR conference on research and development in information retrieval (pp. 275–282). ACM.
Lavrenko, V., & Cro , W. B. (2001). Relevance based language models. In SIGIR ’01: Proceedings of the 24th annual international ACM SIGIR conference on research and development in information retrieval (pp. 120–127). ACM.
Lawrie, D. J., & Cro , W. B. (2003). Generating hierarchical summaries for web searches. In SIGIR ’03: Proceedings of the 26th annual international ACM SIGIR conference on research and development in information retrieval (pp. 457–458). ACM.
Leouski, A., & Cro , W. (1996). An evaluation of techniques for clustering search results (Tech. Rep. Nos. IR–76). Department of Computer Science, Uni- versity of Massachusetts Amherst.
Lester, N., Moffat, A., & Zobel, J. (2005). Fast on-line index construction by geometric partitioning. In CIKM ’05: Proceedings of the 14th ACM international conference on information and knowledge management (pp. 776–783). New York: ACM.
Leuski, A., & Lavrenko, V. (2006). Tracking dragon-hunters with language mod- els. In CIKM ’06: Proceedings of the 15th ACM international conference on information and knowledge management (pp. 698–707). ACM.
Liu, X., & Cro , W. B. (2004). Cluster-based retrieval using language models. In
SIGIR ’04: Proceedings of the 27th annual international ACM SIGIR con- ference on research and development in information retrieval (pp. 186–193). ACM.
Liu, X., & Cro , W. B. (2008). Evaluating text representations for retrieval of the best group of documents. In ECIR ’08: Proceedings of the 30th European conference on information retrieval (pp. 454–462). Springer.
Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60(2), 91–110.
Lu, J., & Callan, J. (2006). Full-text federated search of text-based digital libraries in peer-to-peer networks. Information Retrieval, 9(4), 477–498.
Lu, J., & Callan, J. (2007). Content-based peer-to-peer network overlay for full- text federated search. In RIAO ’07: Proceedings of the eighth RIAO confer- ence.
Luhn, H. P. (1958).  e automatic creation of literature abstracts. IBM Journal of Research and Development, 2(2), 159–165.
502 References
Lund, K., & Burgess, C. (1996). Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, Instrumentation, and Computers, 28(2), 203–208.
Manning, C. D., Raghavan, P., & Schütze, H. (2008). Introduction to information retrieval. New York: Cambridge University Press.
Manning, C. D., & Schütze, H. (1999). Foundations of statistical natural language processing. Cambridge, MA:  e MIT Press.
Marchionini, G. (2006). Exploratory search: from  nding to understanding. Communications of the ACM, 49(4), 41–46.
McBryan, O. A. (1994). GENVL and WWWW: Tools for Taming the Web. In
WWW ’94: Proceedings of the  rst international World Wide Web conference
(p. 15). CERN, Geneva.
McCallum, A. (2005). Information extraction: distilling structured data from
unstructured text. Queue, 3(9), 48–57.
McCallum, A., & Nigam, K. (1998). A comparison of event models for naive
Bayes text classi cation. In AAAI-98 workshop on learning for text catego-
rization.
Menczer, F., & Belew, R. K. (1998). Adaptive information agents in distributed textual environments. In AGENTS ’98: Proceedings of the second interna- tional conference on autonomous agents (pp. 157–164). ACM.
Metzler, D., & Cro , W. B. (2004). Combining the language model and inference network approaches to retrieval. Information Processing and Management, 40(5), 735–750.
Metzler, D., & Cro , W. B. (2005a). Analysis of statistical question classi cation for fact-based questions. Information Retrieval, 8(3), 481–504.
Metzler, D., & Cro , W. B. (2005b). A Markov random  eld model for term dependencies. In SIGIR ’05: Proceedings of the 28th annual interna- tional ACM SIGIR conference on research and development in information retrieval (pp. 472–479). ACM.
Metzler, D., & Cro , W. B. (2007a). Latent concept expansion using Markov random  elds. In SIGIR ’07: Proceedings of the 30th annual interna- tional ACM SIGIR conference on research and development in information retrieval (pp. 311–318). ACM.
Metzler, D., & Cro , W. B. (2007b). Linear feature-based models for information retrieval. Information Retrieval, 10(3), 257–274.
Metzler,D.,Dumais,S.T.,&Meek,C. (2007). Similaritymeasuresforshort segments of text. In ECIR ’07: Proceedings of the European conference on
References 503
information retrieval (pp. 16–27). Springer.
Metzler, D., Lavrenko, V., & Cro , W. B. (2004). Formal multiple-Bernoulli
models for language modeling. In SIGIR ’04: Proceedings of the 27th an- nual international ACM SIGIR conference on research and development in information retrieval (pp. 540–541). ACM.
Metzler, D., Strohman, T., & Cro , W. B. (2008). A statistical view of binned re- trieval models. In ECIR 2008: Proceedings of the 30th European conference on information retrieval (pp. 175–186). Springer.
Metzler, D., Strohman, T., Turtle, H., & Cro , W. (2004). Indri at TREC 2004: Terabyte track. In NIST special publication 500–261: Text REtrieval Conference proceedings (TREC 2004). National Institute of Standards and Technolog y.
Miller, D. R. H., Leek, T., & Schwartz, R. M. (1999). A Hidden Markov Model information retrieval system. In SIGIR ’99: Proceedings of the 22nd an- nual international ACM SIGIR conference on research and development in information retrieval (pp. 214–221). ACM.
Mizzaro, S. (1997). Relevance:  e whole history. Journal of the American Society of Information Science, 48(9), 810–832.
Moffat, A., Webber, W., & Zobel, J. (2007). Strategic system comparisons via targeted relevance judgments. In SIGIR ’07: Proceedings of the 30th an- nual international ACM SIGIR conference on research and development in information retrieval (pp. 375–382). ACM.
Montague, M., & Aslam, J. A. (2002). Condorcet fusion for improved retrieval. In CIKM ’02: Proceedings of the eleventh international conference on infor- mation and knowledge management (pp. 538–548). ACM.
Morris, M. R. (2008). A survey of collaborative web search practices. In CHI ’08: Proceeding of the twenty-sixth annual SIGCHI conference on human factors in computing systems (pp. 1,657–1,660). ACM.
Morris, M. R., & Horvitz, E. (2007a). S3: Storable, shareable search. In Interact (1) (pp. 120–123).
Morris, M. R., & Horvitz, E. (2007b). SearchTogether: an interface for col- laborative web search. In UIST ’07: Proceedings of the 20th annual ACM symposium on user interface so ware and technology (pp. 3–12). ACM.
Ntoulas, A., Najork, M., Manasse, M., & Fetterly, D. (2006). Detecting spam web pages through content analysis. In WWW ’06: Proceedings of the 15th international conference on World Wide Web (pp. 83–92).
Ogilvie, P., & Callan, J. (2003). Combining document representations for
504 References
known-item search. In SIGIR ’03: Proceedings of the 26th annual interna- tional ACM SIGIR conference on research and development in informaion retrieval (pp. 143–150). ACM.
Pang, B., Lee, L., & Vaithyanathan, S. (2002).  umbs up?: sentiment classi - cation using machine learning techniques. In EMNLP ’02: Proceedings of the ACL-02 conference on empirical methods in natural language processing (pp. 79–86). Association for Computational Linguistics.
Peng, F., Ahmed, N., Li, X., & Lu, Y. (2007). Context sensitive stemming for web search. In SIGIR ’07: Proceedings of the 30th annual international ACM SIGIR conference on research and development in information retrieval (pp. 639–646). ACM.
Peng, F., Feng, F., & McCallum, A. (2004). Chinese segmentation and new word detection using conditional random  elds. In COLING ’04: Proceedings of the 20th international conference on computational linguistics (p. 562). Association for Computational Linguistics.
Pentland, A., Picard, R. W., & Sclaroff, S. (1996). Photobook: Content-based manipulation of image databases. International Journal of Computer Vision, 18(3), 233–254.
Petkova, D., & Cro , W. B. (2007). Proximity-based document represen- tation for named entity retrieval. In CIKM ’07: Proceedings of the six- teenth ACM conference on information and knowledge management (pp. 731–740). ACM.
Pickens, J., Golovchinsky, G., Shah, C., Qvarfordt, P., & Back, M. (2008). Al- gorithmic mediation for collaborative exploratory search. In SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on re- search and development in information retrieval (pp. 315–322). ACM.
Pinto, D., Branstein, M., Coleman, R., Cro , W. B., King, M., Li, W., & Wei, X. (2002). QuASM: a system for question answering using semi-structured data. In JCDL ’02: Proceedings of the 2nd ACM/IEEE-CS joint conference on digital libraries (pp. 46–55). ACM.
Ponte, J. M., & Cro , W. B. (1998). A language modeling approach to infor- mation retrieval. In SIGIR ’98: Proceedings of the 21st annual interna- tional ACM SIGIR conference on research and development in information retrieval (pp. 275–281). ACM.
Porter, M. F. (1997). An algorithm for suffix stripping. In Readings in information retrieval (pp. 313–316). San Francisco: Morgan Kaufmann.
Powell, A. L., French, J. C., Callan, J., Connell, M., & Viles, C. L. (2000).  e
References 505
impact of database selection on distributed searching. In SIGIR ’00: Pro- ceedings of the 23rd annual international ACM SIGIR conference on research and development in information retrieval (pp. 232–239). ACM.
Pritchard-Schoch, T. (1993). WIN–WESTLAW goes natural. Online, 17(1), 101–103.
Rajashekar, T. B., & Cro , W. B. (1995). Combining automatic and manual index representations in probabilistic retrieval. Journal of the American So- ciety of Information Science, 46(4), 272–283.
Ravela, S., & Manmatha, R. (1997). Image retrieval by appearance. In SIGIR ’97: Proceedings of the 20th annual international ACM SIGIR conference on research and development in information retrieval (pp. 278–285). ACM.
Riezler, S., Vasserman, A., Tsochantaridis, I., Mittal, V., & Liu, Y. (2007). Statisti- cal machine translation for query expansion in answer retrieval. In Proceed- ings of the 45th annual meeting of the association of computational linguistics (pp. 464–471). ACL.
Robertson, S. E. (1997).  e probability ranking principle in IR. In Readings in information retrieval (pp. 281–286). Morgan Kaufmann. (Reprinted from Journal of Documentation, 1977, 33, 294–304)
Robertson, S. E. (2004). Understanding inverse document frequency: On theo- retical arguments for IDF. Journal of Documentation, 60, 503–520. Robertson, S. E., & Walker, S. (1994). Some simple effective approximations to
the 2-poisson model for probabilistic weighted retrieval. In SIGIR ’94: Pro- ceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval (pp. 232–241). Springer-Verlag.
Robertson, S. E., Zaragoza, H., & Taylor, M. (2004). Simple BM25 extension to multiple weighted  elds. In CIKM ’04: Proceedings of the thirteenth ACM international conference on information and knowledge management (pp. 42–49). ACM.
Rocchio, J. J. (1971). Relevance feedback in information retrieval. In G. Salton (Ed.),  e SMART retrieval system: Experiments in automatic document processing (pp. 313–323). Englewood Cliffs, NJ: Prentice-Hall.
Romano, N. C., Roussinov, D., Nunamaker, J. F., & Chen, H. (1999). Collab- orative information retrieval environment: Integration of information re- trieval with group support systems. In HICSS ’99: Proceedings of the thirty- second annual Hawaii international conference on system sciences-volume 1 (pp. 1,053). IEEE Computer Society.
Sahami, M., & Heilman, T. D. (2006). A web-based kernel function for measur-
506 References
ing the similarity of short text snippets. In WWW ’06: Proceedings of the
15th international conference on World Wide Web (pp. 377–386). ACM. Salton, G. (1968). Automatic information organization and retrieval. New York:
McGraw-Hill.
Salton, G., & Buckley, C. (1988). Term-weighting approaches in automatic text
retrieval. Information Processing and Management, 24(5), 513–523. Salton, G., & McGill, M. J. (1983). Introduction to modern information retrieval.
New York: McGraw-Hill.
Salton, G., Wong, A., & Yang, C. S. (1975). A vector space model for automatic
indexing. Communications of the ACM, 18(11), 613–620.
Sanderson, M., & Zobel, J. (2005). Information retrieval system evaluation: effort, sensitivity, and reliability. In SIGIR ’05: Proceedings of the 28th an- nual international ACM SIGIR conference on research and development in
information retrieval (pp. 162–169). ACM.
Saracevic, T. (1975). Relevance: A review of and a framework for the thinking
on the notion in information science. Journal of the American Society for
Information Science, 26(6), 321–343.
Saraiva, P. C., de Moura, E. S., Ziviani, N., Meira, W., Fonseca, R., & Riberio-
Neto, B. (2001). Rank-preserving two-level caching for scalable search engines. In SIGIR ’01: Proceedings of the 24th annual international ACM SIGIR conference on research and development in information retrieval (pp. 51–58). New York: ACM.
Schapire, R. E., Singer, Y., & Singhal, A. (1998). Boosting and Rocchio ap- plied to text  ltering. In SIGIR ’98: Proceedings of the 21st annual interna- tional ACM SIGIR conference on research and development in information retrieval (pp. 215–223). ACM.
Shannon, C. (1951). Prediction and entropy in printed English. Bell System Technical Journal, 30, 50–64.
Shannon, C., & Weaver, W. (1963). A mathematical theory of communication. Champaign, IL: University of Illinois Press.
Shneiderman, B., Byrd, D., & Cro , W. B. (1998). Sorting out searching: a user- interface framework for text searches. Communications of the ACM , 41(4), 95–98.
Si, L., & Callan, J. (2003). A semi-supervised learning method to merge search engine results. ACM Transactions on Information Systems, 21(4), 457–491. Si, L., & Callan, J. (2004). Uni ed utility maximization framework for resource
selection. In CIKM ’04: Proceedings of the eleventh international conference
References 507
on information and knowledge management. ACM.
Singhal, A., & Pereira, F. (1999). Document expansion for speech retrieval. In
SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR con- ference on research and development in information retrieval (pp. 34–41). ACM.
Smucker, M., Allan, J., & Carterette, B. (2007). A comparison of statistical sig- ni cance tests for information retrieval evaluation. In CIKM ’07: Proceed- ings of the 14th ACM international conference on information and knowl- edge management. ACM.
Song, F., & Cro , W. B. (1999). A general language model for information retrieval. In CIKM ’99: Proceedings of the eighth international conference on information and knowledge management (pp. 316–321). ACM.
Song, R., Liu, H., Wen, J.-R., & Ma, W.-Y. (2004). Learning block importance models for web pages. In WWW ’04: Proceedings of the 13th international conference on World Wide Web (pp. 203–211). ACM.
Sparck Jones, K., Walker, S., & Robertson, S. E. (2000). A probabilistic model of information retrieval: development and comparative experiments. In- formation Processing and Management, 36(6), 779–808.
Strohman, T. (2007). Efficient processing of complex features for information re- trieval (Unpublished doctoral dissertation). University of Massachusetts Amherst.
Strohman, T., & Cro , W. B. (2006). Low latency index maintenance in Indri. In
OSIR 2006: Proceedings of the second international workshop on open source
information retrieval (pp. 7–11).
Strohman, T., Metzler, D., Turtle, H., & Cro , W. B. (2005). Indri: A language
model-based search engine for complex queries. In Proceedings of the inter-
national conference on intelligence analysis.
Sun, J.-T., Shen, D., Zeng, H.-J., Yang, Q., Lu, Y., & Chen, Z. (2005). Web- page summarization using clickthrough data. In SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval (pp. 194–201). ACM.
Sutton, C., & McCallum, A. (2007). An introduction to conditional random  elds for relational learning. In L. Getoor & B. Taskar (Eds.), Introduction to statistical relational learning. Cambridge, MA, USA: MIT Press.
Taghva, K., Borsack, J., & Condit, A. (1996). Evaluation of model-based retrieval effectiveness with OCR text. ACM Transactions on Information Systems, 14(1), 64–93.
508 References
Trotman, A., & Lalmas, M. (2006). Why structural hints in queries do not help XML-retrieval. In SIGIR ’06: Proceedings of the 29th annual interna- tional ACM SIGIR conference on research and development in information retrieval (pp. 711–712). ACM.
Trotman, A., & Sigurbjörnsson, B. (2004). Narrowed Extended XPath I (NEXI). In INEX workshop proceedings (pp. 16–40). Springer.
Turpin,A.,Tsegay,Y.,&Hawking,D. (2007). Fastgenerationofresultsnip- pets in web search. In SIGIR ’07: Proceedings of the 30th annual interna- tional ACM SIGIR conference on research and development in information retrieval (pp. 127–134). ACM.
Turtle, H. (1994). Natural language vs. Boolean query evaluation: a compari- son of retrieval performance. In SIGIR ’94: Proceedings of the 17th annual international ACM SIGIR conference on research and development in infor- mation retrieval (pp. 212–220). Springer-Verlag.
Turtle, H., & Cro , W. B. (1991). Evaluation of an inference network-based re- trieval model. ACM Transactions on Information Systems, 9(3), 187–222.
Turtle, H., & Flood, J. (1995). Query evaluation: strategies and optimizations. Information Processing and Management, 31(6), 831–850.
Unicode Consortium. (2006).  e Unicode standard, version 5.0. Addison- Wesley Professional.
van Rijsbergen, C. J. (1979). Information retrieval (2nd ed.). London: Butter- worths.
Vasconcelos, N. (2007). From pixels to semantic spaces: Advances in content- based image retrieval. Computer, 40(7), 20–26.
Voorhees, E. M. (1985).  e cluster hypothesis revisited. In SIGIR ’85: Proceed- ings of the 8th annual international ACM SIGIR conference on research and development in information retrieval (pp. 188–196). ACM.
Voorhees, E. M., & Buckley, C. (2002).  e effect of topic set size on retrieval experiment error. In SIGIR ’02: Proceedings of the 25th annual interna- tional ACM SIGIR conference on research and development in information retrieval (pp. 316–323). ACM.
Voorhees, E. M., & Harman, D. (Eds.). (2005). TREC: Experiment and evalua- tion in information retrieval. Cambridge, MA: MIT Press.
Wang, A. (2006).  e Shazam music recognition service. Communications of the ACM, 49(8), 44–48.
Wei, X., & Cro , W. B. (2006). LDA-based document models for ad-hoc re- trieval. In SIGIR ’06: Proceedings of the 29th annual international ACM
References 509
SIGIR conference on research and development in information retrieval (pp.
178–185). ACM.
Wei, X., & Cro , W. B. (2007). Investigating retrieval performance with
manually-built topic models. In RIAO ’07: Proceedings of the eighth RIAO
conference.
Welch, T. A. (1984). A technique for high-performance data compression. Com-
puter, 17, 8–19.
Witten, I. H., Moffat, A., & Bell, T. C. (1999). Managing Gigabytes: Compressing
and indexing documents and images (2nd ed.). San Francisco, CA, USA:
Morgan Kaufmann.
Xia, F., Liu, T., Wang, J., Zhang, W., & Li, H. (2008). Listwise approach to learn-
ing to rank – theory and algorithm. In ICML ’08: Proceedings of the 25th annual international conference on machine learning (pp. 1,192–1,199). Omnipress.
Xu, J., & Cro , W. B. (1998). Corpus-based stemming using cooccurrence of word variants. ACM Transactions on Information Systems, 16(1), 61–81.
Xu, J., & Cro , W. B. (2000). Improving the effectiveness of information re- trieval with local context analysis. ACM Transactions on Information Sys- tems, 18(1), 79–112.
Xu, Z., Fu, Y., Mao, J., & Su, D. (2006). Towards the semantic web: Collaborative tag suggestions. In WWW2006: Proceedings of the collaborative web tagging workshop. Edinburgh, Scotland.
Xue, X., Jeon, J., & Cro , W. B. (2008). Retrieval models for question and answer archives. In SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on research and development in information retrieval (pp. 475–482). ACM.
Yang, S., Zhu, H., Apostoli, A., & Cao, P. (2007). N-gram statistics in English and Chinese: Similarities and differences. In ICSC ’07: International conference on semantic computing (pp. 454–460). IEEE Computer Society.
Yao, Y. (1995). Measuring retrieval effectiveness based on user preference of documents. Journal of the American Society for Information Science, 46(2), 133–145.
Yih, W., Goodman, J., & Carvalho, V. R. (2006). Finding advertising keywords on web pages. In WWW ’06: Proceedings of the 15th international confer- ence on World Wide Web (pp. 213–222). ACM.
Yu, S., Cai, D., Wen, J.-R., & Ma, W.-Y. (2003). Improving pseudo-relevance feedback in web information retrieval using web page segmentation. In
510 References
WWW ’03: Proceedings of the 12th international conference on World Wide
Web (pp. 11–18). ACM.
Zamir, O., & Etzioni, O. (1999). Grouper: a dynamic clustering interface to web
search results. Computer Networks, 31(11–16), 1,361–1,374.
Zeng, H.-J., He, Q.-C., Chen, Z., Ma, W.-Y., & Ma, J. (2004). Learning to cluster web search results. In SIGIR ’04: Proceedings of the 27th annual interna- tional ACM SIGIR conference on research and development in information
retrieval (pp. 210–217). ACM.
Zhai, C., & Lafferty, J. (2004). A study of smoothing methods for language
models applied to information retrieval. ACM Transactions on Information
Systems, 22(2), 179–214.
Zhang, V., Rey, B., Stipp, E., & Jones, R. (2006). Geomodi cation in query
rewriting. In GIR ’06: Proceedings of the workshop on geographic information
retrieval, ACM SIGIR 2006.
Zhang, Y., & Callan, J. (2001). Maximum likelihood estimation for  lter- ing thresholds. In SIGIR ’01: Proceedings of the 24th annual interna- tional ACM SIGIR conference on research and development in information retrieval (pp. 294–302). ACM.
Zhou, Y., Xie, X., Wang, C., Gong, Y., & Ma, W.-Y. (2005). Hybrid index struc- tures for location-based web search. In CIKM ’05: Proceedings of the 14th ACM international conference on information and knowledge management (pp. 155–162). ACM.
Zobel, J. (1998). How reliable are the results of large-scale information re- trieval experiments? In SIGIR ’98: Proceedings of the 21st annual interna- tional ACM SIGIR conference on research and development in information retrieval (pp. 307–314). ACM.
Zobel, J., & Moffat, A. (2006). Inverted  les for text search engines. ACM Computing Surveys, 38(2), 6.
Zobel, J., Moffat, A., & Ramamohanarao, K. (1996). Guidelines for presentation and comparison of indexing techniques. ACM SIGMOD Record, 25(3), 10–15.
Zobel, J., Moffat, A., & Ramamohanarao, K. (1998). Inverted  les versus signa- ture  les for text indexing. ACM Transactions on Database Systems, 23(4), 453–490.
Zukowski, M., Héman, S., Nes, N., & Boncz, P. A. (2006). Super-scalar RAM- CPU cache compression. In ICDE: International conference on data engi- neering (p. 59). IEEE Computer Society.
 Index
absolute error, 437
accuracy, 359
ad hoc search, 3, 280, 423
adaptive  ltering, 425
adversarial information retrieval, 294 advertising, 218, 371
classifying, 371
contextual, 218–221 agglomerative clustering, 375 anchor text, 21, 56, 105, 280 API, 439, 461
architecture, 13–28 authority, 21, 111
automatic indexing, 400
background probability, see collection probability
bag of words, 345, 451 Bayes classi er, 245 Bayes Decision Rule, 245 Bayes’ Rule, 246, 343 Bayes’ rule, 342
Bayesian network, 268 bibliometrics, 120 bidding, 218
bigram, 100, 253 BigTable, 57
binary independence model, 246 blog, 111
BM25, 250–252
BM25F, 294
Boolean query, 235 Boolean query language, 24 Boolean retrieval, 235–237 boosting, 448
BPREF, 322
brute force, 331
burstiness, 254
caching, 26, 181
card catalog, 400
case folding, 87
case normalization, 87
categorization, see classi cation
CBIR, see content-based image retrieval character encoding, 50, 119
checksum, 60
Chi-squared measure, 202
CJK (Chinese-Japanese-Korean), 50, 119 classi cation, 3, 339–373
faceted, 224 monothetic, 223, 374 polythetic, 223, 374
classi er, 21
512 Index
clickthrough, 6, 27, 207, 285, 306
CLIR, see cross-language information retrieval
cluster hypothesis, 389
cluster-based retrieval, 391
clustering, 22, 222–225, 339, 373 co-occurrence, 74, 191
code page, 50
collaborative  ltering, 432 collaborative search, 420
collection, 3
collection language model, 256 collection probability, 256, 440 collocation, 74
color histogram, 473
combining evidence, 267–283 combining searches, 441
CombMNZ, 441
community-based question answering,
415
complete-link clusters, 379
compression, 54 lossless, 141 lossy, 142
conditional random  eld, 122 con ation, see stemming connected component, 192 content match, 371
content-based image retrieval, 473 context, 115, 201, 211–214 context vector, 206, 464 contingency table, 248
controlled vocabulary, 199, 401 conversion, 49
coordination level match, 257 corpus, 6
cosine correlation, 239 coverage, 8
CQA, 415
crawler, 17, 32
cross-language information retrieval, 226 cross-lingual search, see cross-language
information retrieval cross-validation, 331
Damerau-Levenshtein distance, 194 dangling link, 107
data mining, 113
database system, 459
DCG, see discounted cumulative gain deep Web, 41, 448
delta encoding, 144
dendrogram, 375
desktop search, 3, 46
Dice’s coefficient, 192
digital reference, 447
Dirichlet smoothing, 258
discounted cumulative gain, 319 discriminative model, 284, 360 distance measure, 374
distributed hash table, 445 distributed information retrieval, 438 distribution, 23
divisive clustering, 375
document, 2
document conversion, 18
document crawler, 17
document data store, 19
document distribution, 180
document slope curve, 64
document statistics, 22
document structure, 101, 269, 459–466 document summary, 215
downcasing, 87 dumping, 366
duplicate documents, 60 dwell time, 27
dynamic page, 42
edit distance, 194
effectiveness, 233, 297 effectiveness measure, 308–322 efficiency, 297, 322–325 elements, 20
enterprise search, 3
entity recognizer, 115
entity search, 464
entropy, 362
error model, 197
estimation, 256
evaluation, 5, 15, 297–335 evaluation measures, 27 exact-match retrieval, 235 expert search, 465
extent, 137
extraction, 21
F measure, 310
facet, 224
fallout, 309
FAQ, 447
feature, 14
feature selection, 362
feature-based retrieval model, 452 federated search, see distributed informa-
tion retrieval feed, 17, 47
 eld, 91, 136
 elds, see document structure  le-sharing, 442
 ltering, 3, 210
 ngerprint, 61–63
focused crawling, 17, 41 folksonomy, 401
forum, 448
freshness, 8, 39
frontier, 35
generative model, 115, 284, 359
Index 513
geographic information system, 213 geometric mean average precision, 313 Gnutella, 444
Google, 28, 100, 106
graphical model, 455
HAC (hierarchical agglomerative clustering), 376
Hapax Legomena, 76
Heaps’ law, 81
Hidden Markov Model, 115, 293 hidden Web, 41
hierarchical clustering, 375–382 hierarchical P2P, 444 high-dimensional space, 239 highlighting, 25, 215
HITS, 111, 410
HMM, see Hidden Markov Model homophone, 195
HTML, 18, 101
HTTP,33
hub, 111
idf, see inverse document frequency image annotation, 476
image search, 470
index, 8
index term, 14, 73 index vocabulary, 15 indexing, 14
indexing speed, 8, 324 Indri, 8, 273, 294 INEX, 462
inference network retrieval model, 267–273
information extraction, 21, 52, 113, 464 information gain, 362
information need, 6, 187
information retrieval, 1
inlink, 106
514 Index
Inquery, 28, 273, 294
interaction, 15, 231, 336, 480 intermediary, see search intermediary interpolation, 315
inverse document frequency, 22, 241 inverted  le, see inverted index inverted index, 15, 125
inverted list, 130
document-ordered, 130 score-sorted, 140
Jelinek-Mercer smoothing, 257
K nearest neighbor clustering, 384–386 K-means, 223
K-means clustering, 382–387
Kendall’s tau, 321
kernel function, 356, 465
key frame, 475
keyword, 24, 189, 220, 466 KL-divergence, see Kullback-Leibler
divergence known-item search, 280
Kullback-Leibler divergence, 261, 440
language model, 196, 252, 440 latency, 323
Latent Dirichlet allocation, 288 Latent semantic indexing, 288 LDA, see latent Dirichlet allocation learning to rank, 284–288
Lemur, 7, 91, 120
Levenshtein distance, 194 lexical analysis, 86
link analysis, 21, 105–111, 410 link extraction, 104–113
link spam, 111, 365
links, 21
local search, 212
log data, see query log
logistic regression, 294
long-tail query, 218
low-density language, 120
LSI, see latent semantic indexing Lucene, 7, 28
machine learning, 208, 283–291 machine translation, 227, 417 macroaverage, 315
manual indexing, 199, 400 manual tagging, 401
MAP, see mean average precision Markov random  eld model, 454–458 markup language, 19
markup tags, 20
maximal marginal relevance, 405 maximum likelihood estimate, 255 mean average precision, 313
mean reciprocal rank, 319
mean squared error, 438
memory hierarchy, 140
MeSH, 200
metadata, 14, 86, 217, 280 metasearch, 439, 441
metasearch engine, 438
microaverage, 315
MIDI, 477
MMR, see maximal marginal relevance morphology, 21, 121
MRF, see Markov random  eld model MRR, see mean reciprocal rank multinomial distribution, 254 multinomial model, 348–351 multiple-Bernoulli distribution, 270 multiple-Bernoulli model, 346–348 music information retrieval, 477 mutual information, 201
n-gram, 61, 100–101, 253
naïve Bayes, 246, see Bayes classi er
named entity, 21, 113, 468
named entity recognition, 113 Napster, 443
natural language processing, 74 NDCG, see normalized discounted
cumulative gain
near-duplicate documents, 60
nearest neighbor classi er, 361, 372 nearest neighbor clustering, see K nearest
neighbor clustering network overlay, 443
NEXI, 462
noisy channel model, 196
normalized discounted cumulative gain,
321 noun phrase, 97
novelty, 405
OCR, 471
online community, 408 online learning, 430 online testing, 332 ontology, 213, 341 open source, 7
opinion detection, 369 optimization
safe, 26
unsafe, 26 over tting, 287, 331
P2P networks, 442–446
P2P search, see peer-to-peer search PageRank, 21, 106–111, 280 parallel corpus, 227, 419 parameter sweep, 331
parameter value, 330
parsing, 19
part-of-speech tagger, 97
Parzen windows, 388
pay per click, 371
Index 515
peer-to-peer search, 3, 23, 438, 442 personalization, 211
phrase, 97–101
phrase stitching, 366
plagiarism, 60
politeness policy, 35
politeness window, 35
pooling, 303
POS tagger, see part-of-speech tagger posting, 130
potential function, 456
precision, 308
preferences, 306, 321
privacy, 212, 305
probability ranking principle, 243
pro le, 211, 423
proximity, 188, 269
pseudo-relevance feedback, 201, 208, 264,
458, 476 publishing, 47
pull, 47
push, 47
push application, 423
query expansion, 6, 24, 199–207, 264 query language, 23, 188, 273
query likelihood, 227, 254–261, 391, 440 query log, 24, 27, 193, 198, 206, 212, 284,
305
query processing, 125
query reformulation, 219
query suggestion, 6, 24
query throughput, 8, 323
query-based sampling, 440
query-speci c clustering, 392
question answering, 3, 415, 419, 466–470 question classi cation, 468
R-precision, 337 Rabin  ngerprint, 62
516 Index
random variables, 343 randomization test, 328 rank equivalence, 201 ranking, 15
ranking algorithm, 5, 25 ranking SVM, 285 RankNet, 294 readability, 217
recall, 308
recall-precision graph, 312 recency, 8
recommender systems, 432 relational database, 53 relevance, 4, 233, 302
binary, 234 multivalued, 234 topical, 5, 234 user, 5, 234
relevance feedback, 6, 25, 208–211, 242, 248, 429
relevance judgment, 6, 300 relevance model, 261–266, 476 replication, 23
response time, 8
retrieval model, 5, 233–292 ROC curve, 309
Rocchio algorithm, 242 RSS, 18, 47
run-on error, 196
scalability, 8 scoring, 25 scripted page, 42 search
home-page, 280 informational, 279 named-page, 280 navigational, 279 topical, 279 transactional, 279
search engine, 6 monolingual, 118
search engine optimization, 280, 299 search engineer, 10
search intermediary, 188
seed, 35
semantic annotation, 114
semantic web, 401
sentiment, 369
SEO, see search engine optimization session, 219
shape, 474
sign test, 327
signature  le, 129 signi cance factor, 216 signi cance test, 325–330 simhash, 62
similarity, 239
similarity measure, 374 simulation, 27
single-link clusters, 378 site search, 17
sitemap, 43
size estimate
collection, 86
result set, 83
small-world property, 450 smoothing, 256
snippet, 16, 52, 215
snippet generation, 215 social media site, 397 Soundex, 195
spam, 8, 22, 111, 364–368 speech recognition, 471 spell checking, 24, 193–199 spoken documents, 471 sponsored search, 371 SQL, 57
static  ltering, 424
static page, 42 stem class, 191 stemmer, 20
algorithmic, 92 dictionary-based, 92 Krovetz, 94
Porter, 92 query-based, 190 suffix-s, 92
stemming, 20, 91–96, 190 stopping, 20, 90
stopword list, 20, 90 stopwords, 90
structured query, 179
summarization, 215
superpeer, 444
supervised learning, 340
Support Vector Machine, 285, 351–358 SVM, see Support Vector Machine synonym, 199
syntactic analysis, 87
t-test, 327
tag, 400–408
tag cloud, 407
taxonomy, 213
TDT, see topic detection and tracking term association, 74, 192, 201
term dependence, 281, 454
term distribution, 181
term frequency, 22, 241, 260
term spam, 365
test collection, 6, 299
test set, 331, 340
text corpus, 299
text encoding, 18, see character encoding texture, 474
tf, see term frequency
tf.idf, 22, 241
thesaurus, 192, 199
Index 517
tokenizing, 19, 87–90
tokens, 19
topic, 253
topic detection and tracking, 448 topic model, 253, 288–291 training data, 208, 284
training set, 331, 340 translation-based model, 417 transliteration, 228
TREC, 6
trigram, 100, 253
UIMA, 13
Unicode, 51, 119
Uniform Resource Locator (URL), 33 unigram, 100, 252
unsupervised learning, 340
update, 56
user model, 211
UTF-8, 51, 148
v-byte, 148
vector space model, 237–243
vertical search, 3, 17, 41
video retrieval, 475
visualization, 215
Viterbi algorithm, 117
vocabulary growth, 80
vocabulary mismatch, 5, 219, 288, 403 vocabulary size, 76
weaving, 366
Web 2.0, 397
web community, 409
web search, 3, 279–283 weighting, 22, 241
Wilcoxon signed-rank test, 327 wildcard operator, 188
word frequency, 75
word occurrence, 75
518 Index
Wordnet, 200, 468 Zipf distribution, 80
XML, 18, 103, 461 XQuery, 104, 461
Zipf ’s law, 75
